# ==================================================================
# Claire de Binare - Memory Backend Environment Configuration
# ==================================================================
#
# This file configures the persistent memory backend stack:
# - Graphiti MCP Server (knowledge graph + MCP protocol)
# - FalkorDB (embedded graph database)
# - Ollama (local LLM and embeddings)
#
# SETUP CHECKLIST:
# [ ] Copy this file: cp .env.memory.example .env.memory
# [ ] Review settings (defaults work for most setups)
# [ ] Run: docker compose -f infrastructure/compose/memory.yml up -d
# [ ] Initialize: ./infrastructure/scripts/init-memory.sh
#
# ==================================================================

# ------------------------------------------------------------------
# Stack Configuration (matches main stack pattern)
# ------------------------------------------------------------------
# STACK_NAME: Compose project name (determines container prefixes)
# NETWORK: Docker network for inter-service communication
STACK_NAME=cdb
NETWORK=cdb_network

# ------------------------------------------------------------------
# Ollama LLM Configuration
# ------------------------------------------------------------------
# OPENAI_API_KEY: Must be non-empty for Ollama (placeholder value)
# OPENAI_API_BASE: Ollama API endpoint (inter-container URL)
# OPENAI_MODEL: LLM model for knowledge extraction
# MODEL_NAME: Alternative model name (some configs use this)
OPENAI_API_KEY=ollama
OPENAI_API_BASE=http://cdb_ollama:11434/v1
OPENAI_MODEL=deepseek-r1:7b
MODEL_NAME=deepseek-r1:7b

# Ollama host URL (for external access from host machine)
OLLAMA_HOST_URL=http://localhost:11434

# ------------------------------------------------------------------
# Embedding Configuration
# ------------------------------------------------------------------
# EMBEDDING_MODEL: Model for semantic embeddings
# EMBEDDING_DIM: Embedding dimension (768 for nomic-embed-text)
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIM=768

# ------------------------------------------------------------------
# FalkorDB Configuration
# ------------------------------------------------------------------
# FALKORDB_URI: Redis-compatible connection URI
# DATABASE_PROVIDER: Database backend type
# BROWSER: Enable FalkorDB Browser UI on :3000 (1=enabled, 0=disabled)
#
# NOTE: FalkorDB runs INSIDE the graphiti container at localhost:6379
# Do NOT expose port 6379 to host in production (security)
FALKORDB_URI=redis://localhost:6379
DATABASE_PROVIDER=falkordb
# Set to 1 for debugging (enables UI at :3000), 0 for production
BROWSER=0

# ------------------------------------------------------------------
# Graphiti MCP Server Configuration
# ------------------------------------------------------------------
# GRAPHITI_GROUP_ID: Memory namespace/group for agent memories
# SEMAPHORE_LIMIT: Concurrent episode processing limit (tune for API rate limits)
# GRAPHITI_TELEMETRY_ENABLED: Anonymous telemetry (false for privacy)
# GRAPHITI_TRANSPORT: MCP transport protocol
# GRAPHITI_PORT: MCP server HTTP port
GRAPHITI_GROUP_ID=auto-claude
SEMAPHORE_LIMIT=10
GRAPHITI_TELEMETRY_ENABLED=false
GRAPHITI_TRANSPORT=http
GRAPHITI_PORT=8000

# MCP endpoint URL (for Auto-Claude integration)
GRAPHITI_MCP_URL=http://localhost:8000/mcp/

# ------------------------------------------------------------------
# Docker Image Configuration (Pinned for reproducibility)
# ------------------------------------------------------------------
# NOTE: Using :latest for initial setup; pin with SHA256 digest for production
# Example pinned format: image@sha256:abc123...
OLLAMA_IMAGE=ollama/ollama:latest
GRAPHITI_IMAGE=zepai/graphiti-falkordb:latest

# ------------------------------------------------------------------
# Container Names (cdb_ prefix per project convention)
# ------------------------------------------------------------------
OLLAMA_CONTAINER_NAME=cdb_ollama
GRAPHITI_CONTAINER_NAME=cdb_graphiti

# ------------------------------------------------------------------
# Volume Names (cdb_ prefix per project convention)
# ------------------------------------------------------------------
OLLAMA_VOLUME=cdb_ollama_data
GRAPHITI_VOLUME=cdb_graphiti_data

# ------------------------------------------------------------------
# Service Ports
# ------------------------------------------------------------------
# OLLAMA_PORT: Ollama API port (OpenAI-compatible at /v1)
# GRAPHITI_MCP_PORT: Graphiti MCP HTTP port
# FALKORDB_BROWSER_PORT: FalkorDB Browser UI (debugging only)
OLLAMA_PORT=11434
GRAPHITI_MCP_PORT=8000
FALKORDB_BROWSER_PORT=3000

# ==================================================================
# SERVICE URLS REFERENCE
# ==================================================================
# After starting the stack, services are available at:
#
# - Graphiti MCP: http://localhost:8000/mcp/
# - Ollama API: http://localhost:11434/v1
# - FalkorDB Browser: http://localhost:3000 (if BROWSER=1)
#
# Inter-container URLs (Docker network):
# - Ollama: http://cdb_ollama:11434/v1
# - FalkorDB: redis://localhost:6379 (internal to graphiti container)
#
# ==================================================================
# CONFIGURATION CHECKLIST BEFORE FIRST RUN:
# ==================================================================
# [ ] Copy this file: cp .env.memory.example .env.memory
# [ ] Verify OPENAI_API_KEY=ollama (required, non-empty)
# [ ] Verify GRAPHITI_GROUP_ID is set (default: auto-claude)
# [ ] Run: docker compose -f infrastructure/compose/memory.yml up -d
# [ ] Run: ./infrastructure/scripts/init-memory.sh (pulls models, waits for health)
# [ ] Test: curl http://localhost:8000/health
# [ ] Test: curl http://localhost:11434/api/tags
# ==================================================================
