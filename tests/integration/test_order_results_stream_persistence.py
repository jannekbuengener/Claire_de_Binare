import jsonimport loggingimport osimport timefrom datetime import datetime, timezonefrom urllib import requestfrom urllib.error import URLError, HTTPErrorimport psycopg2import pytestimport redisfrom core.domain.event import CANONICAL_ORDER_RESULTS_STREAMfrom core.utils.uuid_gen import generate_uuid_hexfrom tests.e2e.support import reset_circuit_breaker # Re-use existing e2e supportlogger = logging.getLogger(__name__)# --- Test Helpers (adapted from e2e tests) ---def _load_dotenv_for_test() -> None:    try:        from dotenv import load_dotenv    except ModuleNotFoundError:        return    load_dotenv(override=False)def _http_get_json(url: str, timeout: float = 2.0) -> dict:    try:        with request.urlopen(url, timeout=timeout) as response:            payload = response.read()    except (HTTPError, URLError) as exc:        raise RuntimeError(f"HTTP request failed for {url}: {exc}") from exc    return json.loads(payload.decode("utf-8"))def _get_redis_client():    _load_dotenv_for_test()    redis_host = os.getenv("REDIS_HOST", "localhost")    redis_port = int(os.getenv("REDIS_PORT", "6379"))    redis_password = os.getenv("REDIS_PASSWORD")    redis_client = redis.Redis(        host=redis_host,        port=redis_port,        password=redis_password or None,        decode_responses=False, # We expect bytes from streams    )    redis_client.ping()    return redis_clientdef _get_pg_connection():    _load_dotenv_for_test()    pg_host = os.getenv("POSTGRES_HOST", "localhost")    pg_port = int(os.getenv("POSTGRES_PORT", "5432"))    pg_db = os.getenv("POSTGRES_DB", "claire_de_binare")    pg_user = os.getenv("POSTGRES_USER", "claire_user")    pg_password = os.getenv("POSTGRES_PASSWORD")    if not pg_password:        pytest.skip("POSTGRES_PASSWORD not set for E2E tests.")        pg_conn = psycopg2.connect(        host=pg_host,        port=pg_port,        database=pg_db,        user=pg_user,        password=pg_password,    )    pg_conn.autocommit = True # Ensure changes are committed immediately    return pg_conndef _publish_canonical_order_result(    redis_client,    symbol: str,    status: str,    quantity: float,    price: float,    event_id: str | None = None,    bot_id: str | None = None,    fill_id: str | None = None,    exchange_order_id: str | None = None,    commission: float = 0.0,    commission_asset: str = "USDT",    side: str = "BUY",) -> str:    current_time_iso = datetime.now(timezone.utc).isoformat(timespec='milliseconds') + 'Z'    event_id = event_id if event_id is not None else generate_uuid_hex()    fill_id = fill_id if fill_id is not None else generate_uuid_hex()        canonical_payload = {        "event_id": event_id,        "ts": current_time_iso,        "bot_id": bot_id if bot_id is not None else "test_bot",        "symbol": symbol,        "order_id": exchange_order_id if exchange_order_id is not None else f"e2e_exchange_order_{int(time.time() * 1000)}",        "client_order_id": f"e2e_client_order_{int(time.time() * 1000)}",        "result_status": status.upper(),        "side": side,        "price": price,        "quantity": quantity,        "fill_id": fill_id,        "fill_timestamp": current_time_iso,        "commission": commission,        "commission_asset": commission_asset,        "payload": json.dumps({ # Nested original payload            "type": "order_result",            "status": status,            "filled_quantity": quantity,            "order_type": "market",            "execution_price": price,            "exchange": "MEXC",            "exchange_order_id": exchange_order_id if exchange_order_id is not None else f"e2e_exchange_order_{int(time.time() * 1000)}",            "error_message": None,            "source_service": "integration_test",            "reject_reason_code": None,            "reject_stage": None,            "causing_event_id": None,            "metadata": {},        })    }        cleaned_payload = {}    for k, v in canonical_payload.items():        if v is not None:            cleaned_payload[k] = str(v)    redis_client.xadd(CANONICAL_ORDER_RESULTS_STREAM, cleaned_payload, maxlen=10000)    return event_iddef _wait_for_db_trade_row(    pg_conn,    trade_event_id: str,    timeout_s: float = 10,) -> dict | None:    deadline = time.time() + timeout_s    while time.time() < deadline:        with pg_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:            cursor.execute(                f"SELECT * FROM trades WHERE trade_event_id = %s",                (trade_event_id,),            )            result = cursor.fetchone()            if result:                return result        time.sleep(0.5)    return Nonedef _count_db_trade_rows(pg_conn, trade_event_id: str) -> int:    with pg_conn.cursor() as cursor:        cursor.execute(            f"SELECT COUNT(*) FROM trades WHERE trade_event_id = %s",            (trade_event_id,),        )        return int(cursor.fetchone()[0])@pytest.fixture(scope="module")def integration_env():    _load_dotenv_for_test()        redis_client = None    pg_conn = None    try:        redis_client = _get_redis_client()        pg_conn = _get_pg_connection()    except Exception as exc:        pytest.fail(f"Failed to connect to Redis or Postgres: {exc}")    # Ensure clean state for streams and consumer groups    try:        redis_client.delete(CANONICAL_ORDER_RESULTS_STREAM)        redis_client.xgroup_destroy(CANONICAL_ORDER_RESULTS_STREAM, "db_writer_group")        redis_client.xgroup_destroy(CANONICAL_ORDER_RESULTS_STREAM, "risk_group")        logger.info(f"Cleaned up stream {CANONICAL_ORDER_RESULTS_STREAM} and its consumer groups.")    except Exception as exc:        logger.warning(f"Failed to clean up Redis stream/groups (might not exist): {exc}")    yield {        "redis": redis_client,        "pg": pg_conn,    }    if pg_conn:        pg_conn.close()    if redis_client:        redis_client.close()# --- Test Cases ---@pytest.mark.integrationdef test_it_001_execution_to_db_writer_persistence(integration_env):    """    IT-001: Verifies that an order result published to the Redis Stream by execution    is correctly consumed by db_writer and persisted into the trades table.    """    redis_client = integration_env["redis"]    pg_conn = integration_env["pg"]    symbol = "IT_001_BTCUSDT"    quantity = 0.001    price = 30000.0    status = "FILLED"    event_id = _publish_canonical_order_result(redis_client, symbol, status, quantity, price)    logger.info(f"Published order result with event_id: {event_id}")    # Wait for db_writer to process and persist    persisted_trade = _wait_for_db_trade_row(pg_conn, event_id, timeout_s=15)    assert persisted_trade is not None, f"Trade with event_id {event_id} was not persisted to DB."    assert persisted_trade["symbol"] == symbol    assert persisted_trade["status"] == status.lower()    assert float(persisted_trade["size"]) == quantity    assert float(persisted_trade["execution_price"]) == price    assert persisted_trade["trade_event_id"] == event_id@pytest.mark.integrationdef test_it_002_duplicate_event_id_idempotency(integration_env):    """    IT-002: Verifies that a duplicate order result event (same trade_event_id)    is persisted only once due to ON CONFLICT DO NOTHING.    """    redis_client = integration_env["redis"]    pg_conn = integration_env["pg"]    symbol = "IT_002_ETHUSDT"    quantity = 0.01    price = 2000.0    status = "FILLED"    test_event_id = generate_uuid_hex() # Specific event ID for idempotency test    # Publish the same event twice    _publish_canonical_order_result(redis_client, symbol, status, quantity, price, event_id=test_event_id)    logger.info(f"Published first order result with event_id: {test_event_id}")    _publish_canonical_order_result(redis_client, symbol, status, quantity, price, event_id=test_event_id)    logger.info(f"Published second (duplicate) order result with event_id: {test_event_id}")    # Wait for db_writer to process    _wait_for_db_trade_row(pg_conn, test_event_id, timeout_s=15)    # Count rows for the specific event_id    trade_count = _count_db_trade_rows(pg_conn, test_event_id)    assert trade_count == 1, f"Expected 1 trade entry for event_id {test_event_id}, but found {trade_count}."@pytest.mark.integrationdef test_it_003_stream_pending_entries_processing(integration_env):    """    IT-003: Verifies that db_writer correctly processes pending stream entries    after a simulated consumer restart.    This test requires manual intervention or a very controlled shutdown of db_writer,    which is hard to simulate cleanly in a single pytest run without complex orchestration.    For now, this test will verify the setup and basic message processing.    A more advanced version would involve stopping the db_writer service mid-processing.    """    redis_client = integration_env["redis"]    pg_conn = integration_env["pg"]    symbol = "IT_003_SOLUSDT"    quantity = 0.5    price = 100.0    status = "FILLED"        # Ensure consumer group is created by db_writer (if not already there)    # This test will publish, and assume db_writer will pick it up        event_id_1 = _publish_canonical_order_result(redis_client, symbol, status, quantity, price, bot_id="test_bot_it003_1")    event_id_2 = _publish_canonical_order_result(redis_client, symbol, status, quantity, price, bot_id="test_bot_it003_2")    logger.info(f"Published order results with event_ids: {event_id_1}, {event_id_2}")    # Simply check if they are both persisted.    # The actual "pending entries" logic is handled by the xclaim in db_writer,    # and would be implicitly tested by a full restart.    persisted_trade_1 = _wait_for_db_trade_row(pg_conn, event_id_1, timeout_s=15)    persisted_trade_2 = _wait_for_db_trade_row(pg_conn, event_id_2, timeout_s=15)        assert persisted_trade_1 is not None, f"Trade {event_id_1} not persisted."    assert persisted_trade_2 is not None, f"Trade {event_id_2} not persisted."        logger.info("IT-003: Both events persisted. Pending entry processing is implicitly handled by db_writer's xreadgroup/xclaim logic.")