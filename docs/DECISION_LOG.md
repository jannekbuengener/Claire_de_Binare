# Architektur-Entscheidungen (ADR-Style)

## ADR-009: Security Rerun Automation & Evidence Pipeline

**Datum**: 2025-11-11  
**Status**: ‚úÖ Abgeschlossen  
**Kontext**: Sicherheits-Gates blockierten Releases, da Artefakte (Trivy, Gitleaks, Bandit) nicht konsolidiert waren und Nachweise (.env Hash, CVE-Vergleich, Reviewer-Checkliste) fehlten.

-**Entscheidung**: 
- Automatisierte Skripte (`scripts/scan_ports.py`, `scripts/log_parser.py`, `scripts/bandit_postprocess.py`, `scripts/verify_cve_fix.sh`, `scripts/run_hardening.py`, `scripts/cve_triage.py`) generieren Ports-, Logs-, Bandit- und CVE-Artefakte unter `artifacts/`.
- Neues Makefile erweitert um Guarded Targets (`deps_fix`, `trivy_local`, `trivy_triage`, `bandit`, `bandit_gate`, `gitleaks`, `gitleaks_gate`, `verify_cve`, `evidence_review`, `gates`); optionaler Registry-Vergleich via `REGISTRY_IMG` (`make trivy_registry` dokumentiert als Plan).
- `.github/REVIEW_TEMPLATE.md` standardisiert Reviewer-Checkpunkte inkl. Bandit-`justified`-Abnahme und Artefakt-Links.
- `requirements.lock` (per `pip freeze --require-virtualenv`) dient Audit-Nachweis; Evidence-Datei enth√§lt SHA256 der lokalen `.env` und Gitignore-Kontrolle.

-**Ergebnis**:
- Trivy- und Pip-Audit-Daten werden √ºber `scripts/verify_cve_fix.sh` abgeglichen (Pins `aiohttp==3.12.14`, `cryptography==42.0.4` best√§tigt, HIGH/CRITICAL f√ºr lokale Images aktuell 120; `scripts/cve_triage.py` liefert JSON/Markdown-Matrix zur weiteren Triage).
- Bandit-Report erh√§lt `justified`-Feld (Mapping via `scripts/bandit_justification.json` m√∂glich); `unjustified_check.json` zeigt aktuell 37 offene HIGH/MEDIUM-Funde und blockiert Gates bis zur Abnahme.
- Gitleaks l√§uft mit gepflegter `.gitleaks.toml`; sowohl Prim√§r- als auch Post-Clean-Scan liefern 0 Treffer, Gate bleibt gr√ºn.
- Ports-Scan & Log-Parser erzeugen JSON/Markdown-Artefakte f√ºr Reviewer; Evidence-Skript schreibt `evidence/TEST_RERUN_EVIDENCE_<DATE>.md` inklusive automatisiertem Review-Block und PR-Draft unter `artifacts/pr/`.
- Dokumentierter Plan: Registry-Scan (`make trivy_registry`) bleibt optional, Ergebnisse sollen k√ºnftig gegen lokale Pins verglichen und im Evidence-Text referenziert werden.

**Konsequenzen**:
- ‚ûï Wiederholbare Security-Runs mit einheitlichem Artefakt-Layout (`artifacts/security/*`, `artifacts/runtime/*`).
- ‚ûï Reviewer-Workflow beschleunigt (Checkliste + `justified`-Flag als Pflichtpr√ºfung).
- ‚ûï CVE-Evidence kombiniert lokale Scans (Trivy) und Dependency-Audits (pip-audit, safety) mit JSON-Zusammenfassung.
- ‚ûñ Trivy meldet aktuell 120 HIGH/CRITICAL Findings in Basis-Images ‚Üí Folgeaufgabe: Registry-Scan + Upstream-Fix-Analyse.
- üîÑ N√§chste Schritte: `scripts/bandit_justification.json` pflegen (false-positive Tracking) und bei Verf√ºgbarkeit `REGISTRY_IMG` setzen, um lokale vs. Registry-Images im Evidence zu vergleichen.

## ADR-008: Tool Stack - Development & Management Tools

**Datum**: 2025-11-03  
**Status**: ‚úÖ Abgeschlossen  
**Kontext**: Nach Implementierung von CDB (Business Logic) und MCP (Monitoring) fehlten Verwaltungs- und Entwicklungstools f√ºr effizientes Container-Management, Datenbank-Administration und Ressourcen-√úberwachung.

**Entscheidung**: Separater Tool-Stack mit 5 spezialisierten Tools:

1. **Portainer** (portainer-ce:latest) - Docker Management UI
   - Container, Images, Volumes, Networks verwalten
   - Terminal-Zugriff (exec) in Container
   - Stack-Management & Logs
   
2. **pgAdmin** (dpage/pgadmin4:latest) - PostgreSQL UI
   - Vollst√§ndige Datenbank-Administration f√ºr cdb_postgres
   - Query-Tool mit Syntax-Highlighting
   - Backup/Restore-Funktionen
   
3. **Dozzle** (amir20/dozzle:latest) - Docker Logs Viewer
   - Real-time Log-Streaming aller Container
   - Multi-Container-Suche mit Regex
   - Kein Login n√∂tig (localhost-only)
   
4. **Adminer** (adminer:latest) - Lightweight SQL UI
   - Schnelle DB-Queries ohne pgAdmin-Overhead
   - Single-File PHP App
   - Unterst√ºtzt PostgreSQL, MySQL, SQLite
   
5. **cAdvisor** (gcr.io/cadvisor/cadvisor:latest) - Resource Monitoring
   - Container CPU/Memory/Network/Disk-Usage
   - Live-Metriken & historische Graphen
   - Prometheus-Integration (Scrape-Target)

**Begr√ºndung**:

- **Naming Convention:** Alle Container mit `tool_` Pr√§fix f√ºr sofortige Identifikation (analog zu `cdb_` und `mcp_`)
- **Dual-Network:** Alle Tools h√§ngen in `tools_net` (intern) UND `cdb_network` (shared) f√ºr direkten Zugriff auf CDB/MCP-Services
- **No Authentication (localhost):** Dozzle und cAdvisor ohne Login, da nur auf localhost exponiert (Production: Reverse-Proxy mit Auth)
- **cAdvisor statt Prometheus Node-Exporter:** cAdvisor bietet Container-spezifische Metriken, Node-Exporter nur Host-Metriken

**Ports (alle localhost):**
- 9000: Portainer
- 5050: pgAdmin
- 9999: Dozzle (Logs)
- 8085: Adminer (SQL)
- 8080: cAdvisor

**Implementierung**:

- Compose-Datei: `docker/tools/docker-compose.tools.yml`
- Environment: `docker/tools/.env` (pgAdmin-Credentials)
- Volumes: `tool_portainer_data`, `tool_pgadmin_data` (persistent)
- Labels: `com.cdb.role=tool`, `com.cdb.service=<name>` f√ºr alle Container

**Ergebnis**:

- 5 Tool-Container operational
- Direkte Verbindung zu cdb_postgres (pgAdmin, Adminer)
- Real-time Logs aller CDB/MCP-Container (Dozzle)
- Container-Metriken in Prometheus (cAdvisor @ tool_resourceusage:8080)
- Deployment-Script: `docker/tools/deploy.ps1` (Pre-Flight Checks, Backup, Health-Checks)

**Konsequenzen**:

- ‚ûï **Developer Experience:** Grafische UIs statt CLI (pgAdmin > psql, Portainer > docker ps)
- ‚ûï **Debugging:** Dozzle erm√∂glicht schnelle Log-Suche √ºber alle Container (kein `docker logs` n√∂tig)
- ‚ûï **Resource-Awareness:** cAdvisor zeigt Memory-Leaks und CPU-Spikes sofort
- ‚ûï **Self-Service:** Entwickler k√∂nnen ohne Root-Zugriff Container verwalten (Portainer)
- ‚ûñ **Zus√§tzliche Ressourcen:** 5 Container ben√∂tigen ~500 MB RAM
- ‚ö†Ô∏è **Security:** Portainer/pgAdmin Passw√∂rter in `.env` (nicht committed), localhost-only Exposition empfohlen

**Integration mit MCP:**

```yaml
# In docker/mcp/prometheus/prometheus.yml:
- job_name: 'cadvisor'
  static_configs:
    - targets: ['tool_resourceusage:8080']
```

‚Üí Container-Metriken direkt in Prometheus & Grafana verf√ºgbar

**Dokumentation**: `docker/tools/README_TOOLS.md` (vollst√§ndige Tool-Beschreibungen, Setup-Guides, Troubleshooting)

**Referenzen**:
- Portainer: https://docs.portainer.io/
- pgAdmin: https://www.pgadmin.org/docs/
- Dozzle: https://github.com/amir20/dozzle
- Adminer: https://www.adminer.org/
- cAdvisor: https://github.com/google/cadvisor

---

## ADR-007: MCP Observability Stack - Monitoring & Alerting

**Datum**: 2025-11-03  
**Status**: ‚úÖ Abgeschlossen  
**Kontext**: Nach Docker MVP (ADR-006) fehlte vollst√§ndige Observability-Infrastruktur f√ºr Metriken, Logs und Alerts. Produktions-Readiness erfordert Monitoring aller 8 CDB-Services, Alert-Pipeline und Log-Aggregation.

**Entscheidung**: Separater MCP (Monitoring/Control-Plane) Stack mit folgenden Komponenten:
- **Prometheus** (v2.54.1) - Metriken-Sammlung, 15d Retention
- **Alertmanager** (v0.27.0) - Alert-Routing, Slack-Integration
- **Grafana** (11.3.0) - Visualisierung
- **Loki** (3.2.0) - Log-Aggregation, 15d Retention
- **Promtail** (3.2.0) - Docker-Log-Collection
- **Redis Exporter** (v1.63.0) - Redis-Metriken
- **Postgres Exporter** (v0.15.0) - PostgreSQL-Metriken

**Begr√ºndung**:
- Separate Compose-Datei (`docker-compose.observability.yml`) f√ºr klare Trennung von Business-Logic (CDB) und Observability (MCP)
- Shared Network (`cdb_network`) f√ºr direkte Service-Discovery ohne Port-Exposition
- Prefix `mcp_` f√ºr alle MCP-Container zur sofortigen Identifikation
- 15-Tage-Retention als Balance zwischen Disk Space und Compliance
- Slack-Integration f√ºr Alert-Routing (Critical, Warning, Infrastructure)

**Implementierung**:
1. **Alert Rules (15+ konfiguriert)**:
   - ServiceDown, HighCPU, HighMemory, DiskSpaceLow
   - RedisBackpressure (evicted_keys > 100 oder memory > 80%)
   - PostgreSQLDown, PrometheusDown, LokiDown
   - NoAlertsReceived (Watchdog-Meta-Alert)

2. **Automation Scripts (PowerShell)**:
   - `deploy.ps1` - Full-Deployment mit Pre-Flight Checks
   - `sanity-check.ps1` - 8 Validierungskategorien (Container, API, Volumes, Network)
   - `fire-drill.ps1` - Alert-Pipeline-Test (Alertmanager ‚Üí Slack)
   - `test-log-pipeline.ps1` - Loki-Ingestion-Validierung

3. **Dokumentation**:
   - `README.md` (10+ Seiten) - Vollst√§ndige Referenz mit Mini-Runbooks f√ºr 5 h√§ufige Alerts
   - `QUICK_START.md` - 5-Minuten-Installation mit Slack-Setup
   - Troubleshooting-Guides f√ºr ServiceDown, RedisBackpressure, PrometheusDown, LokiDown

**Ergebnis**:
- 7 MCP-Container operational (Prometheus, Alertmanager, Grafana, Loki, Promtail, Redis Exporter, Postgres Exporter)
- 10+ Prometheus-Targets konfiguriert (CDB-Services, Redis, PostgreSQL, MCP-Services selbst)
- Slack-Integration aktiv (3 Alert-Kategorien: critical, warning, infrastructure)
- Log-Pipeline validiert (Docker ‚Üí Promtail ‚Üí Loki ‚Üí Grafana Explore)
- Fire-Drill-Tests bestanden (Alert-Fire & Resolve funktionsf√§hig)
- Retention: 15 Tage f√ºr Prometheus + Loki

**Konsequenzen**:
- ‚ûï **Produktions-Readiness**: Vollst√§ndige Observability f√ºr alle CDB-Services
- ‚ûï **Proaktive Alerts**: Slack-Benachrichtigungen bei Service-Problemen (< 1min Latenz)
- ‚ûï **Root-Cause-Analysis**: Logs in Loki + Metriken in Prometheus erm√∂glichen schnelles Debugging
- ‚ûï **Automatisierte Validierung**: Sanity-Checks in < 60 Sekunden durchf√ºhrbar
- ‚ûï **Self-Monitoring**: MCP √ºberwacht sich selbst (PrometheusDown, LokiDown Alerts)
- ‚ûñ **Zus√§tzliche Ressourcen**: 7 Container ben√∂tigen ~1-2 GB RAM und ~500 MB Disk pro Tag
- üîÑ **N√§chste Schritte**: Grafana-Dashboards importieren, Alert-Tuning nach Produktion-Load

**Technische Details**:
- **Netzwerk**: Shared `cdb_network` (bridge) - keine separaten Netze, direkte Service-Discovery
- **Volumes**: 3 persistente Volumes (`mcp_prometheus_data`, `mcp_grafana_data`, `mcp_loki_data`)
- **Ports**: 9090 (Prometheus), 9093 (Alertmanager), 3000 (Grafana), 3100 (Loki), 9080 (Promtail)
- **Secrets**: Credentials in `.env` (nicht committed), Template in `.env.example`
- **Health-Checks**: Alle Container mit Health-Check konfiguriert (interval: 30s, timeout: 10s)

**Dokumentation**: `docker/mcp/README.md`, `docker/mcp/QUICK_START.md`, `backoffice/CHECKPOINT_INDEX.md` (MCP-Abschnitt)

**Referenzen**:
- Prometheus-Dokumentation: https://prometheus.io/docs/
- Loki-Dokumentation: https://grafana.com/docs/loki/
- Alertmanager-Routing: https://prometheus.io/docs/alerting/latest/configuration/

---

## ADR-006: Docker MVP Complete - Checkpoint Reset/Joined

**Datum**: 2025-11-03  
**Status**: ‚úÖ Abgeschlossen  
**Kontext**: Vollst√§ndige Implementierung aller 6 Kern-Services mit Docker, inklusive Health-Checks, korrekter ENV-Konfiguration und vollst√§ndigem DB-Schema.  
**Entscheidung**: Alle Services mit `cdb_` Pr√§fix, einheitliche Port-Struktur (8000-8003 f√ºr Services), vollst√§ndige Healthcheck-Integration.  
**Ergebnis**:
- 8 Container running & healthy (redis, postgres, prometheus, grafana, ws, core, risk, execution)
- 6 persistente Volumes
- 11 DB-Tabellen/Views geladen
- Alle ENV-Keys vollst√§ndig konfiguriert
- Health-Endpoints auf allen Services verf√ºgbar

**Dokumentation**: `backoffice/CHECKPOINT_RESET_JOINED_2025-11-03.md`  
**Konsequenzen**:
- ‚ûï Stabiler Ausgangspunkt f√ºr E2E-Tests
- ‚ûï Vollst√§ndige Nachvollziehbarkeit aller Build-Artefakte
- ‚ûï Klare Service-Hierarchie und Dependencies
- üîÑ N√§chster Schritt: Redis Pub/Sub Tests & Pipeline-Validierung

---

## ADR-001: Message-Bus-Wahl (Redis statt NATS)

**Datum**: 2025-01-XX  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Brauchten Pub/Sub f√ºr Service-Kommunikation  
**Entscheidung**: Redis (simpler Setup, direkt in Docker)  
**Konsequenzen**:

- ‚ûï Kein zus√§tzlicher Infra-Stack
- ‚ûï Persistenz m√∂glich (List/Stream)
- ‚ûñ Weniger Features als NATS (kein Clustering)

## ADR-002: SQLite f√ºr MVP

**Datum**: 2025-01-XX  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Datenbank f√ºr Audit-Trail  
**Entscheidung**: SQLite embedded, sp√§ter PostgreSQL  
**Konsequenzen**:
- ‚ûñ Single-Writer-Limitation
- üîÑ Migration auf Postgres bei Multi-Instance

## ADR-003: Telegram-Alerts deprecated

**Kontext**: Roadmap fordert interne Push-L√∂sung  
**Entscheidung**: Prim√§r Web-Push (VAPID), Telegram nur Legacy  
**Konsequenzen**:

- ‚ûï Datenschutz (kein Drittanbieter-Zwang)
- ‚ûï Konsistent mit Roadmap-Vision

## ADR-004: Backup-Skripte zentral in operations/backup

**Datum**: 2025-10-25  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Mehrere Backup-Skripte/Anleitungen existierten doppelt im Repository und f√ºhrten zu veralteten Pfadangaben.  
**Optionen**:  

- A) Alles im Projekt-Root behalten  
- B) Skripte und Doku unter `operations/backup/` b√ºndeln  
- C) Externes Repo nur f√ºr Betrieb anlegen  
**Entscheidung**: Option B ‚Äì alle aktiven Skripte/Dokumente liegen unter `operations/backup/`, Root-Dateien bleiben als Weiterleitung bzw. Legacy-Hinweis bestehen.

## ADR-005: compose.yaml Removal - Nur docker-compose.yml verwenden

**Datum**: 2025-10-30  
**Status**: ‚úÖ Beschlossen  
**Kontext**: System hatte zwei konkurrierende Docker Compose Konfigurationen (`docker-compose.yml` + `compose.yaml`), die parallel liefen und zu Restart-Loops aller Python-Services f√ºhrten.

**Problem**:
- Docker Compose bevorzugt automatisch `compose.yaml` √ºber `docker-compose.yml` (neuere Namenskonvention)
- Beide Container-Sets versuchten parallel zu laufen (Port-Konflikte 8001-8003)
- `compose.yaml` hatte fehlerhafte Network-Definition ‚Üí DNS-Aufl√∂sung fehlgeschlagen
- 90 Minuten Downtime f√ºr Signal Engine, Risk Manager, Execution Service

**Optionen**:
- A) `compose.yaml` fixen und als prim√§re Config verwenden (kurze Namen: cdb-exec:v1)
- B) `docker-compose.yml` behalten, `compose.yaml` entfernen (lange Namen: claire_de_binare-*)
- `docker-compose.yml` war bereits funktionsf√§hig und stabil (alle Services healthy)
- Kurze Container-Namen sind Nice-to-Have, aber System-Stabilit√§t ist kritischer
- Eine einzige Source of Truth verhindert zuk√ºnftige Konflikte
**Implementation**:
```bash
docker rm -f cdb-exec cdb-risk cdb-signal  # St√∂rende Container entfernen
**Validation**:
- ‚úÖ Alle Services healthy innerhalb 2 Minuten
- ‚úÖ Health-Endpoints antworten korrekt
- Recovery Report: `backoffice/audits/2025-10-30_RECOVERY_REPORT.md`
- Funktionierende Config: `docker-compose.yml` (Root-Verzeichnis)  
**Konsequenzen**:
- ‚ûñ Benutzer m√ºssen neuen Pfad kennen (wird in Root-Docs kommuniziert)

## ADR-005: Unix-Timestamp f√ºr Datenbank-Zeitstempel
**Problem**: Code verwendete `datetime.utcnow()` (Python datetime-Objekt), DB-Schema erwartet `bigint` (Unix-Timestamp)  
**Optionen**:  

- A) DB-Schema √§ndern zu `timestamp without time zone`  
- B) Code √§ndern zu `int(time.time())` (Unix-Timestamp)  
- C) Beide Formate hybrid unterst√ºtzen

**Entscheidung**: Option B ‚Äì Code auf `int(time.time())` umgestellt  
**Rationale**:

- DB-Schema ist bewusst mit `bigint` designed (EVENT_SCHEMA.json Standard)
- Unix-Timestamps sind plattform√ºbergreifend eindeutig
- `save_order()`: `submitted_at` und `filled_at` auf `int(time.time())` umgestellt
- Bestehende `save_trade()` bereits korrekt (konvertiert ISO-String zu Unix)

- ‚ûï Konsistenz zwischen Events und DB
- ‚ûï E2E Test-Success-Rate: 90% ‚Üí 100%
- ‚ûñ Keine (Code war fehlerhaft, DB-Schema korrekt)
**Status**: ‚úÖ Beschlossen  
**Kontext**: Mehrere Komponenten (Apprise-Alerts, MCP-Dokument, Master-√úbersicht) sind durch neuere Strukturen ersetzt worden und f√ºhren zu Verwirrung/Duplikaten.  
**Optionen**:  
**Entscheidung**: Option B ‚Äì Komponenten werden in `archive/` verschoben mit README zur Dokumentation der Gr√ºnde und Archivierungsdaten.  
**Konsequenzen**:

- ‚ûï Git-Historie bleibt erhalten, kein Datenverlust  
- ‚ûï Nachvollziehbare Projektentscheidungen  
- ‚ûï Sauberer Root-Ordner ohne veraltete Dateien  
- ‚ûñ Zus√§tzlicher Verwaltungsaufwand f√ºr Archiv-Dokumentation

## ADR-006: Governance-Ordner & Leitplanken

**Datum**: 2025-10-25  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Wiederkehrende Audit-Feststellungen (ENV-Duplikate, fehlende Logging-Standards) verlangten nach klaren Strukturen f√ºr Automatisierung, Templates und CI.  
**Optionen**:  

- A) Bestehende Dateien erweitern und verstreut ablegen  
- B) Neue Ordner unter `backoffice/` schaffen (`automation/`, `ci/`, `templates/`) und Regeln in separatem Dokument pflegen  
- C) Externes Wiki verwenden  
**Entscheidung**: Option B ‚Äì dedizierte Governance-Ordner plus `docs/ARCHITEKTUR_REGELN.md` als Verbindlichkeit f√ºr Services.  
**Konsequenzen**:  

- ‚ûï Klare Ablageorte f√ºr Skripte, Pipelines und Vorlagen  
- ‚ûï Architektur- und Logging-Regeln sind zentral versioniert  
- ‚ûñ Initialer Pflegeaufwand (Templates/Skripte m√ºssen gef√ºllt werden)  

## ADR-007: Automatisiertes Repository-Inventar

**Datum**: 2025-10-25  
**Status**: ‚úÖ Beschlossen  
**Kontext**: KI-Agenten verlieren Zeit beim manuellen Erfassen des Dateibestands; Audits verlangen nachvollziehbare Snapshots pro Session.  
**Optionen**:  

- A) Rein manuelle Sichtpr√ºfung der Ordnerstruktur  
- B) Nutzung vorhandener Backup-Skripte f√ºr Inventarinformationen  
- C) Eigenst√§ndiges Repository-Inventar-Skript mit JSON-Ausgabe in `backoffice/logs/inventory/`  
**Entscheidung**: Option C ‚Äì dediziertes Skript `scripts/inventory.ps1`, das bei Session-Start ein Inventar schreibt und `latest.json` f√ºr schnelle Diffs bereitstellt.  
**Konsequenzen**:  

- ‚ûï Einheitliche Start-Routine f√ºr alle Agenten  
- ‚ûï Nachvollziehbarkeit von Struktur√§nderungen √ºber JSON-Historie  
- ‚ûñ Leichter Pflegeaufwand f√ºr Skript bei Struktur√§nderungen  

## ADR-008: Geheimnisrotation & Container-Hardening

**Datum**: 2025-10-25  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Audit 2025-10-25 identifizierte ungesch√ºtzte Redis-/Postgres-Zug√§nge sowie Root-Container ohne Hardening. Risiko: Order-Manipulation, Datenverlust, Privilege Escalation.  
**Optionen**:  

- A) Nur Dokumentation erg√§nzen und manuelle Erinnerung an Secret-Rotation  
- B) Compose/Dockerfiles h√§rten, Secrets erzwingen, Host-Exponierung einschr√§nken  
- C) Komplettumstieg auf Managed Services mit externem Secret-Store  
**Entscheidung**: Option B ‚Äì unmittelbare technische Absicherung durch Pflicht-ENV-Variablen, `--requirepass` f√ºr Redis, entfernte Passwort-Fallbacks, Non-Root-Execution-Service und Security-Optionen in Compose.  
**Konsequenzen**:  
- ‚ûï Reduzierte Angriffsfl√§che, Redis/Postgres nur mit g√ºltigem Secret erreichbar  
- ‚ûï Container laufen ohne Root-Capabilities (`no-new-privileges`, `cap_drop`, Non-Root-User)  
- ‚ûñ Betreiber m√ºssen Secrets vor Deploy setzen; fehlende Variablen verhindern Start (Intentional Fail-Fast)  

## ADR-009: Execution-Feedback im Risk-Loop

**Datum**: 2025-10-25  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Der Execution-Service publiziert `order_result` Events, der Risk-Manager nutzte diese bislang nicht. Exposure-Limits und Circuit-Breaker reagierten daher nicht auf tats√§chliche Ausf√ºhrungen, was auditrelevante L√ºcken lie√ü.  
**Optionen**:  

- A) Weiterhin nur Signal-Events ber√ºcksichtigen und Exposure manuell resetten  
- B) Risk-Manager erweitert um Listener f√ºr `order_result`, Aktualisierung von Exposure/Pending Orders  
- C) Separaten Persistenz-Service vorsehen, der Limits periodisch neu berechnet  
**Entscheidung**: Option B ‚Äì direkter Listener im Risk-Manager synchronisiert Pending Orders, Positions-Exposure und Execution-Rejections in Echtzeit.  
**Konsequenzen**:  
- ‚ûï Exposure- und Circuit-Breaker-Logik basiert auf real ausgef√ºhrten Orders  
- ‚ûï Einheitliche Metriken (`order_results_received`, `orders_rejected_execution`) erlauben Monitoring  
- ‚ûñ Zus√§tzlicher Redis-Listener/Thread erh√∂ht Komplexit√§t minimal  

## ADR-010: Docker Compose als Standard-Orchestrierung

**Datum**: 2025-10-25  
**Status**: ‚úÖ Best√§tigt  
**Kontext**: Diskussion, ob Docker Desktop ohne Compose-Befehle ausreicht. Die Plattform umfasst mehrere Container (Redis, Postgres, Prometheus, Grafana, Services) mit gemeinsamen Netzwerken/Volumes.  
**Optionen**:  

- A) Reine Docker-Desktop-GUI oder Einzel-`docker run` Kommandos  
- B) Docker Desktop inklusive CLI `docker compose` als verbindlicher Weg  
- C) Alternative Orchestrierung (k3s, Nomad)  
**Entscheidung**: Option B ‚Äì Docker Desktop bleibt Voraussetzung, Compose-CLI wird verbindlich f√ºr Mehrcontainer-Start/Stop/Tests verwendet.  
**Konsequenzen**:  
- ‚ûï Einheitliche Skripte und Doku bleiben g√ºltig (`docker compose up ‚Ä¶`)  
- ‚ûï Health-/Security-Checks (Audit 2025-10-25) lassen sich automatisiert ausf√ºhren  
- ‚ûñ Bedienung ohne CLI nicht unterst√ºtzt; reine GUI-Nutzung bleibt optional f√ºr Einzelcontainer  

## Template f√ºr neue ADRs

### ADR-XXX: [Titel]

**Datum**: YYYY-MM-DD  
**Status**: üîÑ Vorgeschlagen / ‚úÖ Beschlossen / ‚ùå Verworfen  
**Kontext**: Warum brauchen wir eine Entscheidung?  
**Optionen**: A, B, C...  
**Entscheidung**: Wir w√§hlen X weil...  
**Konsequenzen**: Pro/Contra, Risiken

## ADR-011: Vereinheitlichung DB-Credentials und Prometheus-Healthcheck

**Datum**: 2025-10-26  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Postgres-Container startete mit bestehendem Datenverzeichnis; Credentials aus `.env` und realer DB-Instanz wichen ab. Zudem war der Prometheus-Healthcheck im Compose mit `curl` definiert, das im `prom/prometheus`-Image nicht verf√ºgbar ist.  
**Optionen**:


- A) Passwort des bestehenden DB-Benutzers im laufenden Container angleichen  
- B) Postgres-Volume neu initialisieren und User/Pass aus `.env` √ºbernehmen  
- C) Compose an `.env` koppeln (POSTGRES_USER variabel) und Prometheus-Healthcheck auf `wget` umstellen  

**Entscheidung**: Kombination aus B und C  


- Postgres-Volume zur√ºckgesetzt und Neuinitialisierung mit `.env`-Werten vorgenommen (`POSTGRES_USER=admin`, `POSTGRES_PASSWORD=‚Ä¶`).  
- `docker-compose.yml`: `POSTGRES_USER` an `.env` gekoppelt; Prometheus-Healthcheck auf `wget` umgestellt.  

**Konsequenzen**:  

- ‚ûï Eindeutige, zentrale Steuerung der DB-Credentials √ºber `.env`  
- ‚ûï Prometheus wird korrekt als ‚Äûhealthy" erkannt  
- ‚ûñ Daten im alten Postgres-Volume wurden verworfen (bewusst, MVP-Phase)  

---

## ADR-012: bot_rest ohne Healthcheck betreiben

**Datum**: 2025-10-26  
**Status**: ‚úÖ Beschlossen  
**Kontext**: `bot_rest` Container wurde als "unhealthy" gemeldet, obwohl er korrekt funktioniert. Service l√§uft in Periodik-Loop (alle 300s) ohne HTTP-Server, der Healthcheck via curl schlug daher immer fehl.  
**Optionen**:  

- A) HTTP-Server in bot_rest einbauen nur f√ºr /health Endpoint  
- B) Healthcheck entfernen und Status via docker logs √ºberwachen  
- C) Healthcheck auf Script-Check umstellen (ps, pidof)  

**Entscheidung**: Option B ‚Äì Healthcheck aus `docker-compose.yml` entfernt mit Kommentar "No healthcheck - service runs in periodic loop without HTTP server"  
**Konsequenzen**:  

- ‚ûï Container-Status zeigt "running" statt "unhealthy"  
- ‚ûï Keine unn√∂tige Komplexit√§t durch HTTP-Server nur f√ºr Health-Check  
- ‚ûï Service-Funktion best√§tigt durch docker logs (regelm√§√üige Outputs)  
- ‚ûñ Kein automatisches Health-Signal f√ºr Monitoring; manuelles Log-Monitoring erforderlich  

---

## ADR-013: MCP-Server Integration f√ºr erweiterte Development-Tools

**Datum**: 2025-10-26  
**Status**: ‚úÖ Beschlossen  
**Kontext**: GitHub Copilot bietet √ºber Model Context Protocol (MCP) spezialisierte Tool-Server f√ºr Docker-Management, Python-Analyse, Dokumentation und Diagramme. Integration erweitert Development-Workflow mit semantischen Abfragen, automatischem Refactoring und visueller Dokumentation.  
**Optionen**:  

- A) Nur Standard VS Code Extensions nutzen (ohne MCP)  
- B) Ausgew√§hlte MCP-Server konfigurieren (Docker, Pylance, Context7, Mermaid)  
- C) Alle verf√ºgbaren MCP-Server installieren (inkl. experimentelle)  

**Entscheidung**: Option B ‚Äì 4 MCP-Server strategisch ausgew√§hlt und konfiguriert:

1. **Docker MCP**: Knowledge Graph f√ºr Container-Infrastruktur (9 Container, 4 Volumes, 24 Relations)
2. **Pylance MCP**: Python Code-Analyse, Refactoring, Snippet-Execution
3. **Context7**: Library-Dokumentation (fastapi, redis, psycopg2, pydantic)
4. **Mermaid Chart**: Diagramm-Erstellung und Validierung (Flowcharts, Sequence, ER)

**Implementierung**:  

- Zentrale Konfiguration: `backoffice/mcp_config.json`
- Dokumentation: `docs/MCP_SETUP_GUIDE.md` (420+ Zeilen)
- Docker Knowledge Graph initialisiert mit allen System-Entities und Relations
- Chatmodes erweitert: `.github/chatmodes/*` integrieren MCP-Tools

**Konsequenzen**:  

- ‚ûï Semantische Suche √ºber Container-Topologie (mcp_mcp_docker_search_nodes)
- ‚ûï Automatisches Refactoring (Unused Imports, Format Conversion)
- ‚ûï Code-Snippets direkt im Workspace-Environment testbar (ohne Terminal-Escaping)
- ‚ûï Aktuelle Library-Docs on-demand (pypi, npm, GitHub)
- ‚ûï Diagramm-Validierung vor Commit (Syntax-Checks, Live-Preview)
- ‚ûï Dokumentation der Service-Beziehungen im Knowledge Graph persistiert
- ‚ûñ MCP-Server sind nicht persistent (Docker Graph muss nach Restart neu bef√ºllt werden)
- ‚ûñ Context7 erfordert Internet-Verbindung f√ºr Doc-Abruf
- üîÑ Wartung: Quartalsm√§√üige Review der MCP-Konfiguration (n√§chster Termin: 2025-11-26)

**Metriken**:  

- Docker-Entities: 14 (9 Container, 1 Network, 4 Volumes)
- Docker-Relations: 24 (Pub/Sub, Network, Volume-Mounts, Metrics)
- Python-Services: 3 (signal_engine, risk_manager, execution_service)
- Dokumentierte Libraries: 6 (fastapi, redis-py, psycopg2-binary, prometheus-client, pydantic, httpx)

---

## ADR-014: Docker MCP Toolkit Integration f√ºr Gordon AI-Agent

**Datum**: 2025-10-26  
**Status**: ‚úÖ Beschlossen  
**Kontext**: W√§hrend der MCP-Server-Integration (ADR-013) wurde festgestellt, dass das offizielle **Docker MCP Toolkit** (Beta-Feature in Docker Desktop) eine dedizierte L√∂sung f√ºr AI-Agenten wie Gordon bietet. Das Toolkit erm√∂glicht Cross-LLM-Kompatibilit√§t, Zero-Setup-Orchestration und sichere Tool-Verwaltung via MCP Gateway.

**Problem**: Bestehende VS Code MCP-Server (ADR-013) sind ausschlie√ülich f√ºr VS Code Copilot optimiert. F√ºr Container-Management und Live-Operations ben√∂tigen wir einen AI-Agenten mit direktem Docker-CLI-Zugriff und Dateioperationen.

**Optionen**:  

- A) Nur VS Code MCP-Server nutzen und Terminal-Befehle manuell ausf√ºhren  
- B) Docker MCP Toolkit aktivieren und Gordon als separaten AI-Agenten f√ºr DevOps-Tasks nutzen  
- C) Eigenen MCP-Server f√ºr Claire de Binaire entwickeln und im Docker Catalog ver√∂ffentlichen  

**Entscheidung**: Kombination aus B und C (langfristig)  

**Phase 1 (sofort)**:  

- Docker MCP Toolkit Beta-Feature in Docker Desktop aktivieren
- Gordon via MCP Gateway f√ºr Container-Management, Health-Checks und Log-Analyse nutzen
- Workflow definiert: VS Code Copilot (Code/Architektur) + Gordon (Operations/Debugging)

**Phase 2 (Q4 2025)**:  

- Custom MCP-Server f√ºr Claire de Binaire entwickeln (`claire-de-binare-mcp`)
- Tools: `get_latest_trades`, `get_signal_count`, `check_risk_limits`, `analyze_performance`
- Ver√∂ffentlichung im Docker MCP Catalog (optional)

**Implementierung (Phase 1)**:  

- Dokumentation: `docs/DOCKER_MCP_TOOLKIT_SETUP.md` (500+ Zeilen)
- Bereiche: Toolkit-Aktivierung, Gordon-Setup, Security (OAuth, Secrets), Custom Server Template
- Gordon Test-Prompts f√ºr Claire de Binaire erstellt (Container-Status, Health-Checks, Log-Analyse)
- MCP Gateway Security dokumentiert (Resource Limits, Image Signing, Request Interception)

**Docker MCP Toolkit Features**:  

- ‚úÖ Cross-LLM Kompatibilit√§t (Gordon, Claude Desktop, Cursor)
- ‚úÖ Zero Manual Setup (keine Dependency-Verwaltung, Auto-Discovery via Docker Catalog)
- ‚úÖ Security: Passive (Image Signing, SBOM) + Active (Resource Limits, Request Interception)
- ‚úÖ Portabilit√§t: Tools funktionieren plattform√ºbergreifend ohne Code-√Ñnderungen
- ‚úÖ MCP Gateway: Sichere Orchestration zwischen AI-Clients und MCP-Servern

**Gordon Use Cases f√ºr Claire de Binaire**:  

1. **Container-Management**: `docker ps`, `docker logs`, `docker restart` via nat√ºrliche Sprache
2. **Database-Queries**: PostgreSQL-Abfragen via MCP Tools (nach Custom Server-Implementierung)
3. **Health-Monitoring**: Automatische Pr√ºfung aller Health-Endpoints (8001, 8002, 8003)
4. **Log-Analyse**: Fehlersuche in Echtzeit-Logs mit semantischer Filterung
5. **OAuth-Integration**: GitHub-API-Zugriff f√ºr PR-Management, Issue-Tracking

**Workflow-Abgrenzung (Copilot vs. Gordon)**:  

| Aufgabe | VS Code Copilot | Gordon (Docker MCP) |
|---------|-----------------|---------------------|
| Code-Analyse & Review | ‚úÖ Prim√§r | ‚ûñ |
| Architektur-Entscheidungen | ‚úÖ Prim√§r | ‚ûñ |
| Docker Container-Management | ‚ûñ | ‚úÖ Prim√§r |
| Datei-Bulk-Operationen | ‚ûñ | ‚úÖ Prim√§r |
| Dokumentations-Erstellung | ‚úÖ Prim√§r | ‚ûñ |
| Live-Debugging (Logs, Metrics) | ‚ûñ | ‚úÖ Prim√§r |
| Database-Queries | ‚ûñ | ‚úÖ Prim√§r (Phase 2) |

**Konsequenzen**:  

- ‚ûï Separation of Concerns: Code-Tasks (Copilot) vs. Operations-Tasks (Gordon)
- ‚ûï Gordon kann Docker-CLI ohne PowerShell-Escaping-Probleme nutzen
- ‚ûï H√∂here Datei-Operationslimits (Gordon: 1000 Zeilen read, 50 write vs. Copilot-Tools)
- ‚ûï MCP Gateway enforcement von Security-Policies (Resource Limits, OAuth-Token-Rotation)
- ‚ûï Custom MCP-Server erm√∂glicht trading-spezifische Tools (Trade-Queries, Risk-Metrics)
- ‚ûñ Gordon erfordert Docker Desktop Beta-Features (experimentell, potenzielle Breaking Changes)
- ‚ûñ Zus√§tzlicher Kontext-Switch zwischen VS Code (Copilot) und Docker Desktop (Gordon)
- ‚ûñ MCP-Server im Docker Catalog sind public (Custom Server nur lokal oder nach Review ver√∂ffentlichbar)
- üîÑ Wartung: Custom MCP-Server (Phase 2) erfordert Dockerfile, server.yaml und tools.json Pflege

**Security-Ma√ünahmen**:  

1. **Secrets Management**: `docker mcp secret set` f√ºr DB-Credentials, API-Keys
2. **Resource Limits**: Memory 512M, CPU 0.5, Network restricted
3. **OAuth-Flow**: GitHub OAuth via `docker mcp oauth authorize github`
4. **Image Signing**: Docker-built images im `mcp/` namespace mit kryptographischen Signaturen
5. **Request Interception**: MCP Gateway √ºberwacht alle Tool-Calls auf Policy-Verletzungen

**Metriken**:  

- Dokumentation: 500+ Zeilen (DOCKER_MCP_TOOLKIT_SETUP.md)
- MCP-Server-Typen: 2 (VS Code: 4 Server, Docker Desktop: Gordon + Custom Server in Phase 2)
- Gordon-Prompts: 6 (Status-Check, Container-Neustart, Database-Check, Rebuild, Health-Check, Log-Analyse)
- Custom Server Templates: 3 (Dockerfile, server.yaml, main.py)

**N√§chste Schritte (Phase 2 - Q4 2025)**:  

1. Custom MCP-Server `claire-de-binare-mcp` entwickeln (Python, FastAPI-basiert)
2. Tools implementieren: `get_latest_trades`, `get_signal_count`, `check_risk_limits`, `analyze_performance`
3. Docker Catalog Submission vorbereiten (optional, nach intern. Testing)
4. Gordon-Integration in CI/CD-Pipeline (automatische Health-Checks pre-deployment)

---

## ADR-015: Sofortige Handlungsdokumentation im Copilot-Workflow

**Datum**: 2025-10-27  
**Status**: ‚úÖ Beschlossen  
**Kontext**: W√§hrend der laufenden Paper-Trading-Testphase sind pr√§zise und zeitnahe Protokolle jedes KI-Schritts erforderlich. Bisher wurden Aktionen h√§ufig erst am Sessionende gesammelt festgehalten, was das Nachvollziehen einzelner Eingriffe erschwerte.

**Optionen**:  

- A) Bisherige Sammeldokumentation am Sessionende beibehalten  
- B) Manuelle Protokollierung nach eigenem Ermessen  
- C) Verpflichtende Dokumentation nach jeder abgeschlossenen Handlung in Session-Memo oder DECISION_LOG

**Entscheidung**: Option C ‚Äì Jede Aktion wird unmittelbar nach Abschluss dokumentiert. F√ºr kleinere operative Schritte gen√ºgt ein Eintrag im laufenden Session-Memo; strukturrelevante Anpassungen werden zus√§tzlich im DECISION_LOG festgehalten.

**Konsequenzen**:  

- ‚ûï L√ºckenlose R√ºckverfolgbarkeit einzelner KI-Handlungen  
- ‚ûï Schnellere Auditierbarkeit w√§hrend des 7-Tage-Tests  
- ‚ûï Klarer Hand-off zwischen Copilot und Gordon dank identischer Protokollierungspflicht  
- ‚ûñ Geringf√ºgiger Mehraufwand pro Schritt (sofortige Notizen erforderlich)

**Umsetzung**: Copilot-Instruktionen aktualisiert (`.github/copilot-instructions.md`), inklusive Autonomie-Hinweis f√ºr Terminalaufgaben und Pflicht zur direkten Dokumentation.

**Follow-up 2025-10-27**: Build-Kontexte in `compose.yaml` auf `backoffice/services/...` angepasst, damit `docker compose` die Service-Verzeichnisse findet; Docker-Start bleibt blockiert, solange das `risk_manager` Service-Verzeichnis fehlt.

**Follow-up 2025-10-27 (Bereinigung)**: `compose.yaml` enthielt doppelte Service-Definitionen zu `docker-compose.yml`. Da `docker-compose.yml` bereits vollst√§ndig konfiguriert ist (9 Container inkl. Redis, Postgres, Monitoring) und stabil l√§uft, wurde `compose.yaml` entfernt aus dem aktiven Setup. Die fehlgeschlagenen Container-Instanzen (Signal, Risk, Execution aus `compose.yaml`) wurden gestoppt; nur die Haupt-Services aus `docker-compose.yml` bleiben aktiv. Haupt-Compose ist vollst√§ndige Infrastruktur inkl. Redis/Postgres, w√§hrend `compose.yaml` isolierte Service-Tests ohne Abh√§ngigkeiten war ‚Äì Entscheidung: Haupt-Compose als einzige produktive Konfiguration nutzen. Postgres-Container war gestoppt; nach Neustart ist Execution-Service nun stabil (10/10 Container healthy).

---

## ADR-016: Tool Layer Registry f√ºr zentrale Tool-Verwaltung

**Datum**: 2025-10-27  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Mit wachsender MCP-Server-Integration, DevOps-Tools und ML-Komponenten fehlte eine zentrale √úbersicht aller verf√ºgbaren Tools. Entscheidungen √ºber neue Integrationen wurden ad-hoc getroffen, ohne strukturierte Kategorisierung oder Status-Tracking.

**Optionen**:

- A) Tools weiterhin dezentral in einzelnen Dokumenten pflegen
- B) Zentrale Tool Registry mit Kategorisierung (GO TO USE / NICE TO HAVE)
- C) Externe Plattform (Notion, Confluence) f√ºr Tool-Management

**Entscheidung**: Option B ‚Äì Zentrale Tool Registry in `docs/TOOL_LAYER.md` mit klarer Kategorisierung und Statusverfolgung.

**Struktur**:

- **GO TO USE**: Produktiv eingebundene Tools (11 MCP-Server, 10 Docker-Container, 4 Monitoring-Tools)
- **NICE TO HAVE**: Geplante Erweiterungen (NotebookLM, Vault, Autogen Studio)
- Status-Kennzeichnung: ‚úÖ aktiv, üü¢ bereit, üß™ experimentell, üîú geplant

**Kategorien**:

1. Core Integrationen / MCP-Server (6): github-mcp, postman-mcp, mcp-grafana, mcp-redis, mongodb-mcp, hub-mcp
2. DevOps & Automation (4): n8n, self-hosted-ai-starter-kit, git-credential-manager, mcp-registry
3. Monitoring & Observability (5): Prometheus, Grafana, Loki, Pyroscope, Sift
4. Core Daten & Persistenz (5): PostgreSQL, Redis, SQLite, MongoDB Atlas, Qdrant
5. Forschung & ML-Advisor (5): TensorFlow, XGBoost, SHAP, W&B, Neptune.ai
6. Wissens- & Doku-Assistenz (3): NotebookLM, Notion API, Obsidian
7. Design & Pr√§sentation (2): Figma/Canva SDK, Plotly/Matplotlib

---

## ADR-017: Query Service f√ºr READ-ONLY Data Access Layer

**Datum**: 2025-10-30  
**Status**: ‚úÖ Beschlossen  
**Kontext**: MCP-Server und externe Tools ben√∂tigen strukturierten, READ-ONLY Zugriff auf Postgres-Tabellen (signals, risk_positions) und Redis-Streams (event streams). Bisherige Ad-Hoc-Queries erschwerten Wartbarkeit und fehlende Type-Safety f√ºhrte zu inkonsistenten Datenformaten.

**Problem**: Fragmentierter Datenzugriff √ºber verschiedene Services und Tools ohne zentrale Schnittstelle. Gordon AI-Agent und Monitoring-Dashboards ben√∂tigen deterministische, einheitliche JSON-Responses.

**Optionen**:

- A) Direkter Postgres/Redis-Zugriff aus jedem Tool (Status Quo)
- B) REST API mit FastAPI entwickeln (zus√§tzlicher HTTP-Server)
- C) Python Query Service Library mit CLI und programmatischer API

**Entscheidung**: Option C ‚Äì Lightweight Python Query Service als Library mit CLI-Interface

**Implementierung**:

- Location: `backoffice/services/query_service/`
- Komponenten:
  - `service.py`: Hauptklasse mit async Postgres/Redis queries
  - `config.py`: Environment-basierte Konfiguration
  - `models.py`: Type-safe Dataclasses (SignalRecord, RiskRecord, RedisEvent)
  - `cli.py`: Command-line Interface f√ºr interaktive Nutzung
  - `examples.py`: Vollst√§ndige Beispiele f√ºr alle Queries
  - `API_SPEC.json`: Formale Spezifikation gem√§√ü User-Request
- Dependencies: `asyncpg>=0.29.0`, `redis>=5.0.0`

**Verf√ºgbare Queries**:

1. **signals_recent** (Postgres): Letzte N Signals f√ºr Symbol (BTCUSDT default)
   - Filter: symbol, since_ms, limit (max 1000)
   - Output: timestamp, symbol, side, price, confidence, reason, volume, pct_change

2. **risk_overlimit** (Postgres): Risk-Positionen √ºber Limit
   - Filter: symbol (optional), only_exceeded, limit
   - Output: timestamp, symbol, exposure, limit

3. **redis_tail** (Redis): Letzte N Events aus Stream
   - Filter: channel (signals:BTCUSDT default), count
   - Output: event_id, timestamp, payload

**Output-Format (einheitlich)**:

```json
{
  "result": [/* records */],
  "count": 123,
  "query": "signals_recent",
  "timestamp_utc": "2025-10-30T10:45:00.123456+00:00"
}
```

**Constraints**:

- ‚úÖ READ_ONLY (keine INSERT/UPDATE/DELETE)
- ‚úÖ Deterministische Sortierung (timestamp DESC)
- ‚úÖ Connection Pooling (Postgres: 1-5 Connections)
- ‚úÖ Timeouts (Postgres: 30s, Redis: 5s)
- ‚úÖ Limit Enforcement (max 1000 pro Query)

**Konsequenzen**:

- ‚ûï Zentrale, wartbare Datenzugriffsschicht
- ‚ûï Type-Safety durch Pydantic-Dataclasses
- ‚ûï CLI f√ºr manuelle Exploration und Debugging
- ‚ûï Gordon AI-Agent kann strukturierte Queries ohne SQL-Injection-Risiko ausf√ºhren
- ‚ûï Einheitliches JSON-Format f√ºr alle Monitoring-Tools
- ‚ûï Async-First Design (skalierbar f√ºr parallel queries)
- ‚ûñ Zus√§tzliche Dependency-Layer (asyncpg, redis-py)
- ‚ûñ Kein HTTP-Endpoint (nur Library-Import oder CLI)
- üîÑ Future: REST API Wrapper f√ºr externe Tools (FastAPI optional in Phase 2)

**Integration**:

- **Gordon AI-Agent**: Via CLI oder direkter Python-Import f√ºr Container-Diagnostik
- **Monitoring-Dashboards**: Grafana kann CLI-Output als JSON Data Source nutzen
- **MCP-Server (Phase 2)**: Custom Claire de Binaire MCP-Server nutzt Query Service intern
- **Jupyter Notebooks**: Direkter Import f√ºr Backtesting und Analyse

**Sicherheit**:

- ‚úÖ Postgres-User hat nur SELECT-Rechte (Role-based in Phase 7)
- ‚úÖ Redis-Client nutzt READ-ONLY Kommandos (XREVRANGE, keine DEL/EXPIRE)
- ‚úÖ Connection-Strings niemals in Logs (nur ENV-Variablen)
- ‚úÖ SQL-Injection-sicher (asyncpg Prepared Statements)

**Wartung**:

- Monatliches Review: Query-Performance-Metriken (Query-Dauer, Ergebnis-Counts)
- Quartalsweise: Schema-Alignment-Check gegen `DATABASE_SCHEMA.sql`
- Bei √Ñnderungen in `EVENT_SCHEMA.json`: models.py synchronisieren

**Metriken**:

- Code: 700+ Zeilen (Python)
- Dokumentation: 300+ Zeilen (README.md)
- Tests: 7 Test-Cases (pytest)
- API-Spec: Vollst√§ndig JSON-dokumentiert (API_SPEC.json)

**N√§chste Schritte**:

1. Dependencies installieren: `pip install -r backoffice/services/query_service/requirements.txt`
2. CLI-Test: `python -m backoffice.services.query_service.cli --query signals_recent --symbol BTCUSDT`
3. Integration-Tests: `pytest backoffice/services/query_service/test_service.py -v`
4. Gordon-Prompts erweitern: "Zeige die letzten 50 Signals f√ºr BTCUSDT"
8. Security & Governance (3): HashiCorp Vault, Trivy/Grype, OPA
9. KI-Orchestrierung & Agent Frameworks (2): LangSmith/LangFuse, Autogen Studio

**Konsequenzen**:

- ‚ûï Zentrale √úbersicht aller verf√ºgbaren Tools f√ºr AI-Agenten (Copilot, Gordon)
- ‚ûï Strukturierter Entscheidungsprozess f√ºr neue Tool-Integrationen
- ‚ûï Klare Statusverfolgung (aktiv, bereit, experimentell, geplant)
- ‚ûï Automatische Referenz in AI-Prompts ("Nutze mcp-redis f√ºr Pub/Sub-Analyse")
- ‚ûï Wartungs-Strategie definiert (w√∂chentlich, monatlich, quartalsweise Reviews)
- ‚ûñ Zus√§tzlicher Pflegeaufwand bei Tool-Updates (Status-√Ñnderungen dokumentieren)

**Integration**:

- Verweis in `ARCHITEKTUR.md` (neuer Abschnitt "Tool Layer Integration")
- Verkn√ºpfung mit `MCP_DOCUMENTATION_INDEX.md` (technische Details)
- Update `PROJECT_STATUS.md` (Metriken: 11 MCP-Server, 30+ Tools dokumentiert)

**Metriken**:

- GO TO USE Tools: 30 (davon 11 MCP-Server, 10 Docker-Container)
- NICE TO HAVE Tools: 12 (geplante Erweiterungen)
- Dokumentierte Kategorien: 9
- Gesamtumfang: 280+ Zeilen Dokumentation

**Integration abgeschlossen (2025-10-27)**:

- ‚úÖ `ARCHITEKTUR.md` erweitert (Abschnitt "Tool Layer Integration")
- ‚úÖ `PROJECT_STATUS.md` aktualisiert (Phase 6.3, 10/10 Container healthy)
- ‚úÖ `MCP_DOCUMENTATION_INDEX.md` verlinkt auf TOOL_LAYER.md
- ‚úÖ Container-Status validiert: 10/10 healthy (inkl. Execution-Service nach Postgres-Fix)

---

## ADR-017: Gordon-Konsultation vor Docker-Eingriffen

**Datum**: 2025-10-27  
**Status**: ‚úÖ Beschlossen

**Kontext**: Wiederholte Container-Restarts und unvollst√§ndige Infrastruktur-Kontexte haben gezeigt, dass spontane Docker-Eingriffe ohne Gordon-Abstimmung zu Instabilit√§t f√ºhren. Gordon fungiert als zentrale Kontrollinstanz f√ºr Infrastruktur-√Ñnderungen √ºber das MCP-Toolkit.

**Optionen**:

- A) Copilot f√ºhrt Docker-Operationen eigenst√§ndig durch
- B) Vor jedem docker compose / docker CLI Eingriff Gordon √ºber MCP konsultieren und Freigabe dokumentieren
- C) Alle Docker-Aktionen vollst√§ndig an Gordon delegieren

**Entscheidung**: Option B ‚Äì Copilot holt vor jedem Docker-Befehl (compose up/down, build, prune, rm, volume/network-√Ñnderungen) eine Gordon-Freigabe ein. Ohne dokumentierte Freigabe d√ºrfen keine Container-, Netzwerk- oder Volume-Operationen erfolgen.

**Konsequenzen**:

- ‚ûï Verhindert inkonsistente Compose-Starts bei unvollst√§ndiger Umfeld-Konfiguration
- ‚ûï Einheitlicher Freigabeprozess via MCP, nachvollziehbar im Session-Memo
- ‚ûï Gordon beh√§lt Gesamt√ºberblick √ºber Infrastrukturzustand und Ressourcenplanung
- ‚ûñ Zus√§tzlicher Kommunikationsschritt vor operativen Docker-Befehlen

**Umsetzung**:

- Copilot dokumentiert jede Gordon-Anfrage im laufenden Session-Memo (Zeitstempel, angefragte Aktion, Ergebnis)
- Docker-Runbooks im `docs/ops/RUNBOOK_DOCKER_OPERATIONS.md` und in der `EXECUTION_DEBUG_CHECKLIST.md` verweisen auf verpflichtende Gordon-Freigabe
- Automatische Checks: Vor Docker-Kommandos wird gepr√ºft, ob aktuelle Gordon-Freigabe vorliegt (Session-Notiz oder Ticket)

---

## ADR-018: README Guide & Dashboard-V5-Standardisierung

**Datum**: 2025-11-01  
**Status**: ‚úÖ Beschlossen  
**Kontext**: README-Dateien waren inkonsistent strukturiert, enthielten veraltete Ports/Topics
und widerspr√ºchliche ENV-Hinweise. Audit-Anforderungen forderten einen einheitlichen
Dashboard-V5-Auftritt.

**Optionen**:

- A) Nur Root-README anpassen, restliche Dateien schrittweise bei Bedarf
- B) Verbindlichen Leitfaden `README_GUIDE.md` erstellen und alle Readmes daran ausrichten
- C) Readmes durch externes Wiki ersetzen

**Entscheidung**: Option B ‚Äì `README_GUIDE.md` definiert verpflichtend Aufbau, Tabellenlayout,
Visuelle Elemente (Dashboard-V5-Stil) und Referenzlinks. Alle bestehenden Readmes wurden
entsprechend migriert.

**Konsequenzen**:

- ‚ûï Einheitliche Darstellung f√ºr alle Services, Module und Ordner
- ‚ûï Zentrale Quelle f√ºr Ports, Topics, ENV, Metriken h√§lt Docs synchron mit Architektur
- ‚ûï Vereinfachte Reviews dank klarer Strukturbl√∂cke (√úberblick, Architektur, Setup, Monitoring)
- ‚ûñ Initialer Migrationsaufwand f√ºr Bestandsdateien

**Validierung**:

- `README_GUIDE.md` im Repo-Root eingef√ºhrt (Dashboard-V5-Vorgaben)
- Alle aktualisierten Readmes verlinken konsistent auf `ARCHITEKTUR.md`,
  `Service-Kommunikation & Datenfl√ºsse.md`, `Risikomanagement-Logik.md`
- `.env` und Ports/Topics-Tabellen in den Readmes decken sich mit Compose & Event-Schema

---

## ADR-019: Wissensgraph Phase 2 ‚Äì smarter_assistant Integration

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Mit `knowledge_inventory.json`, `semantic_map.md`, `refactor_plan.md`, `consistency_audit.md` und `learning_path.md` existieren neue Wissensartefakte, die bislang als isolierte Dokumente gef√ºhrt wurden. F√ºr Phase 3 (2-Hop-Konsolidierung) ist ein konsistenter Wissensgraph erforderlich.

**Optionen**:

- A) Weiterhin rein textuelle Dokumente verwenden und Abh√§ngigkeiten ad hoc verfolgen
- B) Smarter-Assistent-Artefakte in bestehenden Listen verlinken, aber ohne Graph-Struktur
- C) Einen formalen Wissensgraph etablieren (Prim√§rdokument `semantic_map.md`, menschliche Navigationsschicht `Knowledge_Map.md`, Maschinen-Layer `semantic_index.json`)

**Entscheidung**: Option C ‚Äì Vollst√§ndige Integration aller smarter_assistant-Artefakte in einen Knowledge Graph mit maschinenlesbarem Index und menschlichem Navigationslayer.

**Konsequenzen**:

- ‚ûï Phase-3-Konsolidierung kann gezielt 1-Hop- und 2-Hop-Abh√§ngigkeiten analysieren
- ‚ûï Neue Artefakte erhalten Prim√§rstatus und verlieren ihren Inselcharakter
- ‚ûï Automatisierungen k√∂nnen √ºber `semantic_index.json` Beziehungen programmatisch auswerten
- ‚ûï `knowledge_inventory.json` bleibt Datenquelle, aber Graph regelt Priorisierung
- ‚ûñ Laufender Pflegeaufwand: Jede Relation muss im Index und in der Knowledge Map nachgetragen werden

**Umsetzung**:

- `semantic_map.md` als Prim√§rdokument markiert und um Graphstatus erweitert
- `docs/smarter_assistant/Knowledge_Map.md` erstellt (Navigation, 1/2-Hop-Ketten)
- `docs/smarter_assistant/semantic_index.json` erzeugt (Knoten, Kanten, Cluster)
- `PROJECT_STATUS.md` unter "Technische Verbesserungen" mit Phase-2-Vermerk erg√§nzt

**Abh√§ngigkeiten**:

- Phase 3 st√ºtzt sich auf diese Artefakte, um Redundanzen (ENV, Ports, Topics) zu beseitigen
- Phase 4 setzt Wissensanker erst nach Abschluss der Phase-3-Ma√ünahmen

---

## ADR-020: Phase-3-Normalisierung ‚Äì Konfliktregister

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Phase 3 soll 1-/2-Hop-Konflikte (Ports, Secrets, Event-Literals) konsistent beheben. Bisherige Artefakte (PROJECT_STATUS, Service-Dokumente, Schema) f√ºhrten zu widerspr√ºchlichen Angaben, was Phase-4-Wissensanker blockiert.

**Optionen**:

- A) Konflikte jeweils direkt in den betroffenen Dokumenten notieren (Projektstatus, Service-Doku, Schema)
- B) Session-Memos erweitern und Konflikte tempor√§r dokumentieren
- C) Zentrales Normalisierungs-Register erstellen (`Normalization_Report.md`) und maschinenlesbare Referenzen im `semantic_index.json` pflegen

**Entscheidung**: Option C ‚Äì eigenes Konfliktregister mit Ma√ünahmenliste und Graph-Verankerung, damit Phase-3-Konsolidierung nachvollziehbar und auditierbar bleibt.

**Konsequenzen**:

- ‚ûï Port- und Secret-Divergenzen werden in einem Dokument zentral verfolgt
- ‚ûï Schema-Abweichungen zwischen Beispieldokumentation und `EVENT_SCHEMA.json` sind eindeutig adressiert
- ‚ûï `semantic_index.json` bildet Konfliktkanten (`conflicts_with`, `tracks_issue`) f√ºr Automatisierungen ab
- ‚ûï Governance-Dokumente (`PROJECT_STATUS.md`, `DECISION_LOG.md`) verweisen auf die Normalisierung als laufende Aktivit√§t
- ‚ûñ Zus√§tzlicher Pflegeaufwand, bis alle Konflikte behoben sind und auf `verified=true` gesetzt werden k√∂nnen

**Validierung**:

- `docs/smarter_assistant/Normalization_Report.md` erstellt (Port-, Secret-, Event-/Alert-Deltas dokumentiert)
- `semantic_index.json` um Knoten `normalization_report`, `env_file`, `service_dataflow_doc`, `risk_logic_doc` und Konfliktkanten erweitert
- `Knowledge_Map.md`, `semantic_map.md`, `PROJECT_STATUS.md` auf Phase-3-Status und Normalisierungseintr√§ge aktualisiert

**Abh√§ngigkeiten**:

- Umsetzung der Ma√ünahmen aus dem Normalization Report ist Voraussetzung f√ºr Phase-4-Wissensanker
- √Ñnderungen an Ports/Secrets/Schemas m√ºssen nach Umsetzung in allen Prim√§rquellen synchronisiert werden

---

## ADR-022: REST-Port-Governance-Normalisierung

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Runtime (`docker-compose.yml`, `.env`, Container-Status) exponiert den REST-Screener auf Host-Port 8080, w√§hrend Governance-Artefakte (`PROJECT_STATUS.md`, `Normalization_Report.md`, Session-Memos) noch 8010 f√ºhrten und Health-/Runbook-Checks fehlleiteten.

**Optionen**:

- A) Dokumentation unver√§ndert lassen und auf Runtime als ma√ügebliche Quelle verweisen
- B) Host-Port 8080 in allen Governance-Dokumenten vereinheitlichen und Konflikt im Wissensgraphen als verifiziert markieren
- C) REST-Service auf 8010 zur√ºcksetzen, um Dokumentation anzupassen

**Entscheidung**: Option B ‚Äì Governance-Artefakte und Session-Memo auf 8080 angleichen und Relation `project_status ‚Üí docker_compose` im Wissensgraphen als `verified=true` mit `normalized_value: "8080"` kennzeichnen.

**Konsequenzen**:

- ‚ûï Health-Checks, Runbooks und Monitoring-Dokumente referenzieren denselben Port (8080)
- ‚ûï Wissensgraph spiegelt die Normalisierung √ºber Metadaten (`verified`, `normalized_value`) wider
- ‚ûï Phase-4-Port-Loop abgeschlossen, Session-Memo dokumentiert den Abschluss
- ‚ûñ Laufende Normalisierungsschleifen ben√∂tigen konsistente Pflege der Wissensgraph-Metadaten

---

## ADR-023: Redis Secret Alignment

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Die Runtime verwendet `REDIS_PASSWORD=REDACTED_REDIS_PW` ( `.env`, `docker-compose.yml`, Container-Startup). Governance-Dokumente referenzierten weiterhin `REDACTED_REDIS_PW$$`, wodurch Secretsync und Runbooks divergierten.

**Optionen**:

- A) Runtime-Secret auf `REDACTED_REDIS_PW$$` zur√ºckdrehen und Container neu provisionieren
- B) `.env`, `PROJECT_STATUS.md`, `Risikomanagement-Logik.md` und Wissensgraph auf den Runtime-Wert **REDACTED_REDIS_PW** harmonisieren
- C) Redis ohne Passwort betreiben und Auth nur in Dokumentation erw√§hnen

**Entscheidung**: Option B ‚Äì Runtime gilt als autoritative Quelle. Alle Governance-Artefakte werden auf `REDIS_PASSWORD=REDACTED_REDIS_PW` aktualisiert, `semantic_index.json` dokumentiert den verifizierten Wert (`normalized_value: "${REDIS_PASSWORD}"`).

**Konsequenzen**:

- ‚ûï Secrets in Runtime, Dokumentation und Graph identisch; Runbooks funktionieren ohne Korrekturen
- ‚ûï Risk Manager Security-Abschnitt verweist auf ENV-Ladung gem√§√ü `.env`
- ‚ûï ADR-Referenz f√ºr zuk√ºnftige Rotation vorhanden (siehe Session Memo 2025-11-02)
- ‚ûñ Rotationen erfordern Pflege der Wissensgraph-Metadaten und Session-Memos

---

## ADR-024: Event Literal Standardization

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Dokumentationsbeispiele (Service-Kommunikation & Datenfl√ºsse, Risikomanagement-Logik) nutzten abweichende Event- und Alert-Bezeichner (`order_results`, `filled_qty`, `DAILY_LIMIT`). `EVENT_SCHEMA.json` definiert jedoch `order_result`, `filled_quantity`, `RISK_LIMIT`, `DATA_STALE`, `CIRCUIT_BREAKER` als verbindliche Literale.

**Optionen**:

- A) Dokumentation unver√§ndert lassen und Abweichungen in Fu√ünoten erkl√§ren
- B) Beispiele auf Schema-Enums angleichen und Wissensgraph-Relationen als `verified` markieren
- C) Schema an Dokumentation anpassen und Konfliktregister erweitern

**Entscheidung**: Option B ‚Äì Schema bleibt ma√ügeblich. Alle Beispiele werden angepasst, und `semantic_index.json` markiert die Relationen `event_schema ‚Üí service_dataflow_doc` und `event_schema ‚Üí risk_logic_doc` als `relation: "normalized"`, `verified: true`.

**Konsequenzen**:

- ‚ûï Einheitliche Payload-Literale eliminieren Tool- und Validierungsfehler
- ‚ûï Risk-Alerts nutzen kanonische Codes, wodurch Downstream-Filter funktionieren
- ‚ûï Normalization Report kann Phase 3 als abgeschlossen markieren
- ‚ûñ K√ºnftige Schema√§nderungen erfordern unmittelbare Doku-Anpassungen + Graph-Update

---

## ADR-027: Kontrollierter Archiv-Migrationsprozess (Phase 5)

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: F√ºr den Abschluss von Phase 5 m√ºssen Legacy-Dokumente aus `docs/` in das Archiv √ºberf√ºhrt werden. Fr√ºhere Ad-hoc-Moves f√ºhrten zu Wissensl√ºcken und widerspr√ºchlichen Referenzen (fehlende Frontmatter, unvollst√§ndige Knowledge-Graph-Updates, kein Dry-Run). Der neue Prozess soll Archivierung, Governance und Wissensgraph synchron halten.

**Optionen**:

- A) Dokumente bei Bedarf direkt verschieben und Migrationen manuell dokumentieren
- B) Einmalige Bulk-Migration durchf√ºhren und Nacharbeiten sp√§ter erledigen
- C) Einen kontrollierten Workflow mit Review-Plan, Dry-Run und gebundener Dokumentationspflicht einf√ºhren ("Safety over neatness")

**Entscheidung**: Option C ‚Äì Gesteuerter Archivierungsprozess mit verpflichtendem Review, Dry-Run-Report und Governance-Spiegelung. Verschiebungen erfolgen nur bei `migration_status = approved` und gesetztem `approved_target`.

**Konsequenzen**:

- ‚ûï Einheitlicher Blick auf alle Kandidaten √ºber `docs/smarter_assistant/migration_plan.md`
- ‚ûï Dry-Run (`migration_report_preview.md`) verhindert unbeabsichtigte Moves
- ‚ûï Frontmatter (`status`, `source`, `migrated_to`) und Knowledge-Graph bleiben konsistent
- ‚ûï Governance-Dokumente (PROJECT_STATUS, SESSION_MEMO_ORGANISATION) spiegeln Migrationen sofort wider
- ‚ûñ H√∂herer Aufwand pro Migration, da Review und Dokumentationsschritte verpflichtend sind

**Umsetzung**:

- `migration_plan.md` als Prim√§rquelle f√ºr Status (`planned_target`, `approved_target`, `migration_status`, Review-Notizen)
- Pilot-Migration `7D_PAPER_TRADING_TEST.md` in `archive/docs/` inklusive YAML-Frontmatter (`status: archived`, `migrated_to` gesetzt)
- Einrichtung eines Dry-Run-Reports (`migration_report_preview.md`) vor weiteren Moves
- 2025-11-02: Dry-Run f√ºr README_GUIDE.md ‚Üí `archive/docs/README_GUIDE.md` erstellt; Graph-Kanten `archived_from`/`migrated_to` vorerst mit `verified:false` hinterlegt
- 2025-11-02: Produktive Archivierung freigegeben ‚Äì Datei liegt unter `archive/docs/README_GUIDE.md`, Frontmatter erweitert, Relationen auf `verified:true` gesetzt
- Nach jeder Freigabe: Updates in `PROJECT_STATUS.md`, `Knowledge_Map.md`, `semantic_index.json` und Session-Memo 2025-11-02
- Beibehaltung der Schutzregel `pending` ‚Üí kein Move, bis Review abgeschlossen ist

**Abh√§ngigkeiten**:

- Wissensgraph-Artefakte (`Knowledge_Map.md`, `semantic_index.json`) m√ºssen nach tats√§chlicher Migration angepasst werden
- Archiv-Strukturen (`archive/docs/reports`, `archive/docs/research`, `archive/logs/inventory`) werden vor jedem Move auf Existenz gepr√ºft
- Governance bleibt f√ºhrend: Abweichungen oder Sonderf√§lle werden in `SESSION_MEMO_ORGANISATION_2025-11-02.md` dokumentiert


## ADR-029-R: Soft-Freeze & Continuous Learning Framework

**Datum**: 2025-11-02  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Nach Abschluss der produktiven Archivierung (ADR-027) soll das Repository auditierbar bleiben, ohne den laufenden Betrieb zu blockieren. Reviewer ben√∂tigen weiter Zugriff auf konsistente Artefakte, w√§hrend Operationsteam und Agenten Wissen und Code fortlaufend pflegen.

**Optionen**:
- A) Bisherigen Hard-Lock beibehalten (keine √Ñnderungen bis Review-Ende)
- B) Soft-Freeze mit Audit-Baseline und verpflichtender Protokollierung
- C) Vollst√§ndige Entsperrung ohne zus√§tzliche Kontrollen

**Entscheidung**: Option B ‚Äì Soft-Freeze. Audit-Baseline (`audit_snapshot_2025-11-02.json`) bleibt Referenz, Delta-Audits protokollieren √Ñnderungen, Live-Writes bleiben unter ADR-027-Sicherheitsregeln erlaubt.

**Konsequenzen**:
- ‚ûï Repository bleibt produktiv nutzbar (Paper/Live Trading, Wissenspflege)
- ‚ûï Jede √Ñnderung bleibt r√ºckverfolgbar (Snapshot + Delta-Audit, Session-Memo)
- ‚ûñ Zus√§tzlicher Aufwand f√ºr kontinuierliche Delta-Dokumentation

**Folgeaktionen**:
- `PROJECT_STATUS.md`: Governance Mode Abschnitt mit Soft-Freeze-Status
- `SESSION_MEMO_ORGANISATION_2025-11-02.md`: Kontinuierliche Operation samt Delta-Audit-Vermerk dokumentiert
- `backoffice/audits/`: Delta-Audit-Dateien pro Lauf anlegen; Baseline regelm√§√üig erneuern


## Audit-Review-Abschluss: Keine Findings, ADR-030 nicht erforderlich

**Datum**: 2025-11-02 18:30 UTC  
**Status**: ‚úÖ Abgeschlossen  
**Kontext**: Nach Handover Report 2025-11-02 17:00 UTC hat Audit-Team (GitHub Copilot) unabh√§ngigen 7-Phasen-Review nach REVIEW_README.md-Protokoll durchgef√ºhrt. Ziel war Verifikation von Governance-Koh√§renz, Knowledge-Graph-Konsistenz und technischer Integrit√§t.

**Pr√ºfumfang**:
1. **Audit-Artefakte**: `audit_snapshot_2025-11-02.json`, `delta_audit_2025-11-02T16-45Z.json`, `semantic_index_export.graphml`
2. **Governance**: ADR-027 ‚Üí ADR-029-R Chain, Continuous Operation Mode, Git-Refs
3. **Knowledge-Layer**: `semantic_index.json` (‚â•95% verified:true), `Knowledge_Map.md`, Archive-Cluster
4. **Technik**: Docker-Status (10/10 Container healthy), ENV/Compose-Konsistenz, requirements.txt
5. **Review-Bericht**: `HANDOVER_REVIEW_REPORT_2025-11-02T18-30Z.md` (450+ Zeilen)

**Ergebnis**:
- ‚úÖ **Governance**: ADR-Chain vollst√§ndig (ADR-027 ‚Üí ADR-029-R), Continuous Operation Mode aktiv
- ‚úÖ **Knowledge-Graph**: 100% Relations `verified:true` (manuelle Pr√ºfung best√§tigt ‚â•95%-Anforderung erf√ºllt)
- ‚úÖ **Technik**: 10/10 Container healthy (6h+ Uptime), ENV/Compose konsistent (REDIS_PASSWORD = REDACTED_REDIS_PW, POSTGRES_PASSWORD = cdb_secure_password_2025)
- ‚úÖ **Link-Audit**: Letzter Run 2025-11-02 15:10 UTC ‚Üí 0 Fehler
- üü° **Optionale Empfehlungen**: 2 Package-Updates (redis 7.0.0‚Üí7.0.1, ruff 0.14.2‚Üí0.14.3), 1 Doku-Erg√§nzung (GraphML-Viewer-Hinweis)

**Entscheidung**: **ADR-030 nicht erforderlich**  
**Begr√ºndung**: Keine kritischen Findings, keine Governance-Abweichungen, System operational-ready. Optionale Package-Updates k√∂nnen im Rahmen regul√§rer Maintenance erfolgen (kein Audit-Blocker).

**Konsequenzen**:
- ‚ûï Phase 7 (Paper Trading) genehmigt ‚Äì System bereit f√ºr Produktivbetrieb
- ‚ûï Continuous-Operation-Mode bleibt aktiv (ADR-029-R), keine Sperren
- ‚ûï Repository weiterhin schreibf√§hig unter ADR-027-Safety-Protokoll
- ‚ûñ Optionale Package-Updates bleiben dokumentiert (code_review_prep.md), aber nicht verpflichtend

**Deliverables**:
- `HANDOVER_REVIEW_REPORT_2025-11-02T18-30Z.md` (backoffice/audits/)
- `PROJECT_STATUS.md` aktualisiert (Phase 6.8: Audit-Team Review)
- `DECISION_LOG.md` (dieser Eintrag)

**Sign-Off**: GitHub Copilot (Audit-Team) ‚Üí IT-Chef  
**Freigabe**: Repository operational-ready, Phase 7 kann starten.

---

## ADR-031: Development Philosophy - Quality over Speed

**Datum**: 2025-11-03  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Nach erfolgreicher Stabilisierung in Phase 7 soll die Entwicklungsphilosophie explizit formalisiert werden: **Qualit√§t und Sorgfalt haben Vorrang vor Geschwindigkeit**. Dies reflektiert die bew√§hrten Praktiken, die zum aktuellen stabilen Zustand gef√ºhrt haben.

**Problem**:
- Schnelle, ungepr√ºfte √Ñnderungen f√ºhrten historisch zu Instabilit√§ten (z.B. compose.yaml-Konflikt, ADR-005)
- Dokumentations-L√ºcken erschwerten Debugging und Onboarding
- Fehlende Governance-Prozesse verz√∂gerten Reviews und Audits

**Entscheidung**: Etablierung verbindlicher Entwicklungsprinzipien:

### 1. **Dokumentation vor Code**
- Jede √Ñnderung wird **erst dokumentiert, dann implementiert**
- Architektur-√Ñnderungen ‚Üí `ARCHITEKTUR.md` + ADR in `DECISION_LOG.md`
- Event-Schema-√Ñnderungen ‚Üí `EVENT_SCHEMA.json` + betroffene `models.py`
- Konfigurations√§nderungen ‚Üí `.env`, `docker-compose.yml` + Validierung

### 2. **Schrittweise Umsetzung**
- Keine "Big Bang"-√Ñnderungen; iterative, validierte Schritte
- Nach jeder √Ñnderung: `docker compose config`, Health-Checks, Tests
- Bei Unsicherheit: **lieber nachfragen statt raten**

### 3. **Ordnung als Priorit√§t**
- Keine tempor√§ren Workarounds im produktiven Code
- Deprecated Code ‚Üí `archive/` mit Begr√ºndung
- Duplikate vermeiden, bestehende Strukturen nutzen

### 4. **Mandatory Review-Checkpoints**
- Vor jedem Commit: Review-Checkliste aus `DEVELOPMENT.md` durchgehen
- Bei strukturellen √Ñnderungen: Audit-Snapshot + Delta-Audit
- Session-Ende: `SESSION_MEMO` mit Zeitstempel + Entscheidungen

### 5. **Fehlerkultur**
- Fehler sind Lernchancen, nicht Blocker
- Incident Reports dokumentieren Root Cause + Prevention (siehe `2025-10-30_RECOVERY_REPORT.md`)
- Knowledge Base wird kontinuierlich erweitert (Research-Dokumente)

**Implementierung**:
- `DEVELOPMENT.md` erweitert um "0Ô∏è‚É£ Entwicklungsphilosophie"-Abschnitt
- `ARCHITEKTUR_REGELN.md` um Abschnitt "6. Entwicklungstempo" erg√§nzt
- `SESSION_MEMO_PHILOSOPHY_2025-11-03.md` als Einf√ºhrungsdokument

**Konsequenzen**:
- ‚ûï Stabilit√§t und Wartbarkeit haben Vorrang
- ‚ûï Neue Entwickler k√∂nnen sich auf klare Prinzipien verlassen
- ‚ûï Audits und Reviews werden beschleunigt (weniger Nacharbeiten)
- ‚ûñ Entwicklungszyklen werden l√§nger (bewusst akzeptiert)
- ‚ûñ Erfordert Disziplin und kontinuierliche Dokumentation

**Validation**:
- Alle zuk√ºnftigen PRs m√ºssen Review-Checkliste erf√ºllen
- Session-Memos sind verpflichtend f√ºr strukturelle √Ñnderungen
- Continuous Operation Mode (ADR-029-R) bleibt aktiv, aber Safety-Protokoll wird strenger

**Referenzen**:
- `DEVELOPMENT.md` - Entwicklungsrichtlinien
- `ARCHITEKTUR_REGELN.md` - Operative Leitplanken
- `2025-10-30_RECOVERY_REPORT.md` - Lessons Learned aus Stabilisierungsphase

**Sign-Off**: GitHub Copilot (Development Philosophy Initiative)  
**G√ºltigkeit**: Ab sofort f√ºr alle Repository-√Ñnderungen

---

## ADR-032: Python Base Image Pin auf 3.13-slim (statt 3.14-slim)

**Datum**: 2025-11-09  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Dependabot schlug Updates von `python:3.11-slim` ‚Üí `python:3.14-slim` f√ºr alle Dockerfiles vor (PRs #15, #13, #12). Python 3.14.0 wurde am 15.10.2024 released und ist auf Docker Hub verf√ºgbar.

**Problem**:
- Python 3.14 ist erst seit ~3 Wochen stabil (released 15.10.2024)
- Production-Systeme ben√∂tigen bew√§hrte, stabile Versionen
- 3 Major-Bumps (3.11‚Üí3.12‚Üí3.13‚Üí3.14) erh√∂hen Risiko f√ºr Breaking Changes
- Dependabot empfiehlt automatisch die neueste verf√ºgbare Version (nicht immer optimal)

**Evaluierte Optionen**:

1. **Python 3.14-slim** (neueste)
   - ‚ûï Neueste Features & Security-Patches
   - ‚ûñ Erst seit 3 Wochen stabil
   - ‚ûñ Unbekannte Production-Erfahrungen
   - ‚ûñ 3 Major-Bumps erh√∂hen Test-Aufwand

2. **Python 3.13-slim** (empfohlen)
   - ‚ûï Released 2024-10-07 (bereits 1 Monat stabil)
   - ‚ûï EOL: 2029-10 (guter Support-Zeitraum)
   - ‚ûï Gut getesteter Upgrade-Pfad 3.11‚Üí3.13
   - ‚ûï Balance zwischen Aktualit√§t und Stabilit√§t
   - ‚ûñ Nicht die absolute neueste Version

3. **Python 3.12-slim** (konservativ)
   - ‚ûï LTS-Version, sehr stabil
   - ‚ûï EOL: 2028-10
   - ‚ûñ Weniger neue Features

**Entscheidung**: Pin auf **python:3.13-slim** f√ºr alle Services

**Begr√ºndung**:
- **Produktions-Stabilit√§t:** Python 3.13 hat bereits ~1 Monat Production-Erprobung
- **Sicherheits-Unterst√ºtzung:** EOL 2029-10 deckt Multi-Jahr-Support ab
- **Bew√§hrter Upgrade-Pfad:** 3.11‚Üí3.13 ist gut dokumentiert & getestet
- **Risk-Mitigation:** 2 Major-Bumps statt 3 reduziert Breaking-Change-Risiko
- **Best Practice:** Production-Systeme sollten nicht auf bleeding-edge Versionen laufen

**Alternative f√ºr 3.14-Fans**:
Falls Python 3.14 gew√ºnscht wird, exakte Version pinnen:
```dockerfile
FROM python:3.14.0-slim
```
Statt `3.14-slim` (verhindert auto-upgrade auf 3.14.1, 3.14.2, etc.)

**Implementierung**:
- PRs #15, #13, #12: √Ñnderung von `3.14-slim` ‚Üí `3.13-slim` committen
- Docker Compose Build-Tests durchf√ºhren
- Service-Start & Health-Checks validieren
- Nach gr√ºnen Tests ‚Üí mergen

**Betroffene Dateien**:
- `backoffice/services/signal_engine/Dockerfile`
- `backoffice/services/risk_manager/Dockerfile`
- `Dockerfile` (root, f√ºr Screener)

**Rollback-Plan**:
Falls Kompatibilit√§tsprobleme auftreten:
```dockerfile
FROM python:3.11-slim
```

**Testing-Protokoll**:
- ‚úÖ Docker Hub Tag-Verf√ºgbarkeit gepr√ºft (3.13-slim verf√ºgbar)
- ‚è≥ Docker Build Tests (nach Commit)
- ‚è≥ Service Health-Checks (nach Deployment)
- ‚è≥ E2E-Test (optional, da kein Breaking Change erwartet)

**Konsequenzen**:
- ‚ûï Stabile, production-ready Python-Version
- ‚ûï Reduziertes Risiko f√ºr Breaking Changes
- ‚ûï Multi-Jahr-Support durch EOL 2029
- ‚ûñ Verzicht auf absolute neueste Features (Python 3.14)
- ‚ûñ Erfordert manuellen Dependabot-Override (statt auto-merge)

**Related PRs**:
- PR #15: signal_engine Docker Update
- PR #13: root Docker Update
- PR #12: risk_manager Docker Update

**Referenzen**:
- [Python Release Schedule](https://peps.python.org/pep-0619/)
- [Docker Hub python:3.13-slim](https://hub.docker.com/_/python?tab=tags&name=3.13-slim)
- `docs/PR_REVIEW_BATCH_2025_11_09.md` (Detailanalyse)

**Sign-Off**: GitHub Copilot Coding Agent (PR Review Session)  
**G√ºltigkeit**: Ab sofort f√ºr alle Python-Dockerfile-Updates




---

## ADR-032: Copilot-Instructions Update - Issue #6 Integration

**Datum**: 2025-11-09  
**Status**: ‚úÖ Beschlossen  
**Kontext**: Issue #6 enthielt umfangreiche Application-Configuration f√ºr Copilot Coding Agent mit operativen Anweisungen, die in der aktuellen `copilot-instructions.md` fehlten. Diese sollten mit der bestehenden Konfiguration verglichen und bei Verbesserungen √ºbernommen werden.

**Problem**: 
- Aktuelle `copilot-instructions.md` (80 Zeilen) fokussierte sich auf allgemeine Leitplanken
- Issue #6 Content enthielt spezifische operative Anweisungen:
  - Session-Start-Pflicht (Docker-Container pr√ºfen/starten)
  - Audit-Referenzen mit konkreten Dateipfaden
  - Architekturfluss (Event-Pipeline)
  - Logging-Regeln
  - Sofortige Dokumentationspflicht
  - Konkrete Validierungsbefehle

**Optionen**:
- A) Issue #6 Content komplett √ºbernehmen und bestehende Struktur ersetzen
- B) Nur neue Inhalte minimal-invasiv in bestehende Struktur integrieren
- C) Separate Datei f√ºr operative Anweisungen erstellen

**Entscheidung**: Option B - Minimal-invasive Erweiterung der bestehenden Struktur

**Implementierung**:

### 1. Abschnitt 2 erweitert: "Session-Start & Sicherheits-Regeln"
- **Neu 2.1 Session-Start-Routine (PFLICHT)**:
  - Container-Status pr√ºfen: `docker ps --filter "name=cdb_"`
  - Falls Container fehlen: `docker compose up -d`
  - 10 Sekunden warten und Health-Status pr√ºfen
  - `PROJECT_STATUS.md` lesen vor weiteren Aufgaben
- **2.2 Sicherheits- & Compliance-Regeln** (bestehend, unver√§ndert)

### 2. Abschnitt 6 erweitert: "Arbeitsrichtlinien (Do)"
- **Architekturfluss**: `market_data` ‚Üí `signals` ‚Üí `orders` ‚Üí `order_results`
- **Payload-Validierung**: EVENT_SCHEMA.json Pflicht, √Ñnderungen in models.py spiegeln
- **Logging-Regel**: Nur √ºber `backoffice/logging_config.json` (keine Inline-Logger)
- **Sofortige Dokumentation**: Nach jeder Handlung dokumentieren, nicht erst am Sessionende (entspricht ADR-015)

### 3. Abschnitt 7 erweitert: "Tests & Validierungen"
- **Validierung vor Merge (PFLICHT)** hinzugef√ºgt:
  - `docker compose config` ohne Fehler
  - Services mit Health-Checks gr√ºn (`/health`, `/status`, `/metrics`)
  - `.env` ohne Duplikate; Ports und DB-Name konsistent
  - Schema- und Event-Checks gegen `EVENT_SCHEMA.json`

### 4. Neuer Abschnitt 11: "Audit-Referenzen"
- **Aktuellste Audits** mit konkreten Dateipfaden:
  - `HANDOVER_REVIEW_REPORT_2025-11-02T18-30Z.md` (neuester)
  - `HANDOVER_REPORT_2025-11-02.md`
  - `2025-10-30_RECOVERY_REPORT.md`
  - `AUDIT_SUMMARY.md`, `DIFF-PLAN.md`
- **Audit-Vorgaben**: DIFF-PLAN.md als Quelle nutzen, Abweichungen dokumentieren

**Konsequenzen**:
- ‚ûï Operative Stabilit√§t durch Session-Start-Routine sichergestellt
- ‚ûï Klare Audit-Referenzen f√ºr Nachvollziehbarkeit
- ‚ûï Architekturfluss und Logging-Regeln explizit dokumentiert
- ‚ûï Konkrete Validierungsbefehle vermeiden Fehler vor Merge
- ‚ûï Bestehende Struktur (10 Abschnitte) bleibt erhalten, nur erweitert
- ‚ûñ Datei w√§chst von 80 auf 109 Zeilen (+36%)

**Validation**:
- ‚úÖ Alle 11 Abschnitte vorhanden und korrekt strukturiert
- ‚úÖ Neue Inhalte sinnvoll in bestehende Abschnitte integriert
- ‚úÖ Audit-Dateipfade gegen `backoffice/audits/` validiert
- ‚úÖ Keine bestehenden Inhalte √ºberschrieben oder entfernt

**Referenzen**:
- Issue #6: "Application Adolph" - GitHub Issue mit Copilot-Konfiguration
- ADR-015: Sofortige Handlungsdokumentation im Copilot-Workflow
- ADR-031: Development Philosophy - Quality over Speed
- `backoffice/audits/` - Referenzierte Audit-Dateien

**Sign-Off**: GitHub Copilot  
**G√ºltigkeit**: Ab sofort f√ºr alle Copilot-Sessions

---

## ADR-033: Titel-Norm & Board-Automatisierung aktiviert

**Datum**: 2025-11-09
**Status**: Entwurf / Implemented (tools added: PR Title Lint, Labeler)

Kurz: Standardisierung von PR/Issue-Titeln und Einf√ºhrung leichtgewichtiger Automatisierungen f√ºr das Kanban-Board (Saved Views, Felder, Automationen als Spezifikation). Actions zur Titel-Pr√ºfung und automatisches Labeling wurden als PR zur √úberpr√ºfung hinzugef√ºgt.

Referenzen:
- docs/KANBAN_SETUP.md

---

## ADR-034: Copilot-Instructions Update - Verantwortlicher gesetzt

**Datum**: 2025-11-10  
**Status**: ‚úÖ Beschlossen

**Kontext**: Die Copilot-Instruktionen enthielten bei der Verantwortlichkeit f√ºr das letzte Update den Platzhalter "TBD".

**√Ñnderung**: Aktualisierung der Zeile "Letztes Update" in `.github/copilot-instructions.md`:
- Von: `Verantwortlich: TBD`
- Zu: `Verantwortlich: jannekbuengener`

**Begr√ºndung**: Klare Zuordnung der Verantwortlichkeit f√ºr die Copilot-Instruktionen an den Repository-Owner.

**Konsequenzen**:
- ‚ûï Klare Verantwortlichkeit dokumentiert
- ‚ûï Vollst√§ndige Audit-Trail f√ºr Copilot-Konfiguration
