# Architektur-Entscheidungen (ADR-Style)

## ADR-043: Security-Hardening durch Multi-Layer-Scanning

**Datum**: 2025-11-21
**Status**: âœ… Akzeptiert
**Verantwortlicher**: Claude Code (via CI/CD-Pipeline-Erweiterung)

### Kontext

Nach erfolgreicher Implementierung der Test-Suite (122 Tests, 100%) fehlte eine **systematische Security-PrÃ¼fung** in der CI/CD-Pipeline:

**Probleme**:
1. **Keine automatische Secret-Erkennung**: Risiko versehentlich committeter API-Keys, PasswÃ¶rter
2. **Keine Code-Security-Analyse**: Potenzielle Vulnerabilities (SQL-Injection, XSS, etc.) unerkannt
3. **Keine Dependency-Audits**: Bekannte CVEs in Dependencies wurden nicht geprÃ¼ft
4. **Manueller Prozess**: Security-Checks nur bei expliziter Anforderung
5. **Production-Risk**: Ohne automatisierte Scans hÃ¶heres Risiko fÃ¼r Security-Incidents

**Fragestellung**: Wie integrieren wir systematische Security-Checks in die CI/CD-Pipeline ohne Performance-EinbuÃŸen?

### Entscheidung

**Implementierung einer Multi-Layer-Security-Scanning-Strategie in der CI/CD-Pipeline mit 3 Tools: Gitleaks (Secrets), Bandit (Code), pip-audit (Dependencies).**

**Implementierte MaÃŸnahmen**:

1. **Secret-Scanning (Gitleaks)**:
   - Tool: Gitleaks (Latest Release)
   - Scope: Alle Dateien im Repository
   - Mode: `detect --no-git` (kein Git-History-Scan nÃ¶tig)
   - Blocking: âœ… **JA** - Pipeline schlÃ¤gt fehl bei Secrets
   - Runtime: ~30s

2. **Code-Security-Audit (Bandit)**:
   - Tool: Bandit (SAST fÃ¼r Python)
   - Scope: `services/` Verzeichnis
   - Output: JSON-Report (bandit-report.json)
   - Blocking: âŒ **NEIN** - Nur Warnung (continue-on-error: true)
   - Retention: 30 Tage als GitHub Artifact
   - Runtime: ~20s

3. **Dependency-Audit (pip-audit)**:
   - Tool: pip-audit (PyPI Vulnerability Scanner)
   - Scope: `requirements.txt`
   - Output: JSON-Report (pip-audit.json)
   - Blocking: âŒ **NEIN** - Nur Warnung
   - Retention: 30 Tage als GitHub Artifact
   - Runtime: ~40s

**Pipeline-Integration**:
```yaml
Security-Checks (parallel zu Tests):
- secrets-scan (blocking)
- security-audit (non-blocking)
- dependency-audit (non-blocking)
```

### Konsequenzen

**Positiv**:
- âœ… **Automatisierung**: Security-Checks bei jedem PR/Push
- âœ… **Early Detection**: Secrets/Vulnerabilities vor Merge erkannt
- âœ… **Compliance**: Dokumentierte Security-Reports fÃ¼r Audits
- âœ… **Zero-Config**: Keine False-Positive-Tuning nÃ¶tig (MVP-Phase)
- âœ… **Performance**: Nur ~90s Runtime-Overhead
- âœ… **Nachvollziehbarkeit**: JSON-Reports fÃ¼r 30 Tage verfÃ¼gbar

**Neutral**:
- Bandit/pip-audit sind non-blocking (MVP-Phase)
- False Positives mÃ¶glich (Tuning spÃ¤ter)

**Negativ**:
- Keine signifikanten Nachteile

### Alternativen

1. **CodeQL (GitHub Advanced Security)**:
   - âŒ Abgelehnt: Erfordert GitHub Enterprise (Kosten)
   - âœ… Geplant fÃ¼r Production-Phase

2. **Trivy (Container-Scanning)**:
   - âŒ Verschoben: Erst wenn Docker-Images gebaut werden
   - âœ… Geplant fÃ¼r Phase 3

3. **Snyk/Dependabot**:
   - âœ… ErgÃ¤nzend aktiviert (GitHub-native)
   - pip-audit liefert jedoch direktere Kontrolle

### Compliance

- âœ… **KODEX-konform**: Security-First-Prinzip
- âœ… **OWASP-Alignment**: Covers OWASP Top 10 (A02, A06, A08)
- âœ… **Zero-Trust**: Secrets werden aktiv blockiert
- âœ… **Audit-Trail**: Reports fÃ¼r Compliance-Nachweise

---

## ADR-042: Test-Strategie mit 3-Tier-Architektur und Coverage-Anforderungen

**Datum**: 2025-11-21
**Status**: âœ… Akzeptiert
**Verantwortlicher**: Claude Code (via CI/CD-Pipeline-Erweiterung)

### Kontext

Nach erfolgreicher Implementierung von 122 Tests (100% Pass Rate) fehlte eine **formalisierte Test-Strategie** und Coverage-Enforcement:

**Probleme**:
1. **Keine Coverage-Messung in CI**: Unklare Code-Coverage, kein automatisches Tracking
2. **Keine klare Test-Kategorisierung**: Unit/Integration/E2E nicht systematisch getrennt
3. **Fehlende Coverage-Thresholds**: Keine Mindestanforderungen definiert
4. **E2E-Tests in CI**: Gefahr langsamer Pipelines durch Container-Tests
5. **Manuelle Validierung**: Coverage nur lokal prÃ¼fbar

**Fragestellung**: Wie strukturieren wir Tests systematisch und stellen hohe Coverage sicher, ohne CI-Performance zu beeintrÃ¤chtigen?

### Entscheidung

**Implementierung einer 3-Tier-Test-Architektur (Unit, Integration, E2E) mit automatischer Coverage-Messung in CI und klarer Trennung zwischen CI- und Lokal-Tests.**

**Test-Strategie**:

1. **Tier 1: Unit-Tests** (CI + Lokal):
   - Marker: `@pytest.mark.unit`
   - Scope: Einzelne Funktionen/Klassen isoliert
   - Dependencies: Nur Mocks (keine echten Services)
   - Runtime-Target: <1s pro Test
   - CI-AusfÃ¼hrung: âœ… **JA**

2. **Tier 2: Integration-Tests** (CI + Lokal):
   - Marker: `@pytest.mark.integration`
   - Scope: Service-Interaktionen mit Mock-Services
   - Dependencies: Mock-Redis, Mock-PostgreSQL
   - Runtime-Target: <10s pro Test
   - CI-AusfÃ¼hrung: âœ… **JA**

3. **Tier 3: E2E-Tests** (NUR Lokal):
   - Marker: `@pytest.mark.e2e` + `@pytest.mark.local_only`
   - Scope: VollstÃ¤ndige Event-Flows mit echten Containern
   - Dependencies: docker-compose (Redis, PostgreSQL, alle Services)
   - Runtime-Target: <60s pro Test
   - CI-AusfÃ¼hrung: âŒ **NEIN**
   - Grund: Performance, Resource-Limits, Flakiness

**Coverage-Requirements**:
```yaml
CI-Pipeline:
  - pytest -m "not e2e and not local_only" --cov=services
  - Target: >80% (noch nicht enforced in MVP)
  - Reports: HTML + XML + Terminal
  - Matrix: Python 3.11 & 3.12
  - Artifacts: 30 Tage Retention
```

**Test-Isolation**:
```python
# CI-Tests (schnell, isoliert)
pytest -v -m "not e2e and not local_only"

# Lokale E2E-Tests (mit Docker)
pytest -v -m e2e
```

### Konsequenzen

**Positiv**:
- âœ… **CI-Performance**: <2min fÃ¼r alle CI-Tests (103 Tests)
- âœ… **Coverage-Visibility**: Automatische Reports bei jedem PR
- âœ… **Klare Trennung**: Entwickler wissen, welche Tests wo laufen
- âœ… **E2E-FlexibilitÃ¤t**: Lokal testbar, CI nicht blockiert
- âœ… **Matrix-Testing**: Python 3.11 & 3.12 parallel
- âœ… **Artifact-Retention**: Coverage-Reports 30 Tage verfÃ¼gbar

**Neutral**:
- E2E-Tests mÃ¼ssen manuell lokal ausgefÃ¼hrt werden
- Coverage-Threshold noch nicht enforced (MVP-Phase)

**Negativ**:
- Keine signifikanten Nachteile

### Alternativen

1. **E2E-Tests in CI ausfÃ¼hren**:
   - âŒ Abgelehnt: Zu langsam (~10min), Flakiness-Risiko
   - âœ… Lokal-only ist besser fÃ¼r MVP-Phase

2. **Mutation-Testing (mutmut)**:
   - âŒ Verschoben: Erst nach 80% Coverage
   - âœ… Geplant fÃ¼r Phase 2

3. **Property-Based Testing (Hypothesis)**:
   - âœ… Bereits implementiert (in Integration-Tests)
   - Weiterhin verwenden

### Compliance

- âœ… **KODEX-konform**: Test-Pyramide beachtet
- âœ… **Coverage-Target**: >80% dokumentiert (Enforcement spÃ¤ter)
- âœ… **Marker-System**: pytest.ini definiert alle Marker
- âœ… **Dokumentation**: TESTING_GUIDE.md vollstÃ¤ndig

---

## ADR-041: CI/CD-Pipeline-Architektur mit 8-Job-Design

**Datum**: 2025-11-21
**Status**: âœ… Akzeptiert
**Verantwortlicher**: Claude Code (via CI/CD-Pipeline-Erweiterung)

### Kontext

Die initiale CI/CD-Pipeline (ci.yaml) bestand aus **4 einfachen Jobs** (Lint, Test, Secrets, Security) ohne Coverage-Reporting, Type-Checking oder strukturierte Reports:

**Probleme**:
1. **Fehlende Coverage-Messung**: Keine automatische Code-Coverage-Analyse
2. **Kein Type-Checking**: mypy nicht in CI integriert
3. **Keine Dependency-Audits**: Bekannte Vulnerabilities unerkannt
4. **Keine Dokumentations-Checks**: Markdown-QualitÃ¤t nicht geprÃ¼ft
5. **Single Python-Version**: Nur Python 3.12, keine KompatibilitÃ¤tsprÃ¼fung
6. **Keine Artifacts**: Reports nicht fÃ¼r Analyse verfÃ¼gbar
7. **Fehlende Zusammenfassung**: Kein aggregierter Build-Status

**Fragestellung**: Wie erweitern wir die CI/CD-Pipeline um umfassende QualitÃ¤ts- und Security-Checks, ohne die Performance drastisch zu verschlechtern?

### Entscheidung

**Implementierung einer 8-Job-CI/CD-Pipeline mit paralleler AusfÃ¼hrung, Build-Matrix, Artifact-Management und aggregiertem Build-Summary.**

**Pipeline-Architektur**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      CODE QUALITY (parallel)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Linting (Ruff)                      â”‚
â”‚  2. Format Check (Black)                â”‚
â”‚  3. Type Checking (mypy)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TESTS (matrix)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. Tests (Python 3.11 & 3.12)          â”‚
â”‚     - Coverage Reports (HTML + XML)     â”‚
â”‚     - Artifacts (30 Tage)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SECURITY CHECKS (parallel)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. Secret Scanning (Gitleaks)          â”‚
â”‚  6. Security Audit (Bandit)             â”‚
â”‚  7. Dependency Audit (pip-audit)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       DOCUMENTATION (parallel)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  8. Docs Check (markdownlint)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          BUILD SUMMARY                  â”‚
â”‚  (aggregiert alle Job-Results)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Job-Details**:

1. **Linting (Ruff)**:
   - GitHub-Format-Output (inline annotations)
   - Blocking: âœ… **JA**

2. **Format Check (Black)**:
   - Check-only Mode (kein Auto-Fix)
   - Blocking: âœ… **JA**

3. **Type Checking (mypy)**:
   - Scope: `services/` nur
   - Blocking: âŒ **NEIN** (continue-on-error: true)
   - Grund: MVP-Phase, Type-Coverage noch niedrig

4. **Tests (Matrix)**:
   - Python 3.11 & 3.12 parallel
   - Coverage: HTML + XML + Terminal
   - Artifacts: 30 Tage Retention
   - Blocking: âœ… **JA**

5-7. **Security-Checks**:
   - Siehe ADR-043
   - Parallel zu Tests ausfÃ¼hrbar

8. **Docs Check**:
   - markdownlint fÃ¼r alle `.md` Dateien
   - Config: `.markdownlintrc`
   - Blocking: âŒ **NEIN** (MVP-Phase)

9. **Build Summary**:
   - LÃ¤uft immer (auch bei Fehlern)
   - Aggregiert Status aller Jobs
   - GitHub Step Summary

**Performance-Optimierung**:
- Pip-Cache aktiviert (`cache: 'pip'`)
- Parallele Job-AusfÃ¼hrung
- fail-fast: false (alle Versionen testen)

**Runtime-Targets**:
- Total: ~8 Minuten
- Tests: ~1.5 Minuten
- Security: ~1 Minute
- Code Quality: ~1 Minute

### Konsequenzen

**Positiv**:
- âœ… **Umfassende QualitÃ¤tsprÃ¼fung**: 8 verschiedene Checks
- âœ… **Coverage-Visibility**: Automatische Reports
- âœ… **Multi-Version-Support**: Python 3.11 & 3.12
- âœ… **Security-Integration**: Secrets, Code, Dependencies
- âœ… **Artifact-Management**: Reports 30 Tage verfÃ¼gbar
- âœ… **Performance**: Nur ~8min Gesamtlaufzeit
- âœ… **Dokumentation**: Umfassende CI_CD_GUIDE.md (9.000+ WÃ¶rter)

**Neutral**:
- mypy und Docs-Check sind non-blocking (MVP-Phase)
- Coverage-Threshold noch nicht enforced

**Negativ**:
- HÃ¶here KomplexitÃ¤t (8 statt 4 Jobs)
- Mehr Maintenance-Aufwand

### Alternativen

1. **Monolithischer Job**:
   - âŒ Abgelehnt: Schlechte Fehlerdiagnose, langsamer
   - âœ… Parallele Jobs sind besser

2. **External CI-Services (CircleCI, Travis)**:
   - âŒ Abgelehnt: GitHub Actions ist native, kostenlos
   - âœ… GitHub Actions gewÃ¤hlt

3. **Self-Hosted Runners**:
   - âŒ Verschoben: Erst bei Performance-Problemen
   - âœ… GitHub-Hosted Runner ausreichend (MVP-Phase)

### Compliance

- âœ… **KODEX-konform**: QualitÃ¤ts-Standards eingehalten
- âœ… **Dokumentation**: CI_CD_GUIDE.md vollstÃ¤ndig
- âœ… **Artifact-Retention**: 30 Tage (ausreichend fÃ¼r MVP)
- âœ… **Security-Integration**: Multi-Layer-Scanning

---

## ADR-040: Dokumentations-Konsolidierung und Strukturbereinigung

**Datum**: 2025-11-20
**Status**: âœ… Akzeptiert
**Verantwortlicher**: Claude Code (via Dokumentations-Audit)

### Kontext

Nach erfolgreicher Implementierung der E2E-Test-Suite (2025-11-19) existierten **redundante und veraltete Dokumentationsdateien** im Repository-Root:

**Probleme**:
1. **Redundante Test-Dokumentation**: 4 separate Dateien (`TESTING.md`, `TEST_GUIDE.md`, `PYTEST_LAYOUT.md`, `CLAUDE_CODE_START.md`) mit Ã¼berlappenden Inhalten
2. **Veraltete Status-Dateien**: `CLAUDE_TODO.md`, `PR_NOTES.md`, `DECISION_LOG.md` (Root-Duplikat) waren erledigt/veraltet
3. **Unstrukturierte Ablage**: Wichtige Reports (`E2E_TEST_COMPLETION_REPORT.md`) lagen im Root statt in `backoffice/docs/`
4. **Inkonsistente Links**: Mehrere Dokumente verwiesen auf gelÃ¶schte/verschobene Dateien
5. **Verwirrung fÃ¼r neue Entwickler**: 11 MD-Dateien im Root, unklare PrioritÃ¤ten

**Fragestellung**: Wie strukturieren wir die Dokumentation klar, vermeiden Duplikate und erleichtern Navigation?

### Entscheidung

**Alle Test-Dokumentation wird in `tests/README.md` und `backoffice/docs/testing/` konsolidiert. Redundante Root-Dateien werden gelÃ¶scht.**

**DurchgefÃ¼hrte MaÃŸnahmen**:

1. **GelÃ¶schte redundante Dateien** (7 Dateien):
   - `DECISION_LOG.md` - Duplikat zu `backoffice/docs/DECISION_LOG.md`
   - `CLAUDE_TODO.md` - Alte Aufgabenliste (erledigt)
   - `PR_NOTES.md` - Alte PR-Notizen (in Git-History)
   - `PYTEST_LAYOUT.md` - Minimal, redundant mit `tests/README.md`
   - `CLAUDE_CODE_START.md` - Alte Pytest-Briefing-Datei (Task erledigt)
   - `TESTING.md` - Veraltet, konsolidiert in `tests/README.md`
   - `TEST_GUIDE.md` - Minimal, redundant

2. **Verschobene Dateien** (2 Dateien):
   - `E2E_TEST_COMPLETION_REPORT.md` â†’ `backoffice/docs/testing/E2E_TEST_COMPLETION_REPORT.md`
   - `CLAUDE_GORDON_PIPELINE.md` â†’ `backoffice/docs/runbooks/CLAUDE_GORDON_WORKFLOW.md`

3. **Aktualisierte Links** (3 Dateien):
   - `backoffice/PROJECT_STATUS.md` - Link zu CLAUDE_GORDON_WORKFLOW.md
   - `backoffice/docs/CLAUDE_CODE_BRIEFING.md` - Verweise auf `tests/README.md` und `LOCAL_E2E_TESTS.md`
   - `README.md` - VollstÃ¤ndige Pfade zu allen Dokumenten

4. **Neue Dokumentations-Struktur**:
   - **Root-Level**: Nur essenzielle Dateien (`CLAUDE.md`, `README.md`)
   - **Test-Dokumentation**: `tests/README.md` + `backoffice/docs/testing/`
   - **Runbooks**: `backoffice/docs/runbooks/`
   - **Architektur**: `backoffice/docs/architecture/`

### Konsequenzen

**Positiv**:
- âœ… **Klarheit**: Reduzierung von 11 auf 4 Root-MD-Dateien (-64%)
- âœ… **Navigation**: Klare Struktur, alle Links funktionieren
- âœ… **Wartbarkeit**: Keine Duplikate mehr, Single Source of Truth
- âœ… **Onboarding**: Neue Entwickler finden Dokumentation schneller

**Neutral**:
- Historische Informationen bleiben in Git-History verfÃ¼gbar
- Alte Links in externen Dokumenten mÃ¼ssen ggf. aktualisiert werden

**Negativ**:
- Keine signifikanten Nachteile

### Compliance

- âœ… **KODEX-konform**: Dokumentation folgt Single-Source-Prinzip
- âœ… **Archiv-Regel eingehalten**: Keine Archive geÃ¤ndert
- âœ… **Git-History**: Alle gelÃ¶schten Inhalte bleiben nachvollziehbar

---

## ADR-039: Cleanroom-Repository als kanonische Codebasis etabliert

**Datum**: 2025-01-17
**Status**: âœ… Akzeptiert
**Verantwortlicher**: jannekbuengener (via Nullpunkt-Definition-Workflow)

### Kontext

Nach erfolgreicher Migration vom Backup-Repository in das Cleanroom-Repository (2025-11-16) und Abschluss aller Kanonisierungs-Pipelines existierte eine **ambivalente Dokumentationslage**:

**Probleme**:
1. **Namensinkonsistenz**: 28 Dateien verwendeten noch "Claire de Binare" statt "Claire de Binare"
2. **Status-Verwirrung**: Cleanroom wurde in vielen Dokumenten als "Ziel-Repo" oder "migrations-bereit" beschrieben, obwohl die Migration bereits erfolgt war
3. **Redundante Migrations-Dokumente**: 6 Dokumente (MIGRATION_READY.md, PRE_MIGRATION_*.md, CLEANROOM_MIGRATION_MANIFEST.md) beschrieben die Migration als bevorstehende Aktion
4. **Unklare Single Source of Truth**: Unklar, ob `backoffice/docs/` oder Root-Dateien die gÃ¼ltige Version darstellten

**Fragestellung**: Wie etablieren wir das Cleanroom-Repository eindeutig als aktuellen, kanonischen Stand und vermeiden zukÃ¼nftige Verwirrung?

### Entscheidung

**Das Cleanroom-Repository (`Claire_de_Binare_Cleanroom`) ist ab 2025-01-17 die einzige kanonische Codebasis und Dokumentationsquelle des Projekts.**

**DurchgefÃ¼hrte MaÃŸnahmen**:

1. **Namens-Normalisierung**:
   - Datei `backoffice/docs/KODEX â€“ Claire de Binare.md` â†’ `KODEX â€“ Claire de Binare.md`
   - Alle Vorkommen von "Claire de Binare" im Projektkontext â†’ "Claire de Binare"
   - Technische IDs (`claire_de_binare`) bleiben unverÃ¤ndert
   - Hinweis in KODEX ergÃ¤nzt: "FrÃ¼here Dokumente verwenden teilweise 'Claire de Binare'; gilt als historisch"

2. **Nullpunkt-Definition**:
   - `PROJECT_STATUS.md`: Phase auf "N1 - Paper-Test-Vorbereitung" aktualisiert (100% Cleanroom etabliert)
   - `EXECUTIVE_SUMMARY.md`: Status von "migrations-bereit" â†’ "ABGESCHLOSSEN - CLEANROOM AKTIV"
   - Historischer Kontext ergÃ¤nzt: Migration vom 2025-11-16 ist abgeschlossen
   - NÃ¤chste Schritte fokussieren auf N1-Phase (siehe `N1_ARCHITEKTUR.md`)

3. **Migrations-Dokumente historisiert**:
   - Alle PRE_MIGRATION_* und MIGRATION_READY-Dokumente als "Historische Migration 2025-11-16" gekennzeichnet
   - Migration-Scripts (`cleanroom_migration_script.ps1`) als **Template/Referenz** fÃ¼r zukÃ¼nftige Migrationen deklariert
   - Keine aktiven Aufforderungen mehr, "Migration auszufÃ¼hren"

4. **Archiv-Struktur bestÃ¤tigt**:
   - `archive/sandbox_backups/`: Historische Sandbox-Umgebung, keine Ã„nderungen
   - `archive/docs_original/`: Alte Root-Dateien, keine weiteren Duplikate erlaubt
   - Root-Dokumente (DECISION_LOG, KODEX): Nur `backoffice/docs/` ist gÃ¼ltig

5. **N1-Architektur als nÃ¤chste Phase**:
   - `N1_ARCHITEKTUR.md` definiert Paper-Test-Phase als aktuelles Ziel
   - KODEX ergÃ¤nzt um Phasenmodell: N1 (Paper-Test) vs. Produktion
   - PROJECT_STATUS listet N1-Tasks als "NÃ¤chste Schritte"

### BegrÃ¼ndung

**Warum jetzt?**
- Cleanroom-Migration ist seit 2 Monaten abgeschlossen, aber Dokumentation reflektierte dies nicht
- Neue Team-Mitglieder oder KI-Agenten kÃ¶nnten durch "migrations-bereit"-Formulierungen verwirrt werden
- Vorbereitung fÃ¼r N1-Phase erfordert klaren, stabilen Ausgangspunkt

**Warum "Binare" statt "Binaire"?**
- Konsistente MarkenidentitÃ¤t ohne AmbiguitÃ¤t
- Technische IDs (`claire_de_binare`) beibehalten fÃ¼r StabilitÃ¤t
- Historische Dokumente bewusst nicht retroaktiv geÃ¤ndert (Archiv bleibt original)

**Warum Migrations-Docs nicht lÃ¶schen?**
- Wertvolle Templates fÃ¼r zukÃ¼nftige Repo-Migrationen
- Dokumentieren den erfolgreichen Kanonisierungs-Prozess
- KÃ¶nnten fÃ¼r andere Projekte wiederverwendet werden

### Konsequenzen

**Positiv**:
- â• **Eindeutige Single Source of Truth**: `backoffice/docs/` ist die kanonische Dokumentation
- â• **Vereinfachtes Onboarding**: Neue Contributors sehen sofort, dass Cleanroom der aktuelle Stand ist
- â• **Klare Phasen-Trennung**: Migration (abgeschlossen) vs. N1 (aktuell) vs. Produktion (zukÃ¼nftig)
- â• **Konsistente Namensgebung**: "Claire de Binare" als verbindliche Projektbezeichnung

**Neutral**:
- â—¼ï¸ Historische Dokumente in `archive/` behalten alte Schreibweise "Binaire" (bewusst)
- â—¼ï¸ Migration-Scripts bleiben unter `scripts/migration/` als Templates

**Risiken**:
- âš ï¸ Externe Links oder Referenzen kÃ¶nnten noch "Binaire" verwenden â†’ bei Bedarf manuell aktualisieren
- âš ï¸ Falls Root-Duplikate (KODEX, DECISION_LOG) auftauchen â†’ sofort nach `archive/docs_original/` verschieben

### NÃ¤chste Schritte

1. âœ… ADR-039 in DECISION_LOG integriert
2. â³ CLEANROOM_BASELINE_SUMMARY.md erstellen (Ãœbersicht aller Ã„nderungen)
3. â³ Alle verbleibenden Docs mit "Binaire" aktualisieren (Service-Docs, Schema, etc.)
4. â³ N1-Phase starten: Test-Infrastruktur aufsetzen (siehe PROJECT_STATUS.md)

---

## ADR-009: Security Rerun Automation & Evidence Pipeline

**Datum**: 2025-11-11  
**Status**: âœ… Abgeschlossen  
**Kontext**: Sicherheits-Gates blockierten Releases, da Artefakte (Trivy, Gitleaks, Bandit) nicht konsolidiert waren und Nachweise (.env Hash, CVE-Vergleich, Reviewer-Checkliste) fehlten.

-**Entscheidung**: 
- Automatisierte Skripte (`scripts/scan_ports.py`, `scripts/log_parser.py`, `scripts/bandit_postprocess.py`, `scripts/verify_cve_fix.sh`, `scripts/run_hardening.py`, `scripts/cve_triage.py`) generieren Ports-, Logs-, Bandit- und CVE-Artefakte unter `artifacts/`.
- Neues Makefile erweitert um Guarded Targets (`deps_fix`, `trivy_local`, `trivy_triage`, `bandit`, `bandit_gate`, `gitleaks`, `gitleaks_gate`, `verify_cve`, `evidence_review`, `gates`); optionaler Registry-Vergleich via `REGISTRY_IMG` (`make trivy_registry` dokumentiert als Plan).
- `.github/REVIEW_TEMPLATE.md` standardisiert Reviewer-Checkpunkte inkl. Bandit-`justified`-Abnahme und Artefakt-Links.
- `requirements.lock` (per `pip freeze --require-virtualenv`) dient Audit-Nachweis; Evidence-Datei enthÃ¤lt SHA256 der lokalen `.env` und Gitignore-Kontrolle.

-**Ergebnis**:
- Trivy- und Pip-Audit-Daten werden Ã¼ber `scripts/verify_cve_fix.sh` abgeglichen (Pins `aiohttp==3.12.14`, `cryptography==42.0.4` bestÃ¤tigt, HIGH/CRITICAL fÃ¼r lokale Images aktuell 120; `scripts/cve_triage.py` liefert JSON/Markdown-Matrix zur weiteren Triage).
- Bandit-Report erhÃ¤lt `justified`-Feld (Mapping via `scripts/bandit_justification.json` mÃ¶glich); `unjustified_check.json` zeigt aktuell 37 offene HIGH/MEDIUM-Funde und blockiert Gates bis zur Abnahme.
- Gitleaks lÃ¤uft mit gepflegter `.gitleaks.toml`; sowohl PrimÃ¤r- als auch Post-Clean-Scan liefern 0 Treffer, Gate bleibt grÃ¼n.
- Ports-Scan & Log-Parser erzeugen JSON/Markdown-Artefakte fÃ¼r Reviewer; Evidence-Skript schreibt `evidence/TEST_RERUN_EVIDENCE_<DATE>.md` inklusive automatisiertem Review-Block und PR-Draft unter `artifacts/pr/`.
- Dokumentierter Plan: Registry-Scan (`make trivy_registry`) bleibt optional, Ergebnisse sollen kÃ¼nftig gegen lokale Pins verglichen und im Evidence-Text referenziert werden.

**Konsequenzen**:
- â• Wiederholbare Security-Runs mit einheitlichem Artefakt-Layout (`artifacts/security/*`, `artifacts/runtime/*`).
- â• Reviewer-Workflow beschleunigt (Checkliste + `justified`-Flag als PflichtprÃ¼fung).
- â• CVE-Evidence kombiniert lokale Scans (Trivy) und Dependency-Audits (pip-audit, safety) mit JSON-Zusammenfassung.
- â– Trivy meldet aktuell 120 HIGH/CRITICAL Findings in Basis-Images â†’ Folgeaufgabe: Registry-Scan + Upstream-Fix-Analyse.
- ğŸ”„ NÃ¤chste Schritte: `scripts/bandit_justification.json` pflegen (false-positive Tracking) und bei VerfÃ¼gbarkeit `REGISTRY_IMG` setzen, um lokale vs. Registry-Images im Evidence zu vergleichen.

## ADR-008: Tool Stack - Development & Management Tools

**Datum**: 2025-11-03  
**Status**: âœ… Abgeschlossen  
**Kontext**: Nach Implementierung von CDB (Business Logic) und MCP (Monitoring) fehlten Verwaltungs- und Entwicklungstools fÃ¼r effizientes Container-Management, Datenbank-Administration und Ressourcen-Ãœberwachung.

**Entscheidung**: Separater Tool-Stack mit 5 spezialisierten Tools:

1. **Portainer** (portainer-ce:latest) - Docker Management UI
   - Container, Images, Volumes, Networks verwalten
   - Terminal-Zugriff (exec) in Container
   - Stack-Management & Logs
   
2. **pgAdmin** (dpage/pgadmin4:latest) - PostgreSQL UI
   - VollstÃ¤ndige Datenbank-Administration fÃ¼r cdb_postgres
   - Query-Tool mit Syntax-Highlighting
   - Backup/Restore-Funktionen
   
3. **Dozzle** (amir20/dozzle:latest) - Docker Logs Viewer
   - Real-time Log-Streaming aller Container
   - Multi-Container-Suche mit Regex
   - Kein Login nÃ¶tig (localhost-only)
   
4. **Adminer** (adminer:latest) - Lightweight SQL UI
   - Schnelle DB-Queries ohne pgAdmin-Overhead
   - Single-File PHP App
   - UnterstÃ¼tzt PostgreSQL, MySQL, SQLite
   
5. **cAdvisor** (gcr.io/cadvisor/cadvisor:latest) - Resource Monitoring
   - Container CPU/Memory/Network/Disk-Usage
   - Live-Metriken & historische Graphen
   - Prometheus-Integration (Scrape-Target)

**BegrÃ¼ndung**:

- **Naming Convention:** Alle Container mit `tool_` PrÃ¤fix fÃ¼r sofortige Identifikation (analog zu `cdb_` und `mcp_`)
- **Dual-Network:** Alle Tools hÃ¤ngen in `tools_net` (intern) UND `cdb_network` (shared) fÃ¼r direkten Zugriff auf CDB/MCP-Services
- **No Authentication (localhost):** Dozzle und cAdvisor ohne Login, da nur auf localhost exponiert (Production: Reverse-Proxy mit Auth)
- **cAdvisor statt Prometheus Node-Exporter:** cAdvisor bietet Container-spezifische Metriken, Node-Exporter nur Host-Metriken

**Ports (alle localhost):**
- 9000: Portainer
- 5050: pgAdmin
- 9999: Dozzle (Logs)
- 8085: Adminer (SQL)
- 8080: cAdvisor

**Implementierung**:

- Compose-Datei: `docker/tools/docker-compose.tools.yml`
- Environment: `docker/tools/.env` (pgAdmin-Credentials)
- Volumes: `tool_portainer_data`, `tool_pgadmin_data` (persistent)
- Labels: `com.cdb.role=tool`, `com.cdb.service=<name>` fÃ¼r alle Container

**Ergebnis**:

- 5 Tool-Container operational
- Direkte Verbindung zu cdb_postgres (pgAdmin, Adminer)
- Real-time Logs aller CDB/MCP-Container (Dozzle)
- Container-Metriken in Prometheus (cAdvisor @ tool_resourceusage:8080)
- Deployment-Script: `docker/tools/deploy.ps1` (Pre-Flight Checks, Backup, Health-Checks)

**Konsequenzen**:

- â• **Developer Experience:** Grafische UIs statt CLI (pgAdmin > psql, Portainer > docker ps)
- â• **Debugging:** Dozzle ermÃ¶glicht schnelle Log-Suche Ã¼ber alle Container (kein `docker logs` nÃ¶tig)
- â• **Resource-Awareness:** cAdvisor zeigt Memory-Leaks und CPU-Spikes sofort
- â• **Self-Service:** Entwickler kÃ¶nnen ohne Root-Zugriff Container verwalten (Portainer)
- â– **ZusÃ¤tzliche Ressourcen:** 5 Container benÃ¶tigen ~500 MB RAM
- âš ï¸ **Security:** Portainer/pgAdmin PasswÃ¶rter in `.env` (nicht committed), localhost-only Exposition empfohlen

**Integration mit MCP:**

```yaml
# In docker/mcp/prometheus/prometheus.yml:
- job_name: 'cadvisor'
  static_configs:
    - targets: ['tool_resourceusage:8080']
```

â†’ Container-Metriken direkt in Prometheus & Grafana verfÃ¼gbar

**Dokumentation**: `docker/tools/README_TOOLS.md` (vollstÃ¤ndige Tool-Beschreibungen, Setup-Guides, Troubleshooting)

**Referenzen**:
- Portainer: https://docs.portainer.io/
- pgAdmin: https://www.pgadmin.org/docs/
- Dozzle: https://github.com/amir20/dozzle
- Adminer: https://www.adminer.org/
- cAdvisor: https://github.com/google/cadvisor

---

## ADR-007: MCP Observability Stack - Monitoring & Alerting

**Datum**: 2025-11-03  
**Status**: âœ… Abgeschlossen  
**Kontext**: Nach Docker MVP (ADR-006) fehlte vollstÃ¤ndige Observability-Infrastruktur fÃ¼r Metriken, Logs und Alerts. Produktions-Readiness erfordert Monitoring aller 8 CDB-Services, Alert-Pipeline und Log-Aggregation.

**Entscheidung**: Separater MCP (Monitoring/Control-Plane) Stack mit folgenden Komponenten:
- **Prometheus** (v2.54.1) - Metriken-Sammlung, 15d Retention
- **Alertmanager** (v0.27.0) - Alert-Routing, Slack-Integration
- **Grafana** (11.3.0) - Visualisierung
- **Loki** (3.2.0) - Log-Aggregation, 15d Retention
- **Promtail** (3.2.0) - Docker-Log-Collection
- **Redis Exporter** (v1.63.0) - Redis-Metriken
- **Postgres Exporter** (v0.15.0) - PostgreSQL-Metriken

**BegrÃ¼ndung**:
- Separate Compose-Datei (`docker-compose.observability.yml`) fÃ¼r klare Trennung von Business-Logic (CDB) und Observability (MCP)
- Shared Network (`cdb_network`) fÃ¼r direkte Service-Discovery ohne Port-Exposition
- Prefix `mcp_` fÃ¼r alle MCP-Container zur sofortigen Identifikation
- 15-Tage-Retention als Balance zwischen Disk Space und Compliance
- Slack-Integration fÃ¼r Alert-Routing (Critical, Warning, Infrastructure)

**Implementierung**:
1. **Alert Rules (15+ konfiguriert)**:
   - ServiceDown, HighCPU, HighMemory, DiskSpaceLow
   - RedisBackpressure (evicted_keys > 100 oder memory > 80%)
   - PostgreSQLDown, PrometheusDown, LokiDown
   - NoAlertsReceived (Watchdog-Meta-Alert)

2. **Automation Scripts (PowerShell)**:
   - `deploy.ps1` - Full-Deployment mit Pre-Flight Checks
   - `sanity-check.ps1` - 8 Validierungskategorien (Container, API, Volumes, Network)
   - `fire-drill.ps1` - Alert-Pipeline-Test (Alertmanager â†’ Slack)
   - `test-log-pipeline.ps1` - Loki-Ingestion-Validierung

3. **Dokumentation**:
   - `README.md` (10+ Seiten) - VollstÃ¤ndige Referenz mit Mini-Runbooks fÃ¼r 5 hÃ¤ufige Alerts
   - `QUICK_START.md` - 5-Minuten-Installation mit Slack-Setup
   - Troubleshooting-Guides fÃ¼r ServiceDown, RedisBackpressure, PrometheusDown, LokiDown

**Ergebnis**:
- 7 MCP-Container operational (Prometheus, Alertmanager, Grafana, Loki, Promtail, Redis Exporter, Postgres Exporter)
- 10+ Prometheus-Targets konfiguriert (CDB-Services, Redis, PostgreSQL, MCP-Services selbst)
- Slack-Integration aktiv (3 Alert-Kategorien: critical, warning, infrastructure)
- Log-Pipeline validiert (Docker â†’ Promtail â†’ Loki â†’ Grafana Explore)
- Fire-Drill-Tests bestanden (Alert-Fire & Resolve funktionsfÃ¤hig)
- Retention: 15 Tage fÃ¼r Prometheus + Loki

**Konsequenzen**:
- â• **Produktions-Readiness**: VollstÃ¤ndige Observability fÃ¼r alle CDB-Services
- â• **Proaktive Alerts**: Slack-Benachrichtigungen bei Service-Problemen (< 1min Latenz)
- â• **Root-Cause-Analysis**: Logs in Loki + Metriken in Prometheus ermÃ¶glichen schnelles Debugging
- â• **Automatisierte Validierung**: Sanity-Checks in < 60 Sekunden durchfÃ¼hrbar
- â• **Self-Monitoring**: MCP Ã¼berwacht sich selbst (PrometheusDown, LokiDown Alerts)
- â– **ZusÃ¤tzliche Ressourcen**: 7 Container benÃ¶tigen ~1-2 GB RAM und ~500 MB Disk pro Tag
- ğŸ”„ **NÃ¤chste Schritte**: Grafana-Dashboards importieren, Alert-Tuning nach Produktion-Load

**Technische Details**:
- **Netzwerk**: Shared `cdb_network` (bridge) - keine separaten Netze, direkte Service-Discovery
- **Volumes**: 3 persistente Volumes (`mcp_prometheus_data`, `mcp_grafana_data`, `mcp_loki_data`)
- **Ports**: 9090 (Prometheus), 9093 (Alertmanager), 3000 (Grafana), 3100 (Loki), 9080 (Promtail)
- **Secrets**: Credentials in `.env` (nicht committed), Template in `.env.example`
- **Health-Checks**: Alle Container mit Health-Check konfiguriert (interval: 30s, timeout: 10s)

**Dokumentation**: `docker/mcp/README.md`, `docker/mcp/QUICK_START.md`, `backoffice/CHECKPOINT_INDEX.md` (MCP-Abschnitt)

**Referenzen**:
- Prometheus-Dokumentation: https://prometheus.io/docs/
- Loki-Dokumentation: https://grafana.com/docs/loki/
- Alertmanager-Routing: https://prometheus.io/docs/alerting/latest/configuration/

---

## ADR-006: Docker MVP Complete - Checkpoint Reset/Joined

**Datum**: 2025-11-03  
**Status**: âœ… Abgeschlossen  
**Kontext**: VollstÃ¤ndige Implementierung aller 6 Kern-Services mit Docker, inklusive Health-Checks, korrekter ENV-Konfiguration und vollstÃ¤ndigem DB-Schema.  
**Entscheidung**: Alle Services mit `cdb_` PrÃ¤fix, einheitliche Port-Struktur (8000-8003 fÃ¼r Services), vollstÃ¤ndige Healthcheck-Integration.  
**Ergebnis**:
- 8 Container running & healthy (redis, postgres, prometheus, grafana, ws, core, risk, execution)
- 6 persistente Volumes
- 11 DB-Tabellen/Views geladen
- Alle ENV-Keys vollstÃ¤ndig konfiguriert
- Health-Endpoints auf allen Services verfÃ¼gbar

**Dokumentation**: `backoffice/CHECKPOINT_RESET_JOINED_2025-11-03.md`  
**Konsequenzen**:
- â• Stabiler Ausgangspunkt fÃ¼r E2E-Tests
- â• VollstÃ¤ndige Nachvollziehbarkeit aller Build-Artefakte
- â• Klare Service-Hierarchie und Dependencies
- ğŸ”„ NÃ¤chster Schritt: Redis Pub/Sub Tests & Pipeline-Validierung

---

## ADR-001: Message-Bus-Wahl (Redis statt NATS)

**Datum**: 2025-01-XX  
**Status**: âœ… Beschlossen  
**Kontext**: Brauchten Pub/Sub fÃ¼r Service-Kommunikation  
**Entscheidung**: Redis (simpler Setup, direkt in Docker)  
**Konsequenzen**:

- â• Kein zusÃ¤tzlicher Infra-Stack
- â• Persistenz mÃ¶glich (List/Stream)
- â– Weniger Features als NATS (kein Clustering)

## ADR-002: SQLite fÃ¼r MVP

**Datum**: 2025-01-XX  
**Status**: âœ… Beschlossen  
**Kontext**: Datenbank fÃ¼r Audit-Trail  
**Entscheidung**: SQLite embedded, spÃ¤ter PostgreSQL  
**Konsequenzen**:
- â– Single-Writer-Limitation
- ğŸ”„ Migration auf Postgres bei Multi-Instance

## ADR-003: Telegram-Alerts deprecated

**Kontext**: Roadmap fordert interne Push-LÃ¶sung  
**Entscheidung**: PrimÃ¤r Web-Push (VAPID), Telegram nur Legacy  
**Konsequenzen**:

- â• Datenschutz (kein Drittanbieter-Zwang)
- â• Konsistent mit Roadmap-Vision

## ADR-004: Backup-Skripte zentral in operations/backup

**Datum**: 2025-10-25  
**Status**: âœ… Beschlossen  
**Kontext**: Mehrere Backup-Skripte/Anleitungen existierten doppelt im Repository und fÃ¼hrten zu veralteten Pfadangaben.  
**Optionen**:  

- A) Alles im Projekt-Root behalten  
- B) Skripte und Doku unter `operations/backup/` bÃ¼ndeln  
- C) Externes Repo nur fÃ¼r Betrieb anlegen  
**Entscheidung**: Option B â€“ alle aktiven Skripte/Dokumente liegen unter `operations/backup/`, Root-Dateien bleiben als Weiterleitung bzw. Legacy-Hinweis bestehen.

## ADR-005: compose.yaml Removal - Nur docker-compose.yml verwenden

**Datum**: 2025-10-30  
**Status**: âœ… Beschlossen  
**Kontext**: System hatte zwei konkurrierende Docker Compose Konfigurationen (`docker-compose.yml` + `compose.yaml`), die parallel liefen und zu Restart-Loops aller Python-Services fÃ¼hrten.

**Problem**:
- Docker Compose bevorzugt automatisch `compose.yaml` Ã¼ber `docker-compose.yml` (neuere Namenskonvention)
- Beide Container-Sets versuchten parallel zu laufen (Port-Konflikte 8001-8003)
- `compose.yaml` hatte fehlerhafte Network-Definition â†’ DNS-AuflÃ¶sung fehlgeschlagen
- 90 Minuten Downtime fÃ¼r Signal Engine, Risk Manager, Execution Service

**Optionen**:
- A) `compose.yaml` fixen und als primÃ¤re Config verwenden (kurze Namen: cdb-exec:v1)
- B) `docker-compose.yml` behalten, `compose.yaml` entfernen (lange Namen: claire_de_binare-*)
- `docker-compose.yml` war bereits funktionsfÃ¤hig und stabil (alle Services healthy)
- Kurze Container-Namen sind Nice-to-Have, aber System-StabilitÃ¤t ist kritischer
- Eine einzige Source of Truth verhindert zukÃ¼nftige Konflikte
**Implementation**:
```bash
docker rm -f cdb-exec cdb-risk cdb-signal  # StÃ¶rende Container entfernen
**Validation**:
- âœ… Alle Services healthy innerhalb 2 Minuten
- âœ… Health-Endpoints antworten korrekt
- Recovery Report: `backoffice/audits/2025-10-30_RECOVERY_REPORT.md`
- Funktionierende Config: `docker-compose.yml` (Root-Verzeichnis)  
**Konsequenzen**:
- â– Benutzer mÃ¼ssen neuen Pfad kennen (wird in Root-Docs kommuniziert)

## ADR-005: Unix-Timestamp fÃ¼r Datenbank-Zeitstempel
**Problem**: Code verwendete `datetime.utcnow()` (Python datetime-Objekt), DB-Schema erwartet `bigint` (Unix-Timestamp)  
**Optionen**:  

- A) DB-Schema Ã¤ndern zu `timestamp without time zone`  
- B) Code Ã¤ndern zu `int(time.time())` (Unix-Timestamp)  
- C) Beide Formate hybrid unterstÃ¼tzen

**Entscheidung**: Option B â€“ Code auf `int(time.time())` umgestellt  
**Rationale**:

- DB-Schema ist bewusst mit `bigint` designed (EVENT_SCHEMA.json Standard)
- Unix-Timestamps sind plattformÃ¼bergreifend eindeutig
- `save_order()`: `submitted_at` und `filled_at` auf `int(time.time())` umgestellt
- Bestehende `save_trade()` bereits korrekt (konvertiert ISO-String zu Unix)

- â• Konsistenz zwischen Events und DB
- â• E2E Test-Success-Rate: 90% â†’ 100%
- â– Keine (Code war fehlerhaft, DB-Schema korrekt)
**Status**: âœ… Beschlossen  
**Kontext**: Mehrere Komponenten (Apprise-Alerts, MCP-Dokument, Master-Ãœbersicht) sind durch neuere Strukturen ersetzt worden und fÃ¼hren zu Verwirrung/Duplikaten.  
**Optionen**:  
**Entscheidung**: Option B â€“ Komponenten werden in `archive/` verschoben mit README zur Dokumentation der GrÃ¼nde und Archivierungsdaten.  
**Konsequenzen**:

- â• Git-Historie bleibt erhalten, kein Datenverlust  
- â• Nachvollziehbare Projektentscheidungen  
- â• Sauberer Root-Ordner ohne veraltete Dateien  
- â– ZusÃ¤tzlicher Verwaltungsaufwand fÃ¼r Archiv-Dokumentation

## ADR-006: Governance-Ordner & Leitplanken

**Datum**: 2025-10-25  
**Status**: âœ… Beschlossen  
**Kontext**: Wiederkehrende Audit-Feststellungen (ENV-Duplikate, fehlende Logging-Standards) verlangten nach klaren Strukturen fÃ¼r Automatisierung, Templates und CI.  
**Optionen**:  

- A) Bestehende Dateien erweitern und verstreut ablegen  
- B) Neue Ordner unter `backoffice/` schaffen (`automation/`, `ci/`, `templates/`) und Regeln in separatem Dokument pflegen  
- C) Externes Wiki verwenden  
**Entscheidung**: Option B â€“ dedizierte Governance-Ordner plus `docs/ARCHITEKTUR_REGELN.md` als Verbindlichkeit fÃ¼r Services.  
**Konsequenzen**:  

- â• Klare Ablageorte fÃ¼r Skripte, Pipelines und Vorlagen  
- â• Architektur- und Logging-Regeln sind zentral versioniert  
- â– Initialer Pflegeaufwand (Templates/Skripte mÃ¼ssen gefÃ¼llt werden)  

## ADR-007: Automatisiertes Repository-Inventar

**Datum**: 2025-10-25  
**Status**: âœ… Beschlossen  
**Kontext**: KI-Agenten verlieren Zeit beim manuellen Erfassen des Dateibestands; Audits verlangen nachvollziehbare Snapshots pro Session.  
**Optionen**:  

- A) Rein manuelle SichtprÃ¼fung der Ordnerstruktur  
- B) Nutzung vorhandener Backup-Skripte fÃ¼r Inventarinformationen  
- C) EigenstÃ¤ndiges Repository-Inventar-Skript mit JSON-Ausgabe in `backoffice/logs/inventory/`  
**Entscheidung**: Option C â€“ dediziertes Skript `scripts/inventory.ps1`, das bei Session-Start ein Inventar schreibt und `latest.json` fÃ¼r schnelle Diffs bereitstellt.  
**Konsequenzen**:  

- â• Einheitliche Start-Routine fÃ¼r alle Agenten  
- â• Nachvollziehbarkeit von StrukturÃ¤nderungen Ã¼ber JSON-Historie  
- â– Leichter Pflegeaufwand fÃ¼r Skript bei StrukturÃ¤nderungen  

## ADR-008: Geheimnisrotation & Container-Hardening

**Datum**: 2025-10-25  
**Status**: âœ… Beschlossen  
**Kontext**: Audit 2025-10-25 identifizierte ungeschÃ¼tzte Redis-/Postgres-ZugÃ¤nge sowie Root-Container ohne Hardening. Risiko: Order-Manipulation, Datenverlust, Privilege Escalation.  
**Optionen**:  

- A) Nur Dokumentation ergÃ¤nzen und manuelle Erinnerung an Secret-Rotation  
- B) Compose/Dockerfiles hÃ¤rten, Secrets erzwingen, Host-Exponierung einschrÃ¤nken  
- C) Komplettumstieg auf Managed Services mit externem Secret-Store  
**Entscheidung**: Option B â€“ unmittelbare technische Absicherung durch Pflicht-ENV-Variablen, `--requirepass` fÃ¼r Redis, entfernte Passwort-Fallbacks, Non-Root-Execution-Service und Security-Optionen in Compose.  
**Konsequenzen**:  
- â• Reduzierte AngriffsflÃ¤che, Redis/Postgres nur mit gÃ¼ltigem Secret erreichbar  
- â• Container laufen ohne Root-Capabilities (`no-new-privileges`, `cap_drop`, Non-Root-User)  
- â– Betreiber mÃ¼ssen Secrets vor Deploy setzen; fehlende Variablen verhindern Start (Intentional Fail-Fast)  

## ADR-009: Execution-Feedback im Risk-Loop

**Datum**: 2025-10-25  
**Status**: âœ… Beschlossen  
**Kontext**: Der Execution-Service publiziert `order_result` Events, der Risk-Manager nutzte diese bislang nicht. Exposure-Limits und Circuit-Breaker reagierten daher nicht auf tatsÃ¤chliche AusfÃ¼hrungen, was auditrelevante LÃ¼cken lieÃŸ.  
**Optionen**:  

- A) Weiterhin nur Signal-Events berÃ¼cksichtigen und Exposure manuell resetten  
- B) Risk-Manager erweitert um Listener fÃ¼r `order_result`, Aktualisierung von Exposure/Pending Orders  
- C) Separaten Persistenz-Service vorsehen, der Limits periodisch neu berechnet  
**Entscheidung**: Option B â€“ direkter Listener im Risk-Manager synchronisiert Pending Orders, Positions-Exposure und Execution-Rejections in Echtzeit.  
**Konsequenzen**:  
- â• Exposure- und Circuit-Breaker-Logik basiert auf real ausgefÃ¼hrten Orders  
- â• Einheitliche Metriken (`order_results_received`, `orders_rejected_execution`) erlauben Monitoring  
- â– ZusÃ¤tzlicher Redis-Listener/Thread erhÃ¶ht KomplexitÃ¤t minimal  

## ADR-010: Docker Compose als Standard-Orchestrierung

**Datum**: 2025-10-25  
**Status**: âœ… BestÃ¤tigt  
**Kontext**: Diskussion, ob Docker Desktop ohne Compose-Befehle ausreicht. Die Plattform umfasst mehrere Container (Redis, Postgres, Prometheus, Grafana, Services) mit gemeinsamen Netzwerken/Volumes.  
**Optionen**:  

- A) Reine Docker-Desktop-GUI oder Einzel-`docker run` Kommandos  
- B) Docker Desktop inklusive CLI `docker compose` als verbindlicher Weg  
- C) Alternative Orchestrierung (k3s, Nomad)  
**Entscheidung**: Option B â€“ Docker Desktop bleibt Voraussetzung, Compose-CLI wird verbindlich fÃ¼r Mehrcontainer-Start/Stop/Tests verwendet.  
**Konsequenzen**:  
- â• Einheitliche Skripte und Doku bleiben gÃ¼ltig (`docker compose up â€¦`)  
- â• Health-/Security-Checks (Audit 2025-10-25) lassen sich automatisiert ausfÃ¼hren  
- â– Bedienung ohne CLI nicht unterstÃ¼tzt; reine GUI-Nutzung bleibt optional fÃ¼r Einzelcontainer  

## Template fÃ¼r neue ADRs

### ADR-XXX: [Titel]

**Datum**: YYYY-MM-DD  
**Status**: ğŸ”„ Vorgeschlagen / âœ… Beschlossen / âŒ Verworfen  
**Kontext**: Warum brauchen wir eine Entscheidung?  
**Optionen**: A, B, C...  
**Entscheidung**: Wir wÃ¤hlen X weil...  
**Konsequenzen**: Pro/Contra, Risiken

## ADR-011: Vereinheitlichung DB-Credentials und Prometheus-Healthcheck

**Datum**: 2025-10-26  
**Status**: âœ… Beschlossen  
**Kontext**: Postgres-Container startete mit bestehendem Datenverzeichnis; Credentials aus `.env` und realer DB-Instanz wichen ab. Zudem war der Prometheus-Healthcheck im Compose mit `curl` definiert, das im `prom/prometheus`-Image nicht verfÃ¼gbar ist.  
**Optionen**:


- A) Passwort des bestehenden DB-Benutzers im laufenden Container angleichen  
- B) Postgres-Volume neu initialisieren und User/Pass aus `.env` Ã¼bernehmen  
- C) Compose an `.env` koppeln (POSTGRES_USER variabel) und Prometheus-Healthcheck auf `wget` umstellen  

**Entscheidung**: Kombination aus B und C  


- Postgres-Volume zurÃ¼ckgesetzt und Neuinitialisierung mit `.env`-Werten vorgenommen (`POSTGRES_USER=admin`, `POSTGRES_PASSWORD=â€¦`).  
- `docker-compose.yml`: `POSTGRES_USER` an `.env` gekoppelt; Prometheus-Healthcheck auf `wget` umgestellt.  

**Konsequenzen**:  

- â• Eindeutige, zentrale Steuerung der DB-Credentials Ã¼ber `.env`  
- â• Prometheus wird korrekt als â€healthy" erkannt  
- â– Daten im alten Postgres-Volume wurden verworfen (bewusst, MVP-Phase)  

---

## ADR-012: bot_rest ohne Healthcheck betreiben

**Datum**: 2025-10-26  
**Status**: âœ… Beschlossen  
**Kontext**: `bot_rest` Container wurde als "unhealthy" gemeldet, obwohl er korrekt funktioniert. Service lÃ¤uft in Periodik-Loop (alle 300s) ohne HTTP-Server, der Healthcheck via curl schlug daher immer fehl.  
**Optionen**:  

- A) HTTP-Server in bot_rest einbauen nur fÃ¼r /health Endpoint  
- B) Healthcheck entfernen und Status via docker logs Ã¼berwachen  
- C) Healthcheck auf Script-Check umstellen (ps, pidof)  

**Entscheidung**: Option B â€“ Healthcheck aus `docker-compose.yml` entfernt mit Kommentar "No healthcheck - service runs in periodic loop without HTTP server"  
**Konsequenzen**:  

- â• Container-Status zeigt "running" statt "unhealthy"  
- â• Keine unnÃ¶tige KomplexitÃ¤t durch HTTP-Server nur fÃ¼r Health-Check  
- â• Service-Funktion bestÃ¤tigt durch docker logs (regelmÃ¤ÃŸige Outputs)  
- â– Kein automatisches Health-Signal fÃ¼r Monitoring; manuelles Log-Monitoring erforderlich  

---

## ADR-013: MCP-Server Integration fÃ¼r erweiterte Development-Tools

**Datum**: 2025-10-26  
**Status**: âœ… Beschlossen  
**Kontext**: GitHub Copilot bietet Ã¼ber Model Context Protocol (MCP) spezialisierte Tool-Server fÃ¼r Docker-Management, Python-Analyse, Dokumentation und Diagramme. Integration erweitert Development-Workflow mit semantischen Abfragen, automatischem Refactoring und visueller Dokumentation.  
**Optionen**:  

- A) Nur Standard VS Code Extensions nutzen (ohne MCP)  
- B) AusgewÃ¤hlte MCP-Server konfigurieren (Docker, Pylance, Context7, Mermaid)  
- C) Alle verfÃ¼gbaren MCP-Server installieren (inkl. experimentelle)  

**Entscheidung**: Option B â€“ 4 MCP-Server strategisch ausgewÃ¤hlt und konfiguriert:

1. **Docker MCP**: Knowledge Graph fÃ¼r Container-Infrastruktur (9 Container, 4 Volumes, 24 Relations)
2. **Pylance MCP**: Python Code-Analyse, Refactoring, Snippet-Execution
3. **Context7**: Library-Dokumentation (fastapi, redis, psycopg2, pydantic)
4. **Mermaid Chart**: Diagramm-Erstellung und Validierung (Flowcharts, Sequence, ER)

**Implementierung**:  

- Zentrale Konfiguration: `backoffice/mcp_config.json`
- Dokumentation: `docs/MCP_SETUP_GUIDE.md` (420+ Zeilen)
- Docker Knowledge Graph initialisiert mit allen System-Entities und Relations
- Chatmodes erweitert: `.github/chatmodes/*` integrieren MCP-Tools

**Konsequenzen**:  

- â• Semantische Suche Ã¼ber Container-Topologie (mcp_mcp_docker_search_nodes)
- â• Automatisches Refactoring (Unused Imports, Format Conversion)
- â• Code-Snippets direkt im Workspace-Environment testbar (ohne Terminal-Escaping)
- â• Aktuelle Library-Docs on-demand (pypi, npm, GitHub)
- â• Diagramm-Validierung vor Commit (Syntax-Checks, Live-Preview)
- â• Dokumentation der Service-Beziehungen im Knowledge Graph persistiert
- â– MCP-Server sind nicht persistent (Docker Graph muss nach Restart neu befÃ¼llt werden)
- â– Context7 erfordert Internet-Verbindung fÃ¼r Doc-Abruf
- ğŸ”„ Wartung: QuartalsmÃ¤ÃŸige Review der MCP-Konfiguration (nÃ¤chster Termin: 2025-11-26)

**Metriken**:  

- Docker-Entities: 14 (9 Container, 1 Network, 4 Volumes)
- Docker-Relations: 24 (Pub/Sub, Network, Volume-Mounts, Metrics)
- Python-Services: 3 (signal_engine, risk_manager, execution_service)
- Dokumentierte Libraries: 6 (fastapi, redis-py, psycopg2-binary, prometheus-client, pydantic, httpx)

---

## ADR-014: Docker MCP Toolkit Integration fÃ¼r Gordon AI-Agent

**Datum**: 2025-10-26  
**Status**: âœ… Beschlossen  
**Kontext**: WÃ¤hrend der MCP-Server-Integration (ADR-013) wurde festgestellt, dass das offizielle **Docker MCP Toolkit** (Beta-Feature in Docker Desktop) eine dedizierte LÃ¶sung fÃ¼r AI-Agenten wie Gordon bietet. Das Toolkit ermÃ¶glicht Cross-LLM-KompatibilitÃ¤t, Zero-Setup-Orchestration und sichere Tool-Verwaltung via MCP Gateway.

**Problem**: Bestehende VS Code MCP-Server (ADR-013) sind ausschlieÃŸlich fÃ¼r VS Code Copilot optimiert. FÃ¼r Container-Management und Live-Operations benÃ¶tigen wir einen AI-Agenten mit direktem Docker-CLI-Zugriff und Dateioperationen.

**Optionen**:  

- A) Nur VS Code MCP-Server nutzen und Terminal-Befehle manuell ausfÃ¼hren  
- B) Docker MCP Toolkit aktivieren und Gordon als separaten AI-Agenten fÃ¼r DevOps-Tasks nutzen  
- C) Eigenen MCP-Server fÃ¼r Claire de Binare entwickeln und im Docker Catalog verÃ¶ffentlichen  

**Entscheidung**: Kombination aus B und C (langfristig)  

**Phase 1 (sofort)**:  

- Docker MCP Toolkit Beta-Feature in Docker Desktop aktivieren
- Gordon via MCP Gateway fÃ¼r Container-Management, Health-Checks und Log-Analyse nutzen
- Workflow definiert: VS Code Copilot (Code/Architektur) + Gordon (Operations/Debugging)

**Phase 2 (Q4 2025)**:  

- Custom MCP-Server fÃ¼r Claire de Binare entwickeln (`claire-de-binare-mcp`)
- Tools: `get_latest_trades`, `get_signal_count`, `check_risk_limits`, `analyze_performance`
- VerÃ¶ffentlichung im Docker MCP Catalog (optional)

**Implementierung (Phase 1)**:  

- Dokumentation: `docs/DOCKER_MCP_TOOLKIT_SETUP.md` (500+ Zeilen)
- Bereiche: Toolkit-Aktivierung, Gordon-Setup, Security (OAuth, Secrets), Custom Server Template
- Gordon Test-Prompts fÃ¼r Claire de Binare erstellt (Container-Status, Health-Checks, Log-Analyse)
- MCP Gateway Security dokumentiert (Resource Limits, Image Signing, Request Interception)

**Docker MCP Toolkit Features**:  

- âœ… Cross-LLM KompatibilitÃ¤t (Gordon, Claude Desktop, Cursor)
- âœ… Zero Manual Setup (keine Dependency-Verwaltung, Auto-Discovery via Docker Catalog)
- âœ… Security: Passive (Image Signing, SBOM) + Active (Resource Limits, Request Interception)
- âœ… PortabilitÃ¤t: Tools funktionieren plattformÃ¼bergreifend ohne Code-Ã„nderungen
- âœ… MCP Gateway: Sichere Orchestration zwischen AI-Clients und MCP-Servern

**Gordon Use Cases fÃ¼r Claire de Binare**:  

1. **Container-Management**: `docker ps`, `docker logs`, `docker restart` via natÃ¼rliche Sprache
2. **Database-Queries**: PostgreSQL-Abfragen via MCP Tools (nach Custom Server-Implementierung)
3. **Health-Monitoring**: Automatische PrÃ¼fung aller Health-Endpoints (8001, 8002, 8003)
4. **Log-Analyse**: Fehlersuche in Echtzeit-Logs mit semantischer Filterung
5. **OAuth-Integration**: GitHub-API-Zugriff fÃ¼r PR-Management, Issue-Tracking

**Workflow-Abgrenzung (Copilot vs. Gordon)**:  

| Aufgabe | VS Code Copilot | Gordon (Docker MCP) |
|---------|-----------------|---------------------|
| Code-Analyse & Review | âœ… PrimÃ¤r | â– |
| Architektur-Entscheidungen | âœ… PrimÃ¤r | â– |
| Docker Container-Management | â– | âœ… PrimÃ¤r |
| Datei-Bulk-Operationen | â– | âœ… PrimÃ¤r |
| Dokumentations-Erstellung | âœ… PrimÃ¤r | â– |
| Live-Debugging (Logs, Metrics) | â– | âœ… PrimÃ¤r |
| Database-Queries | â– | âœ… PrimÃ¤r (Phase 2) |

**Konsequenzen**:  

- â• Separation of Concerns: Code-Tasks (Copilot) vs. Operations-Tasks (Gordon)
- â• Gordon kann Docker-CLI ohne PowerShell-Escaping-Probleme nutzen
- â• HÃ¶here Datei-Operationslimits (Gordon: 1000 Zeilen read, 50 write vs. Copilot-Tools)
- â• MCP Gateway enforcement von Security-Policies (Resource Limits, OAuth-Token-Rotation)
- â• Custom MCP-Server ermÃ¶glicht trading-spezifische Tools (Trade-Queries, Risk-Metrics)
- â– Gordon erfordert Docker Desktop Beta-Features (experimentell, potenzielle Breaking Changes)
- â– ZusÃ¤tzlicher Kontext-Switch zwischen VS Code (Copilot) und Docker Desktop (Gordon)
- â– MCP-Server im Docker Catalog sind public (Custom Server nur lokal oder nach Review verÃ¶ffentlichbar)
- ğŸ”„ Wartung: Custom MCP-Server (Phase 2) erfordert Dockerfile, server.yaml und tools.json Pflege

**Security-MaÃŸnahmen**:  

1. **Secrets Management**: `docker mcp secret set` fÃ¼r DB-Credentials, API-Keys
2. **Resource Limits**: Memory 512M, CPU 0.5, Network restricted
3. **OAuth-Flow**: GitHub OAuth via `docker mcp oauth authorize github`
4. **Image Signing**: Docker-built images im `mcp/` namespace mit kryptographischen Signaturen
5. **Request Interception**: MCP Gateway Ã¼berwacht alle Tool-Calls auf Policy-Verletzungen

**Metriken**:  

- Dokumentation: 500+ Zeilen (DOCKER_MCP_TOOLKIT_SETUP.md)
- MCP-Server-Typen: 2 (VS Code: 4 Server, Docker Desktop: Gordon + Custom Server in Phase 2)
- Gordon-Prompts: 6 (Status-Check, Container-Neustart, Database-Check, Rebuild, Health-Check, Log-Analyse)
- Custom Server Templates: 3 (Dockerfile, server.yaml, main.py)

**NÃ¤chste Schritte (Phase 2 - Q4 2025)**:  

1. Custom MCP-Server `claire-de-binare-mcp` entwickeln (Python, FastAPI-basiert)
2. Tools implementieren: `get_latest_trades`, `get_signal_count`, `check_risk_limits`, `analyze_performance`
3. Docker Catalog Submission vorbereiten (optional, nach intern. Testing)
4. Gordon-Integration in CI/CD-Pipeline (automatische Health-Checks pre-deployment)

---

## ADR-015: Sofortige Handlungsdokumentation im Copilot-Workflow

**Datum**: 2025-10-27  
**Status**: âœ… Beschlossen  
**Kontext**: WÃ¤hrend der laufenden Paper-Trading-Testphase sind prÃ¤zise und zeitnahe Protokolle jedes KI-Schritts erforderlich. Bisher wurden Aktionen hÃ¤ufig erst am Sessionende gesammelt festgehalten, was das Nachvollziehen einzelner Eingriffe erschwerte.

**Optionen**:  

- A) Bisherige Sammeldokumentation am Sessionende beibehalten  
- B) Manuelle Protokollierung nach eigenem Ermessen  
- C) Verpflichtende Dokumentation nach jeder abgeschlossenen Handlung in Session-Memo oder DECISION_LOG

**Entscheidung**: Option C â€“ Jede Aktion wird unmittelbar nach Abschluss dokumentiert. FÃ¼r kleinere operative Schritte genÃ¼gt ein Eintrag im laufenden Session-Memo; strukturrelevante Anpassungen werden zusÃ¤tzlich im DECISION_LOG festgehalten.

**Konsequenzen**:  

- â• LÃ¼ckenlose RÃ¼ckverfolgbarkeit einzelner KI-Handlungen  
- â• Schnellere Auditierbarkeit wÃ¤hrend des 7-Tage-Tests  
- â• Klarer Hand-off zwischen Copilot und Gordon dank identischer Protokollierungspflicht  
- â– GeringfÃ¼giger Mehraufwand pro Schritt (sofortige Notizen erforderlich)

**Umsetzung**: Copilot-Instruktionen aktualisiert (`.github/copilot-instructions.md`), inklusive Autonomie-Hinweis fÃ¼r Terminalaufgaben und Pflicht zur direkten Dokumentation.

**Follow-up 2025-10-27**: Build-Kontexte in `compose.yaml` auf `backoffice/services/...` angepasst, damit `docker compose` die Service-Verzeichnisse findet; Docker-Start bleibt blockiert, solange das `risk_manager` Service-Verzeichnis fehlt.

**Follow-up 2025-10-27 (Bereinigung)**: `compose.yaml` enthielt doppelte Service-Definitionen zu `docker-compose.yml`. Da `docker-compose.yml` bereits vollstÃ¤ndig konfiguriert ist (9 Container inkl. Redis, Postgres, Monitoring) und stabil lÃ¤uft, wurde `compose.yaml` entfernt aus dem aktiven Setup. Die fehlgeschlagenen Container-Instanzen (Signal, Risk, Execution aus `compose.yaml`) wurden gestoppt; nur die Haupt-Services aus `docker-compose.yml` bleiben aktiv. Haupt-Compose ist vollstÃ¤ndige Infrastruktur inkl. Redis/Postgres, wÃ¤hrend `compose.yaml` isolierte Service-Tests ohne AbhÃ¤ngigkeiten war â€“ Entscheidung: Haupt-Compose als einzige produktive Konfiguration nutzen. Postgres-Container war gestoppt; nach Neustart ist Execution-Service nun stabil (10/10 Container healthy).

---

## ADR-016: Tool Layer Registry fÃ¼r zentrale Tool-Verwaltung

**Datum**: 2025-10-27  
**Status**: âœ… Beschlossen  
**Kontext**: Mit wachsender MCP-Server-Integration, DevOps-Tools und ML-Komponenten fehlte eine zentrale Ãœbersicht aller verfÃ¼gbaren Tools. Entscheidungen Ã¼ber neue Integrationen wurden ad-hoc getroffen, ohne strukturierte Kategorisierung oder Status-Tracking.

**Optionen**:

- A) Tools weiterhin dezentral in einzelnen Dokumenten pflegen
- B) Zentrale Tool Registry mit Kategorisierung (GO TO USE / NICE TO HAVE)
- C) Externe Plattform (Notion, Confluence) fÃ¼r Tool-Management

**Entscheidung**: Option B â€“ Zentrale Tool Registry in `docs/TOOL_LAYER.md` mit klarer Kategorisierung und Statusverfolgung.

**Struktur**:

- **GO TO USE**: Produktiv eingebundene Tools (11 MCP-Server, 10 Docker-Container, 4 Monitoring-Tools)
- **NICE TO HAVE**: Geplante Erweiterungen (NotebookLM, Vault, Autogen Studio)
- Status-Kennzeichnung: âœ… aktiv, ğŸŸ¢ bereit, ğŸ§ª experimentell, ğŸ”œ geplant

**Kategorien**:

1. Core Integrationen / MCP-Server (6): github-mcp, postman-mcp, mcp-grafana, mcp-redis, mongodb-mcp, hub-mcp
2. DevOps & Automation (4): n8n, self-hosted-ai-starter-kit, git-credential-manager, mcp-registry
3. Monitoring & Observability (5): Prometheus, Grafana, Loki, Pyroscope, Sift
4. Core Daten & Persistenz (5): PostgreSQL, Redis, SQLite, MongoDB Atlas, Qdrant
5. Forschung & ML-Advisor (5): TensorFlow, XGBoost, SHAP, W&B, Neptune.ai
6. Wissens- & Doku-Assistenz (3): NotebookLM, Notion API, Obsidian
7. Design & PrÃ¤sentation (2): Figma/Canva SDK, Plotly/Matplotlib

---

## ADR-017: Query Service fÃ¼r READ-ONLY Data Access Layer

**Datum**: 2025-10-30  
**Status**: âœ… Beschlossen  
**Kontext**: MCP-Server und externe Tools benÃ¶tigen strukturierten, READ-ONLY Zugriff auf Postgres-Tabellen (signals, risk_positions) und Redis-Streams (event streams). Bisherige Ad-Hoc-Queries erschwerten Wartbarkeit und fehlende Type-Safety fÃ¼hrte zu inkonsistenten Datenformaten.

**Problem**: Fragmentierter Datenzugriff Ã¼ber verschiedene Services und Tools ohne zentrale Schnittstelle. Gordon AI-Agent und Monitoring-Dashboards benÃ¶tigen deterministische, einheitliche JSON-Responses.

**Optionen**:

- A) Direkter Postgres/Redis-Zugriff aus jedem Tool (Status Quo)
- B) REST API mit FastAPI entwickeln (zusÃ¤tzlicher HTTP-Server)
- C) Python Query Service Library mit CLI und programmatischer API

**Entscheidung**: Option C â€“ Lightweight Python Query Service als Library mit CLI-Interface

**Implementierung**:

- Location: `backoffice/services/query_service/`
- Komponenten:
  - `service.py`: Hauptklasse mit async Postgres/Redis queries
  - `config.py`: Environment-basierte Konfiguration
  - `models.py`: Type-safe Dataclasses (SignalRecord, RiskRecord, RedisEvent)
  - `cli.py`: Command-line Interface fÃ¼r interaktive Nutzung
  - `examples.py`: VollstÃ¤ndige Beispiele fÃ¼r alle Queries
  - `API_SPEC.json`: Formale Spezifikation gemÃ¤ÃŸ User-Request
- Dependencies: `asyncpg>=0.29.0`, `redis>=5.0.0`

**VerfÃ¼gbare Queries**:

1. **signals_recent** (Postgres): Letzte N Signals fÃ¼r Symbol (BTCUSDT default)
   - Filter: symbol, since_ms, limit (max 1000)
   - Output: timestamp, symbol, side, price, confidence, reason, volume, pct_change

2. **risk_overlimit** (Postgres): Risk-Positionen Ã¼ber Limit
   - Filter: symbol (optional), only_exceeded, limit
   - Output: timestamp, symbol, exposure, limit

3. **redis_tail** (Redis): Letzte N Events aus Stream
   - Filter: channel (signals:BTCUSDT default), count
   - Output: event_id, timestamp, payload

**Output-Format (einheitlich)**:

```json
{
  "result": [/* records */],
  "count": 123,
  "query": "signals_recent",
  "timestamp_utc": "2025-10-30T10:45:00.123456+00:00"
}
```

**Constraints**:

- âœ… READ_ONLY (keine INSERT/UPDATE/DELETE)
- âœ… Deterministische Sortierung (timestamp DESC)
- âœ… Connection Pooling (Postgres: 1-5 Connections)
- âœ… Timeouts (Postgres: 30s, Redis: 5s)
- âœ… Limit Enforcement (max 1000 pro Query)

**Konsequenzen**:

- â• Zentrale, wartbare Datenzugriffsschicht
- â• Type-Safety durch Pydantic-Dataclasses
- â• CLI fÃ¼r manuelle Exploration und Debugging
- â• Gordon AI-Agent kann strukturierte Queries ohne SQL-Injection-Risiko ausfÃ¼hren
- â• Einheitliches JSON-Format fÃ¼r alle Monitoring-Tools
- â• Async-First Design (skalierbar fÃ¼r parallel queries)
- â– ZusÃ¤tzliche Dependency-Layer (asyncpg, redis-py)
- â– Kein HTTP-Endpoint (nur Library-Import oder CLI)
- ğŸ”„ Future: REST API Wrapper fÃ¼r externe Tools (FastAPI optional in Phase 2)

**Integration**:

- **Gordon AI-Agent**: Via CLI oder direkter Python-Import fÃ¼r Container-Diagnostik
- **Monitoring-Dashboards**: Grafana kann CLI-Output als JSON Data Source nutzen
- **MCP-Server (Phase 2)**: Custom Claire de Binare MCP-Server nutzt Query Service intern
- **Jupyter Notebooks**: Direkter Import fÃ¼r Backtesting und Analyse

**Sicherheit**:

- âœ… Postgres-User hat nur SELECT-Rechte (Role-based in Phase 7)
- âœ… Redis-Client nutzt READ-ONLY Kommandos (XREVRANGE, keine DEL/EXPIRE)
- âœ… Connection-Strings niemals in Logs (nur ENV-Variablen)
- âœ… SQL-Injection-sicher (asyncpg Prepared Statements)

**Wartung**:

- Monatliches Review: Query-Performance-Metriken (Query-Dauer, Ergebnis-Counts)
- Quartalsweise: Schema-Alignment-Check gegen `DATABASE_SCHEMA.sql`
- Bei Ã„nderungen in `EVENT_SCHEMA.json`: models.py synchronisieren

**Metriken**:

- Code: 700+ Zeilen (Python)
- Dokumentation: 300+ Zeilen (README.md)
- Tests: 7 Test-Cases (pytest)
- API-Spec: VollstÃ¤ndig JSON-dokumentiert (API_SPEC.json)

**NÃ¤chste Schritte**:

1. Dependencies installieren: `pip install -r backoffice/services/query_service/requirements.txt`
2. CLI-Test: `python -m backoffice.services.query_service.cli --query signals_recent --symbol BTCUSDT`
3. Integration-Tests: `pytest backoffice/services/query_service/test_service.py -v`
4. Gordon-Prompts erweitern: "Zeige die letzten 50 Signals fÃ¼r BTCUSDT"
8. Security & Governance (3): HashiCorp Vault, Trivy/Grype, OPA
9. KI-Orchestrierung & Agent Frameworks (2): LangSmith/LangFuse, Autogen Studio

**Konsequenzen**:

- â• Zentrale Ãœbersicht aller verfÃ¼gbaren Tools fÃ¼r AI-Agenten (Copilot, Gordon)
- â• Strukturierter Entscheidungsprozess fÃ¼r neue Tool-Integrationen
- â• Klare Statusverfolgung (aktiv, bereit, experimentell, geplant)
- â• Automatische Referenz in AI-Prompts ("Nutze mcp-redis fÃ¼r Pub/Sub-Analyse")
- â• Wartungs-Strategie definiert (wÃ¶chentlich, monatlich, quartalsweise Reviews)
- â– ZusÃ¤tzlicher Pflegeaufwand bei Tool-Updates (Status-Ã„nderungen dokumentieren)

**Integration**:

- Verweis in `ARCHITEKTUR.md` (neuer Abschnitt "Tool Layer Integration")
- VerknÃ¼pfung mit `MCP_DOCUMENTATION_INDEX.md` (technische Details)
- Update `PROJECT_STATUS.md` (Metriken: 11 MCP-Server, 30+ Tools dokumentiert)

**Metriken**:

- GO TO USE Tools: 30 (davon 11 MCP-Server, 10 Docker-Container)
- NICE TO HAVE Tools: 12 (geplante Erweiterungen)
- Dokumentierte Kategorien: 9
- Gesamtumfang: 280+ Zeilen Dokumentation

**Integration abgeschlossen (2025-10-27)**:

- âœ… `ARCHITEKTUR.md` erweitert (Abschnitt "Tool Layer Integration")
- âœ… `PROJECT_STATUS.md` aktualisiert (Phase 6.3, 10/10 Container healthy)
- âœ… `MCP_DOCUMENTATION_INDEX.md` verlinkt auf TOOL_LAYER.md
- âœ… Container-Status validiert: 10/10 healthy (inkl. Execution-Service nach Postgres-Fix)

---

## ADR-017: Gordon-Konsultation vor Docker-Eingriffen

**Datum**: 2025-10-27  
**Status**: âœ… Beschlossen

**Kontext**: Wiederholte Container-Restarts und unvollstÃ¤ndige Infrastruktur-Kontexte haben gezeigt, dass spontane Docker-Eingriffe ohne Gordon-Abstimmung zu InstabilitÃ¤t fÃ¼hren. Gordon fungiert als zentrale Kontrollinstanz fÃ¼r Infrastruktur-Ã„nderungen Ã¼ber das MCP-Toolkit.

**Optionen**:

- A) Copilot fÃ¼hrt Docker-Operationen eigenstÃ¤ndig durch
- B) Vor jedem docker compose / docker CLI Eingriff Gordon Ã¼ber MCP konsultieren und Freigabe dokumentieren
- C) Alle Docker-Aktionen vollstÃ¤ndig an Gordon delegieren

**Entscheidung**: Option B â€“ Copilot holt vor jedem Docker-Befehl (compose up/down, build, prune, rm, volume/network-Ã„nderungen) eine Gordon-Freigabe ein. Ohne dokumentierte Freigabe dÃ¼rfen keine Container-, Netzwerk- oder Volume-Operationen erfolgen.

**Konsequenzen**:

- â• Verhindert inkonsistente Compose-Starts bei unvollstÃ¤ndiger Umfeld-Konfiguration
- â• Einheitlicher Freigabeprozess via MCP, nachvollziehbar im Session-Memo
- â• Gordon behÃ¤lt GesamtÃ¼berblick Ã¼ber Infrastrukturzustand und Ressourcenplanung
- â– ZusÃ¤tzlicher Kommunikationsschritt vor operativen Docker-Befehlen

**Umsetzung**:

- Copilot dokumentiert jede Gordon-Anfrage im laufenden Session-Memo (Zeitstempel, angefragte Aktion, Ergebnis)
- Docker-Runbooks im `docs/ops/RUNBOOK_DOCKER_OPERATIONS.md` und in der `EXECUTION_DEBUG_CHECKLIST.md` verweisen auf verpflichtende Gordon-Freigabe
- Automatische Checks: Vor Docker-Kommandos wird geprÃ¼ft, ob aktuelle Gordon-Freigabe vorliegt (Session-Notiz oder Ticket)

---

## ADR-018: README Guide & Dashboard-V5-Standardisierung

**Datum**: 2025-11-01  
**Status**: âœ… Beschlossen  
**Kontext**: README-Dateien waren inkonsistent strukturiert, enthielten veraltete Ports/Topics
und widersprÃ¼chliche ENV-Hinweise. Audit-Anforderungen forderten einen einheitlichen
Dashboard-V5-Auftritt.

**Optionen**:

- A) Nur Root-README anpassen, restliche Dateien schrittweise bei Bedarf
- B) Verbindlichen Leitfaden `README_GUIDE.md` erstellen und alle Readmes daran ausrichten
- C) Readmes durch externes Wiki ersetzen

**Entscheidung**: Option B â€“ `README_GUIDE.md` definiert verpflichtend Aufbau, Tabellenlayout,
Visuelle Elemente (Dashboard-V5-Stil) und Referenzlinks. Alle bestehenden Readmes wurden
entsprechend migriert.

**Konsequenzen**:

- â• Einheitliche Darstellung fÃ¼r alle Services, Module und Ordner
- â• Zentrale Quelle fÃ¼r Ports, Topics, ENV, Metriken hÃ¤lt Docs synchron mit Architektur
- â• Vereinfachte Reviews dank klarer StrukturblÃ¶cke (Ãœberblick, Architektur, Setup, Monitoring)
- â– Initialer Migrationsaufwand fÃ¼r Bestandsdateien

**Validierung**:

- `README_GUIDE.md` im Repo-Root eingefÃ¼hrt (Dashboard-V5-Vorgaben)
- Alle aktualisierten Readmes verlinken konsistent auf `ARCHITEKTUR.md`,
  `Service-Kommunikation & DatenflÃ¼sse.md`, `Risikomanagement-Logik.md`
- `.env` und Ports/Topics-Tabellen in den Readmes decken sich mit Compose & Event-Schema

---

## ADR-019: Wissensgraph Phase 2 â€“ smarter_assistant Integration

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: Mit `knowledge_inventory.json`, `semantic_map.md`, `refactor_plan.md`, `consistency_audit.md` und `learning_path.md` existieren neue Wissensartefakte, die bislang als isolierte Dokumente gefÃ¼hrt wurden. FÃ¼r Phase 3 (2-Hop-Konsolidierung) ist ein konsistenter Wissensgraph erforderlich.

**Optionen**:

- A) Weiterhin rein textuelle Dokumente verwenden und AbhÃ¤ngigkeiten ad hoc verfolgen
- B) Smarter-Assistent-Artefakte in bestehenden Listen verlinken, aber ohne Graph-Struktur
- C) Einen formalen Wissensgraph etablieren (PrimÃ¤rdokument `semantic_map.md`, menschliche Navigationsschicht `Knowledge_Map.md`, Maschinen-Layer `semantic_index.json`)

**Entscheidung**: Option C â€“ VollstÃ¤ndige Integration aller smarter_assistant-Artefakte in einen Knowledge Graph mit maschinenlesbarem Index und menschlichem Navigationslayer.

**Konsequenzen**:

- â• Phase-3-Konsolidierung kann gezielt 1-Hop- und 2-Hop-AbhÃ¤ngigkeiten analysieren
- â• Neue Artefakte erhalten PrimÃ¤rstatus und verlieren ihren Inselcharakter
- â• Automatisierungen kÃ¶nnen Ã¼ber `semantic_index.json` Beziehungen programmatisch auswerten
- â• `knowledge_inventory.json` bleibt Datenquelle, aber Graph regelt Priorisierung
- â– Laufender Pflegeaufwand: Jede Relation muss im Index und in der Knowledge Map nachgetragen werden

**Umsetzung**:

- `semantic_map.md` als PrimÃ¤rdokument markiert und um Graphstatus erweitert
- `docs/smarter_assistant/Knowledge_Map.md` erstellt (Navigation, 1/2-Hop-Ketten)
- `docs/smarter_assistant/semantic_index.json` erzeugt (Knoten, Kanten, Cluster)
- `PROJECT_STATUS.md` unter "Technische Verbesserungen" mit Phase-2-Vermerk ergÃ¤nzt

**AbhÃ¤ngigkeiten**:

- Phase 3 stÃ¼tzt sich auf diese Artefakte, um Redundanzen (ENV, Ports, Topics) zu beseitigen
- Phase 4 setzt Wissensanker erst nach Abschluss der Phase-3-MaÃŸnahmen

---

## ADR-020: Phase-3-Normalisierung â€“ Konfliktregister

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: Phase 3 soll 1-/2-Hop-Konflikte (Ports, Secrets, Event-Literals) konsistent beheben. Bisherige Artefakte (PROJECT_STATUS, Service-Dokumente, Schema) fÃ¼hrten zu widersprÃ¼chlichen Angaben, was Phase-4-Wissensanker blockiert.

**Optionen**:

- A) Konflikte jeweils direkt in den betroffenen Dokumenten notieren (Projektstatus, Service-Doku, Schema)
- B) Session-Memos erweitern und Konflikte temporÃ¤r dokumentieren
- C) Zentrales Normalisierungs-Register erstellen (`Normalization_Report.md`) und maschinenlesbare Referenzen im `semantic_index.json` pflegen

**Entscheidung**: Option C â€“ eigenes Konfliktregister mit MaÃŸnahmenliste und Graph-Verankerung, damit Phase-3-Konsolidierung nachvollziehbar und auditierbar bleibt.

**Konsequenzen**:

- â• Port- und Secret-Divergenzen werden in einem Dokument zentral verfolgt
- â• Schema-Abweichungen zwischen Beispieldokumentation und `EVENT_SCHEMA.json` sind eindeutig adressiert
- â• `semantic_index.json` bildet Konfliktkanten (`conflicts_with`, `tracks_issue`) fÃ¼r Automatisierungen ab
- â• Governance-Dokumente (`PROJECT_STATUS.md`, `DECISION_LOG.md`) verweisen auf die Normalisierung als laufende AktivitÃ¤t
- â– ZusÃ¤tzlicher Pflegeaufwand, bis alle Konflikte behoben sind und auf `verified=true` gesetzt werden kÃ¶nnen

**Validierung**:

- `docs/smarter_assistant/Normalization_Report.md` erstellt (Port-, Secret-, Event-/Alert-Deltas dokumentiert)
- `semantic_index.json` um Knoten `normalization_report`, `env_file`, `service_dataflow_doc`, `risk_logic_doc` und Konfliktkanten erweitert
- `Knowledge_Map.md`, `semantic_map.md`, `PROJECT_STATUS.md` auf Phase-3-Status und NormalisierungseintrÃ¤ge aktualisiert

**AbhÃ¤ngigkeiten**:

- Umsetzung der MaÃŸnahmen aus dem Normalization Report ist Voraussetzung fÃ¼r Phase-4-Wissensanker
- Ã„nderungen an Ports/Secrets/Schemas mÃ¼ssen nach Umsetzung in allen PrimÃ¤rquellen synchronisiert werden

---

## ADR-022: REST-Port-Governance-Normalisierung

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: Runtime (`docker-compose.yml`, `.env`, Container-Status) exponiert den REST-Screener auf Host-Port 8080, wÃ¤hrend Governance-Artefakte (`PROJECT_STATUS.md`, `Normalization_Report.md`, Session-Memos) noch 8010 fÃ¼hrten und Health-/Runbook-Checks fehlleiteten.

**Optionen**:

- A) Dokumentation unverÃ¤ndert lassen und auf Runtime als maÃŸgebliche Quelle verweisen
- B) Host-Port 8080 in allen Governance-Dokumenten vereinheitlichen und Konflikt im Wissensgraphen als verifiziert markieren
- C) REST-Service auf 8010 zurÃ¼cksetzen, um Dokumentation anzupassen

**Entscheidung**: Option B â€“ Governance-Artefakte und Session-Memo auf 8080 angleichen und Relation `project_status â†’ docker_compose` im Wissensgraphen als `verified=true` mit `normalized_value: "8080"` kennzeichnen.

**Konsequenzen**:

- â• Health-Checks, Runbooks und Monitoring-Dokumente referenzieren denselben Port (8080)
- â• Wissensgraph spiegelt die Normalisierung Ã¼ber Metadaten (`verified`, `normalized_value`) wider
- â• Phase-4-Port-Loop abgeschlossen, Session-Memo dokumentiert den Abschluss
- â– Laufende Normalisierungsschleifen benÃ¶tigen konsistente Pflege der Wissensgraph-Metadaten

---

## ADR-023: Redis Secret Alignment

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: Die Runtime verwendet `REDIS_PASSWORD=REDACTED_REDIS_PW` ( `.env`, `docker-compose.yml`, Container-Startup). Governance-Dokumente referenzierten weiterhin `REDACTED_REDIS_PW$$`, wodurch Secretsync und Runbooks divergierten.

**Optionen**:

- A) Runtime-Secret auf `REDACTED_REDIS_PW$$` zurÃ¼ckdrehen und Container neu provisionieren
- B) `.env`, `PROJECT_STATUS.md`, `Risikomanagement-Logik.md` und Wissensgraph auf den Runtime-Wert **REDACTED_REDIS_PW** harmonisieren
- C) Redis ohne Passwort betreiben und Auth nur in Dokumentation erwÃ¤hnen

**Entscheidung**: Option B â€“ Runtime gilt als autoritative Quelle. Alle Governance-Artefakte werden auf `REDIS_PASSWORD=REDACTED_REDIS_PW` aktualisiert, `semantic_index.json` dokumentiert den verifizierten Wert (`normalized_value: "${REDIS_PASSWORD}"`).

**Konsequenzen**:

- â• Secrets in Runtime, Dokumentation und Graph identisch; Runbooks funktionieren ohne Korrekturen
- â• Risk Manager Security-Abschnitt verweist auf ENV-Ladung gemÃ¤ÃŸ `.env`
- â• ADR-Referenz fÃ¼r zukÃ¼nftige Rotation vorhanden (siehe Session Memo 2025-11-02)
- â– Rotationen erfordern Pflege der Wissensgraph-Metadaten und Session-Memos

---

## ADR-024: Event Literal Standardization

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: Dokumentationsbeispiele (Service-Kommunikation & DatenflÃ¼sse, Risikomanagement-Logik) nutzten abweichende Event- und Alert-Bezeichner (`order_results`, `filled_qty`, `DAILY_LIMIT`). `EVENT_SCHEMA.json` definiert jedoch `order_result`, `filled_quantity`, `RISK_LIMIT`, `DATA_STALE`, `CIRCUIT_BREAKER` als verbindliche Literale.

**Optionen**:

- A) Dokumentation unverÃ¤ndert lassen und Abweichungen in FuÃŸnoten erklÃ¤ren
- B) Beispiele auf Schema-Enums angleichen und Wissensgraph-Relationen als `verified` markieren
- C) Schema an Dokumentation anpassen und Konfliktregister erweitern

**Entscheidung**: Option B â€“ Schema bleibt maÃŸgeblich. Alle Beispiele werden angepasst, und `semantic_index.json` markiert die Relationen `event_schema â†’ service_dataflow_doc` und `event_schema â†’ risk_logic_doc` als `relation: "normalized"`, `verified: true`.

**Konsequenzen**:

- â• Einheitliche Payload-Literale eliminieren Tool- und Validierungsfehler
- â• Risk-Alerts nutzen kanonische Codes, wodurch Downstream-Filter funktionieren
- â• Normalization Report kann Phase 3 als abgeschlossen markieren
- â– KÃ¼nftige SchemaÃ¤nderungen erfordern unmittelbare Doku-Anpassungen + Graph-Update

---

## ADR-027: Kontrollierter Archiv-Migrationsprozess (Phase 5)

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: FÃ¼r den Abschluss von Phase 5 mÃ¼ssen Legacy-Dokumente aus `docs/` in das Archiv Ã¼berfÃ¼hrt werden. FrÃ¼here Ad-hoc-Moves fÃ¼hrten zu WissenslÃ¼cken und widersprÃ¼chlichen Referenzen (fehlende Frontmatter, unvollstÃ¤ndige Knowledge-Graph-Updates, kein Dry-Run). Der neue Prozess soll Archivierung, Governance und Wissensgraph synchron halten.

**Optionen**:

- A) Dokumente bei Bedarf direkt verschieben und Migrationen manuell dokumentieren
- B) Einmalige Bulk-Migration durchfÃ¼hren und Nacharbeiten spÃ¤ter erledigen
- C) Einen kontrollierten Workflow mit Review-Plan, Dry-Run und gebundener Dokumentationspflicht einfÃ¼hren ("Safety over neatness")

**Entscheidung**: Option C â€“ Gesteuerter Archivierungsprozess mit verpflichtendem Review, Dry-Run-Report und Governance-Spiegelung. Verschiebungen erfolgen nur bei `migration_status = approved` und gesetztem `approved_target`.

**Konsequenzen**:

- â• Einheitlicher Blick auf alle Kandidaten Ã¼ber `docs/smarter_assistant/migration_plan.md`
- â• Dry-Run (`migration_report_preview.md`) verhindert unbeabsichtigte Moves
- â• Frontmatter (`status`, `source`, `migrated_to`) und Knowledge-Graph bleiben konsistent
- â• Governance-Dokumente (PROJECT_STATUS, SESSION_MEMO_ORGANISATION) spiegeln Migrationen sofort wider
- â– HÃ¶herer Aufwand pro Migration, da Review und Dokumentationsschritte verpflichtend sind

**Umsetzung**:

- `migration_plan.md` als PrimÃ¤rquelle fÃ¼r Status (`planned_target`, `approved_target`, `migration_status`, Review-Notizen)
- Pilot-Migration `7D_PAPER_TRADING_TEST.md` in `archive/docs/` inklusive YAML-Frontmatter (`status: archived`, `migrated_to` gesetzt)
- Einrichtung eines Dry-Run-Reports (`migration_report_preview.md`) vor weiteren Moves
- 2025-11-02: Dry-Run fÃ¼r README_GUIDE.md â†’ `archive/docs/README_GUIDE.md` erstellt; Graph-Kanten `archived_from`/`migrated_to` vorerst mit `verified:false` hinterlegt
- 2025-11-02: Produktive Archivierung freigegeben â€“ Datei liegt unter `archive/docs/README_GUIDE.md`, Frontmatter erweitert, Relationen auf `verified:true` gesetzt
- Nach jeder Freigabe: Updates in `PROJECT_STATUS.md`, `Knowledge_Map.md`, `semantic_index.json` und Session-Memo 2025-11-02
- Beibehaltung der Schutzregel `pending` â†’ kein Move, bis Review abgeschlossen ist

**AbhÃ¤ngigkeiten**:

- Wissensgraph-Artefakte (`Knowledge_Map.md`, `semantic_index.json`) mÃ¼ssen nach tatsÃ¤chlicher Migration angepasst werden
- Archiv-Strukturen (`archive/docs/reports`, `archive/docs/research`, `archive/logs/inventory`) werden vor jedem Move auf Existenz geprÃ¼ft
- Governance bleibt fÃ¼hrend: Abweichungen oder SonderfÃ¤lle werden in `SESSION_MEMO_ORGANISATION_2025-11-02.md` dokumentiert


## ADR-029-R: Soft-Freeze & Continuous Learning Framework

**Datum**: 2025-11-02  
**Status**: âœ… Beschlossen  
**Kontext**: Nach Abschluss der produktiven Archivierung (ADR-027) soll das Repository auditierbar bleiben, ohne den laufenden Betrieb zu blockieren. Reviewer benÃ¶tigen weiter Zugriff auf konsistente Artefakte, wÃ¤hrend Operationsteam und Agenten Wissen und Code fortlaufend pflegen.

**Optionen**:
- A) Bisherigen Hard-Lock beibehalten (keine Ã„nderungen bis Review-Ende)
- B) Soft-Freeze mit Audit-Baseline und verpflichtender Protokollierung
- C) VollstÃ¤ndige Entsperrung ohne zusÃ¤tzliche Kontrollen

**Entscheidung**: Option B â€“ Soft-Freeze. Audit-Baseline (`audit_snapshot_2025-11-02.json`) bleibt Referenz, Delta-Audits protokollieren Ã„nderungen, Live-Writes bleiben unter ADR-027-Sicherheitsregeln erlaubt.

**Konsequenzen**:
- â• Repository bleibt produktiv nutzbar (Paper/Live Trading, Wissenspflege)
- â• Jede Ã„nderung bleibt rÃ¼ckverfolgbar (Snapshot + Delta-Audit, Session-Memo)
- â– ZusÃ¤tzlicher Aufwand fÃ¼r kontinuierliche Delta-Dokumentation

**Folgeaktionen**:
- `PROJECT_STATUS.md`: Governance Mode Abschnitt mit Soft-Freeze-Status
- `SESSION_MEMO_ORGANISATION_2025-11-02.md`: Kontinuierliche Operation samt Delta-Audit-Vermerk dokumentiert
- `backoffice/audits/`: Delta-Audit-Dateien pro Lauf anlegen; Baseline regelmÃ¤ÃŸig erneuern


## Audit-Review-Abschluss: Keine Findings, ADR-030 nicht erforderlich

**Datum**: 2025-11-02 18:30 UTC  
**Status**: âœ… Abgeschlossen  
**Kontext**: Nach Handover Report 2025-11-02 17:00 UTC hat Audit-Team (GitHub Copilot) unabhÃ¤ngigen 7-Phasen-Review nach REVIEW_README.md-Protokoll durchgefÃ¼hrt. Ziel war Verifikation von Governance-KohÃ¤renz, Knowledge-Graph-Konsistenz und technischer IntegritÃ¤t.

**PrÃ¼fumfang**:
1. **Audit-Artefakte**: `audit_snapshot_2025-11-02.json`, `delta_audit_2025-11-02T16-45Z.json`, `semantic_index_export.graphml`
2. **Governance**: ADR-027 â†’ ADR-029-R Chain, Continuous Operation Mode, Git-Refs
3. **Knowledge-Layer**: `semantic_index.json` (â‰¥95% verified:true), `Knowledge_Map.md`, Archive-Cluster
4. **Technik**: Docker-Status (10/10 Container healthy), ENV/Compose-Konsistenz, requirements.txt
5. **Review-Bericht**: `HANDOVER_REVIEW_REPORT_2025-11-02T18-30Z.md` (450+ Zeilen)

**Ergebnis**:
- âœ… **Governance**: ADR-Chain vollstÃ¤ndig (ADR-027 â†’ ADR-029-R), Continuous Operation Mode aktiv
- âœ… **Knowledge-Graph**: 100% Relations `verified:true` (manuelle PrÃ¼fung bestÃ¤tigt â‰¥95%-Anforderung erfÃ¼llt)
- âœ… **Technik**: 10/10 Container healthy (6h+ Uptime), ENV/Compose konsistent (REDIS_PASSWORD = REDACTED_REDIS_PW, POSTGRES_PASSWORD = cdb_secure_password_2025)
- âœ… **Link-Audit**: Letzter Run 2025-11-02 15:10 UTC â†’ 0 Fehler
- ğŸŸ¡ **Optionale Empfehlungen**: 2 Package-Updates (redis 7.0.0â†’7.0.1, ruff 0.14.2â†’0.14.3), 1 Doku-ErgÃ¤nzung (GraphML-Viewer-Hinweis)

**Entscheidung**: **ADR-030 nicht erforderlich**  
**BegrÃ¼ndung**: Keine kritischen Findings, keine Governance-Abweichungen, System operational-ready. Optionale Package-Updates kÃ¶nnen im Rahmen regulÃ¤rer Maintenance erfolgen (kein Audit-Blocker).

**Konsequenzen**:
- â• Phase 7 (Paper Trading) genehmigt â€“ System bereit fÃ¼r Produktivbetrieb
- â• Continuous-Operation-Mode bleibt aktiv (ADR-029-R), keine Sperren
- â• Repository weiterhin schreibfÃ¤hig unter ADR-027-Safety-Protokoll
- â– Optionale Package-Updates bleiben dokumentiert (code_review_prep.md), aber nicht verpflichtend

**Deliverables**:
- `HANDOVER_REVIEW_REPORT_2025-11-02T18-30Z.md` (backoffice/audits/)
- `PROJECT_STATUS.md` aktualisiert (Phase 6.8: Audit-Team Review)
- `DECISION_LOG.md` (dieser Eintrag)

**Sign-Off**: GitHub Copilot (Audit-Team) â†’ IT-Chef  
**Freigabe**: Repository operational-ready, Phase 7 kann starten.

---

## ADR-031: Development Philosophy - Quality over Speed

**Datum**: 2025-11-03  
**Status**: âœ… Beschlossen  
**Kontext**: Nach erfolgreicher Stabilisierung in Phase 7 soll die Entwicklungsphilosophie explizit formalisiert werden: **QualitÃ¤t und Sorgfalt haben Vorrang vor Geschwindigkeit**. Dies reflektiert die bewÃ¤hrten Praktiken, die zum aktuellen stabilen Zustand gefÃ¼hrt haben.

**Problem**:
- Schnelle, ungeprÃ¼fte Ã„nderungen fÃ¼hrten historisch zu InstabilitÃ¤ten (z.B. compose.yaml-Konflikt, ADR-005)
- Dokumentations-LÃ¼cken erschwerten Debugging und Onboarding
- Fehlende Governance-Prozesse verzÃ¶gerten Reviews und Audits

**Entscheidung**: Etablierung verbindlicher Entwicklungsprinzipien:

### 1. **Dokumentation vor Code**
- Jede Ã„nderung wird **erst dokumentiert, dann implementiert**
- Architektur-Ã„nderungen â†’ `ARCHITEKTUR.md` + ADR in `DECISION_LOG.md`
- Event-Schema-Ã„nderungen â†’ `EVENT_SCHEMA.json` + betroffene `models.py`
- KonfigurationsÃ¤nderungen â†’ `.env`, `docker-compose.yml` + Validierung

### 2. **Schrittweise Umsetzung**
- Keine "Big Bang"-Ã„nderungen; iterative, validierte Schritte
- Nach jeder Ã„nderung: `docker compose config`, Health-Checks, Tests
- Bei Unsicherheit: **lieber nachfragen statt raten**

### 3. **Ordnung als PrioritÃ¤t**
- Keine temporÃ¤ren Workarounds im produktiven Code
- Deprecated Code â†’ `archive/` mit BegrÃ¼ndung
- Duplikate vermeiden, bestehende Strukturen nutzen

### 4. **Mandatory Review-Checkpoints**
- Vor jedem Commit: Review-Checkliste aus `DEVELOPMENT.md` durchgehen
- Bei strukturellen Ã„nderungen: Audit-Snapshot + Delta-Audit
- Session-Ende: `SESSION_MEMO` mit Zeitstempel + Entscheidungen

### 5. **Fehlerkultur**
- Fehler sind Lernchancen, nicht Blocker
- Incident Reports dokumentieren Root Cause + Prevention (siehe `2025-10-30_RECOVERY_REPORT.md`)
- Knowledge Base wird kontinuierlich erweitert (Research-Dokumente)

**Implementierung**:
- `DEVELOPMENT.md` erweitert um "0ï¸âƒ£ Entwicklungsphilosophie"-Abschnitt
- `ARCHITEKTUR_REGELN.md` um Abschnitt "6. Entwicklungstempo" ergÃ¤nzt
- `SESSION_MEMO_PHILOSOPHY_2025-11-03.md` als EinfÃ¼hrungsdokument

**Konsequenzen**:
- â• StabilitÃ¤t und Wartbarkeit haben Vorrang
- â• Neue Entwickler kÃ¶nnen sich auf klare Prinzipien verlassen
- â• Audits und Reviews werden beschleunigt (weniger Nacharbeiten)
- â– Entwicklungszyklen werden lÃ¤nger (bewusst akzeptiert)
- â– Erfordert Disziplin und kontinuierliche Dokumentation

**Validation**:
- Alle zukÃ¼nftigen PRs mÃ¼ssen Review-Checkliste erfÃ¼llen
- Session-Memos sind verpflichtend fÃ¼r strukturelle Ã„nderungen
- Continuous Operation Mode (ADR-029-R) bleibt aktiv, aber Safety-Protokoll wird strenger

**Referenzen**:
- `DEVELOPMENT.md` - Entwicklungsrichtlinien
- `ARCHITEKTUR_REGELN.md` - Operative Leitplanken
- `2025-10-30_RECOVERY_REPORT.md` - Lessons Learned aus Stabilisierungsphase

**Sign-Off**: GitHub Copilot (Development Philosophy Initiative)  
**GÃ¼ltigkeit**: Ab sofort fÃ¼r alle Repository-Ã„nderungen

---

## ADR-032: Python Base Image Pin auf 3.13-slim (statt 3.14-slim)

**Datum**: 2025-11-09  
**Status**: âœ… Beschlossen  
**Kontext**: Dependabot schlug Updates von `python:3.11-slim` â†’ `python:3.14-slim` fÃ¼r alle Dockerfiles vor (PRs #15, #13, #12). Python 3.14.0 wurde am 15.10.2024 released und ist auf Docker Hub verfÃ¼gbar.

**Problem**:
- Python 3.14 ist erst seit ~3 Wochen stabil (released 15.10.2024)
- Production-Systeme benÃ¶tigen bewÃ¤hrte, stabile Versionen
- 3 Major-Bumps (3.11â†’3.12â†’3.13â†’3.14) erhÃ¶hen Risiko fÃ¼r Breaking Changes
- Dependabot empfiehlt automatisch die neueste verfÃ¼gbare Version (nicht immer optimal)

**Evaluierte Optionen**:

1. **Python 3.14-slim** (neueste)
   - â• Neueste Features & Security-Patches
   - â– Erst seit 3 Wochen stabil
   - â– Unbekannte Production-Erfahrungen
   - â– 3 Major-Bumps erhÃ¶hen Test-Aufwand

2. **Python 3.13-slim** (empfohlen)
   - â• Released 2024-10-07 (bereits 1 Monat stabil)
   - â• EOL: 2029-10 (guter Support-Zeitraum)
   - â• Gut getesteter Upgrade-Pfad 3.11â†’3.13
   - â• Balance zwischen AktualitÃ¤t und StabilitÃ¤t
   - â– Nicht die absolute neueste Version

3. **Python 3.12-slim** (konservativ)
   - â• LTS-Version, sehr stabil
   - â• EOL: 2028-10
   - â– Weniger neue Features

**Entscheidung**: Pin auf **python:3.13-slim** fÃ¼r alle Services

**BegrÃ¼ndung**:
- **Produktions-StabilitÃ¤t:** Python 3.13 hat bereits ~1 Monat Production-Erprobung
- **Sicherheits-UnterstÃ¼tzung:** EOL 2029-10 deckt Multi-Jahr-Support ab
- **BewÃ¤hrter Upgrade-Pfad:** 3.11â†’3.13 ist gut dokumentiert & getestet
- **Risk-Mitigation:** 2 Major-Bumps statt 3 reduziert Breaking-Change-Risiko
- **Best Practice:** Production-Systeme sollten nicht auf bleeding-edge Versionen laufen

**Alternative fÃ¼r 3.14-Fans**:
Falls Python 3.14 gewÃ¼nscht wird, exakte Version pinnen:
```dockerfile
FROM python:3.14.0-slim
```
Statt `3.14-slim` (verhindert auto-upgrade auf 3.14.1, 3.14.2, etc.)

**Implementierung**:
- PRs #15, #13, #12: Ã„nderung von `3.14-slim` â†’ `3.13-slim` committen
- Docker Compose Build-Tests durchfÃ¼hren
- Service-Start & Health-Checks validieren
- Nach grÃ¼nen Tests â†’ mergen

**Betroffene Dateien**:
- `backoffice/services/signal_engine/Dockerfile`
- `backoffice/services/risk_manager/Dockerfile`
- `Dockerfile` (root, fÃ¼r Screener)

**Rollback-Plan**:
Falls KompatibilitÃ¤tsprobleme auftreten:
```dockerfile
FROM python:3.11-slim
```

**Testing-Protokoll**:
- âœ… Docker Hub Tag-VerfÃ¼gbarkeit geprÃ¼ft (3.13-slim verfÃ¼gbar)
- â³ Docker Build Tests (nach Commit)
- â³ Service Health-Checks (nach Deployment)
- â³ E2E-Test (optional, da kein Breaking Change erwartet)

**Konsequenzen**:
- â• Stabile, production-ready Python-Version
- â• Reduziertes Risiko fÃ¼r Breaking Changes
- â• Multi-Jahr-Support durch EOL 2029
- â– Verzicht auf absolute neueste Features (Python 3.14)
- â– Erfordert manuellen Dependabot-Override (statt auto-merge)

**Related PRs**:
- PR #15: signal_engine Docker Update
- PR #13: root Docker Update
- PR #12: risk_manager Docker Update

**Referenzen**:
- [Python Release Schedule](https://peps.python.org/pep-0619/)
- [Docker Hub python:3.13-slim](https://hub.docker.com/_/python?tab=tags&name=3.13-slim)
- `docs/PR_REVIEW_BATCH_2025_11_09.md` (Detailanalyse)

**Sign-Off**: GitHub Copilot Coding Agent (PR Review Session)  
**GÃ¼ltigkeit**: Ab sofort fÃ¼r alle Python-Dockerfile-Updates




---

## ADR-032: Copilot-Instructions Update - Issue #6 Integration

**Datum**: 2025-11-09  
**Status**: âœ… Beschlossen  
**Kontext**: Issue #6 enthielt umfangreiche Application-Configuration fÃ¼r Copilot Coding Agent mit operativen Anweisungen, die in der aktuellen `copilot-instructions.md` fehlten. Diese sollten mit der bestehenden Konfiguration verglichen und bei Verbesserungen Ã¼bernommen werden.

**Problem**: 
- Aktuelle `copilot-instructions.md` (80 Zeilen) fokussierte sich auf allgemeine Leitplanken
- Issue #6 Content enthielt spezifische operative Anweisungen:
  - Session-Start-Pflicht (Docker-Container prÃ¼fen/starten)
  - Audit-Referenzen mit konkreten Dateipfaden
  - Architekturfluss (Event-Pipeline)
  - Logging-Regeln
  - Sofortige Dokumentationspflicht
  - Konkrete Validierungsbefehle

**Optionen**:
- A) Issue #6 Content komplett Ã¼bernehmen und bestehende Struktur ersetzen
- B) Nur neue Inhalte minimal-invasiv in bestehende Struktur integrieren
- C) Separate Datei fÃ¼r operative Anweisungen erstellen

**Entscheidung**: Option B - Minimal-invasive Erweiterung der bestehenden Struktur

**Implementierung**:

### 1. Abschnitt 2 erweitert: "Session-Start & Sicherheits-Regeln"
- **Neu 2.1 Session-Start-Routine (PFLICHT)**:
  - Container-Status prÃ¼fen: `docker ps --filter "name=cdb_"`
  - Falls Container fehlen: `docker compose up -d`
  - 10 Sekunden warten und Health-Status prÃ¼fen
  - `PROJECT_STATUS.md` lesen vor weiteren Aufgaben
- **2.2 Sicherheits- & Compliance-Regeln** (bestehend, unverÃ¤ndert)

### 2. Abschnitt 6 erweitert: "Arbeitsrichtlinien (Do)"
- **Architekturfluss**: `market_data` â†’ `signals` â†’ `orders` â†’ `order_results`
- **Payload-Validierung**: EVENT_SCHEMA.json Pflicht, Ã„nderungen in models.py spiegeln
- **Logging-Regel**: Nur Ã¼ber `backoffice/logging_config.json` (keine Inline-Logger)
- **Sofortige Dokumentation**: Nach jeder Handlung dokumentieren, nicht erst am Sessionende (entspricht ADR-015)

### 3. Abschnitt 7 erweitert: "Tests & Validierungen"
- **Validierung vor Merge (PFLICHT)** hinzugefÃ¼gt:
  - `docker compose config` ohne Fehler
  - Services mit Health-Checks grÃ¼n (`/health`, `/status`, `/metrics`)
  - `.env` ohne Duplikate; Ports und DB-Name konsistent
  - Schema- und Event-Checks gegen `EVENT_SCHEMA.json`

### 4. Neuer Abschnitt 11: "Audit-Referenzen"
- **Aktuellste Audits** mit konkreten Dateipfaden:
  - `HANDOVER_REVIEW_REPORT_2025-11-02T18-30Z.md` (neuester)
  - `HANDOVER_REPORT_2025-11-02.md`
  - `2025-10-30_RECOVERY_REPORT.md`
  - `AUDIT_SUMMARY.md`, `DIFF-PLAN.md`
- **Audit-Vorgaben**: DIFF-PLAN.md als Quelle nutzen, Abweichungen dokumentieren

**Konsequenzen**:
- â• Operative StabilitÃ¤t durch Session-Start-Routine sichergestellt
- â• Klare Audit-Referenzen fÃ¼r Nachvollziehbarkeit
- â• Architekturfluss und Logging-Regeln explizit dokumentiert
- â• Konkrete Validierungsbefehle vermeiden Fehler vor Merge
- â• Bestehende Struktur (10 Abschnitte) bleibt erhalten, nur erweitert
- â– Datei wÃ¤chst von 80 auf 109 Zeilen (+36%)

**Validation**:
- âœ… Alle 11 Abschnitte vorhanden und korrekt strukturiert
- âœ… Neue Inhalte sinnvoll in bestehende Abschnitte integriert
- âœ… Audit-Dateipfade gegen `backoffice/audits/` validiert
- âœ… Keine bestehenden Inhalte Ã¼berschrieben oder entfernt

**Referenzen**:
- Issue #6: "Application Adolph" - GitHub Issue mit Copilot-Konfiguration
- ADR-015: Sofortige Handlungsdokumentation im Copilot-Workflow
- ADR-031: Development Philosophy - Quality over Speed
- `backoffice/audits/` - Referenzierte Audit-Dateien

**Sign-Off**: GitHub Copilot  
**GÃ¼ltigkeit**: Ab sofort fÃ¼r alle Copilot-Sessions

---

## ADR-033: Titel-Norm & Board-Automatisierung aktiviert

**Datum**: 2025-11-09
**Status**: Entwurf / Implemented (tools added: PR Title Lint, Labeler)

Kurz: Standardisierung von PR/Issue-Titeln und EinfÃ¼hrung leichtgewichtiger Automatisierungen fÃ¼r das Kanban-Board (Saved Views, Felder, Automationen als Spezifikation). Actions zur Titel-PrÃ¼fung und automatisches Labeling wurden als PR zur ÃœberprÃ¼fung hinzugefÃ¼gt.

Referenzen:
- docs/KANBAN_SETUP.md

---

## ADR-034: Copilot-Instructions Update - Verantwortlicher gesetzt

**Datum**: 2025-11-10  
**Status**: âœ… Beschlossen

**Kontext**: Die Copilot-Instruktionen enthielten bei der Verantwortlichkeit fÃ¼r das letzte Update den Platzhalter "TBD".

**Ã„nderung**: Aktualisierung der Zeile "Letztes Update" in `.github/copilot-instructions.md`:
- Von: `Verantwortlich: TBD`
- Zu: `Verantwortlich: jannekbuengener`

**BegrÃ¼ndung**: Klare Zuordnung der Verantwortlichkeit fÃ¼r die Copilot-Instruktionen an den Repository-Owner.

**Konsequenzen**:
- â• Klare Verantwortlichkeit dokumentiert
- â• VollstÃ¤ndige Audit-Trail fÃ¼r Copilot-Konfiguration
## ADR-035: ENV-Naming-Konvention fÃ¼r Risk-Parameter (Dezimal-Format)

**Datum**: 2025-11-16
**Status**: âœ… Akzeptiert
**Verantwortlicher**: jannekbuengener (via Pipeline 4 - Multi-Agenten-System)

### Kontext

Vor der Migration existierte eine inkonsistente ENV-Naming-Konvention fÃ¼r Risk-Parameter:
- `MAX_DAILY_DRAWDOWN=5.0` (Bedeutung unklar: 5% oder 500%?)
- `MAX_POSITION_SIZE=10.0` (10% oder 1000%?)
- `MAX_TOTAL_EXPOSURE=50.0` (50% oder 5000%?)

**Problem**: Service-Code interpretierte diese Werte als Ganzzahlen, nicht als Prozentangaben:
```python
# FALSCH - liest 5.0 als 500%:
max_dd = float(os.getenv("MAX_DAILY_DRAWDOWN"))  # 5.0 â†’ wird als 500% behandelt!
if daily_loss > max_dd:  # Daily loss 6% > 5.0? NEIN â†’ Limit unwirksam!
```

**Konsequenz**: Risk-Limits waren faktisch unwirksam, da sie um Faktor 100 zu hoch interpretiert wurden.

### Entscheidung

Alle Prozent-Angaben in ENV-Variablen nutzen **Dezimal-Format** (0.05 = 5%) und Suffix `_PCT`.

**Neue Konvention**:
```bash
# Alte Namen (ENTFERNT):
# MAX_DAILY_DRAWDOWN=5.0
# MAX_POSITION_SIZE=10.0
# MAX_TOTAL_EXPOSURE=50.0

# Neue Namen (Dezimal-Format):
MAX_DAILY_DRAWDOWN_PCT=0.05    # 5%
MAX_POSITION_PCT=0.10          # 10%
MAX_EXPOSURE_PCT=0.50          # 50%
STOP_LOSS_PCT=0.02             # 2%
MAX_SLIPPAGE_PCT=0.01          # 1%

# Ausnahmen (keine Prozente):
MAX_SPREAD_MULTIPLIER=5.0      # 5x (Faktor, kein Prozent)
DATA_STALE_TIMEOUT_SEC=30      # 30 Sekunden
```

**Code-Ã„nderung** (Service-Side):
```python
# KORREKT - liest 0.05 als 5%:
max_dd_pct = float(os.getenv("MAX_DAILY_DRAWDOWN_PCT"))  # 0.05 â†’ 5%
if daily_loss_pct > max_dd_pct:  # Daily loss 6% > 5%? JA â†’ Limit greift!
    halt_trading()
```

### Konsequenzen

**Positiv**:
- âœ… Eindeutige Interpretation (0.05 = 5%, nicht 500%)
- âœ… Konsistent mit Python float-Arithmetik (0.05 * portfolio_value)
- âœ… Alle Risk-Parameter mit `_PCT` Suffix (Typ-Safety durch Naming)
- âœ… Min/Max-Werte in Dezimal-Format dokumentiert (z.B. Min: 0.01, Max: 0.20 fÃ¼r Drawdown)

**Negativ**:
- âš ï¸ **Breaking Change**: Alte ENV-Namen (`MAX_DAILY_DRAWDOWN`) nicht mehr gÃ¼ltig
- âš ï¸ Code-Ã„nderungen in allen Services erforderlich (config.py, risk_manager)
- âš ï¸ Bestehende .env-Dateien mÃ¼ssen aktualisiert werden

**Migration-Aufwand**:
- .env.template: Alle ENV-Namen aktualisiert âœ…
- Service-Code: `os.getenv("MAX_DAILY_DRAWDOWN")` â†’ `os.getenv("MAX_DAILY_DRAWDOWN_PCT")`
- Tests: Risk-Parameter-Tests an neue Werte anpassen (5.0 â†’ 0.05)

### Betroffene ENV-Variablen

| Alte Variable | Neue Variable | Default | Min | Max |
|---------------|---------------|---------|-----|-----|
| `MAX_DAILY_DRAWDOWN=5.0` | `MAX_DAILY_DRAWDOWN_PCT=0.05` | 0.05 (5%) | 0.01 | 0.20 |
| `MAX_POSITION_SIZE=10.0` | `MAX_POSITION_PCT=0.10` | 0.10 (10%) | 0.01 | 0.25 |
| `MAX_TOTAL_EXPOSURE=50.0` | `MAX_EXPOSURE_PCT=0.50` | 0.50 (50%) | 0.10 | 1.00 |
| *(neu)* | `STOP_LOSS_PCT=0.02` | 0.02 (2%) | 0.005 | 0.10 |
| *(neu)* | `MAX_SLIPPAGE_PCT=0.01` | 0.01 (1%) | 0.001 | 0.05 |
| *(neu)* | `MAX_SPREAD_MULTIPLIER=5.0` | 5.0 (5x) | 2.0 | 10.0 |
| *(neu)* | `DATA_STALE_TIMEOUT_SEC=30` | 30 (30s) | 10 | 120 |

### Referenzen

- **Pre-Migration Task**: SR-002 (ENV-Naming normalisieren)
- **Canonical Schema**: `backoffice/docs/canonical_schema.yaml` â†’ Sektion `env_variables`
- **Security-Risk**: SR-002 in `infra_conflicts.md`
- **Pipeline**: Pipeline 4 - Kanonische Systemrekonstruktion

---

## ADR-036: Secrets-Management-Policy (Never Commit Secrets)

**Datum**: 2025-11-16
**Status**: âœ… Akzeptiert
**Verantwortlicher**: jannekbuengener (via Pipeline 4 - Multi-Agenten-System)

### Kontext

Vor der Migration wurden Secrets im Klartext in ` - Kopie.env` committed:
```bash
# ` - Kopie.env` (FALSCH - Secrets committed!):
POSTGRES_PASSWORD=Jannek8$
GRAFANA_PASSWORD=Jannek2025!
DATABASE_URL=postgresql://claire:Jannek8$@cdb_postgres:5432/claire_de_binare
```

**Probleme**:
1. **Security-Risk SR-001**: Exposed Secrets im Git-Repo (Ã¶ffentlich oder intern sichtbar)
2. **Git-History**: Secrets bleiben in Git-History, selbst nach LÃ¶schen der Datei
3. **Rotation unmÃ¶glich**: Passwort-Wechsel erfordert Git-History-Bereinigung
4. **Compliance**: VerstÃ¶ÃŸt gegen Security-Best-Practices (OWASP, CIS Benchmarks)

### Entscheidung

**Strikte Trennung** zwischen `.env.template` (committed) und `.env` (gitignored, lokal):

1. **`.env.template`** (committed im Git-Repo):
   - EnthÃ¤lt ALLE ENV-Variablen-Namen
   - Secrets als Platzhalter: `<SET_IN_ENV>`
   - Dokumentation (Kommentare): Bedeutung, Min/Max, Defaults
   - Versioniert, Teil des Repos

2. **`.env`** (lokal, NIEMALS committed):
   - Kopie von `.env.template`
   - Platzhalter durch echte Secrets ersetzt
   - In `.gitignore` eingetragen
   - Nur auf lokalem System / Production-Servern

### Konsequenzen

**Positiv**:
- âœ… Keine Secrets im Git-Repo (weder aktuell noch in History)
- âœ… Neue Setups einfach: `cp .env.template .env` â†’ Platzhalter ersetzen
- âœ… Rotation: Nur lokale `.env` Ã¤ndern + Container-Restart (kein Git-Commit nÃ¶tig)
- âœ… Dokumentation: `.env.template` zeigt ALLE benÃ¶tigten Variablen
- âœ… Compliance: ErfÃ¼llt Security-Best-Practices

**Negativ**:
- âš ï¸ Manuelle Arbeit: Platzhalter mÃ¼ssen lokal ersetzt werden
- âš ï¸ Secret-Management: Keine automatische Distribution (z.B. via Vault, AWS Secrets Manager)
- âš ï¸ Backup: Lokale `.env` muss separat gesichert werden (auÃŸerhalb Git)

### Umsetzung

#### .env.template (Beispiel-Struktur)

```bash
# ============================================================================
# DATABASE (PostgreSQL)
# ============================================================================
POSTGRES_DB=claire_de_binare
POSTGRES_USER=<SET_IN_ENV>           # Username fÃ¼r PostgreSQL (z.B. "claire")
POSTGRES_PASSWORD=<SET_IN_ENV>       # Starkes Passwort (min. 16 Zeichen)
DATABASE_URL=postgresql://<USER>:<PASSWORD>@cdb_postgres:5432/claire_de_binare

# ============================================================================
# MESSAGE BUS (Redis)
# ============================================================================
REDIS_HOST=cdb_redis
REDIS_PORT=6379
REDIS_PASSWORD=<SET_IN_ENV>          # Starkes Passwort (min. 16 Zeichen)

# ============================================================================
# MEXC API (CRITICAL - System nicht funktionsfÃ¤hig ohne!)
# ============================================================================
MEXC_API_KEY=<SET_IN_ENV>            # API-Key aus MEXC-Account
MEXC_API_SECRET=<SET_IN_ENV>         # API-Secret aus MEXC-Account
```

#### .gitignore (Eintrag sicherstellen)

```bash
# Environment
.env
.env.local
*.env
# Exclude all .env files in docker directories
docker/**/.env
# But include .env.example templates
!docker/**/.env.example
!.env.template
```

#### Setup-Prozess (neue Deployments)

```bash
# 1. .env.template kopieren
cp .env.template .env

# 2. .env Ã¶ffnen und Platzhalter ersetzen
nano .env  # oder code .env

# 3. Secrets eintragen (manuell oder via Secret-Manager)
# POSTGRES_PASSWORD=<starkes-passwort-generieren>
# REDIS_PASSWORD=<starkes-passwort-generieren>
# MEXC_API_KEY=<aus-mexc-account>
# ...

# 4. Validieren: .env nicht in git status
git status | grep -q "\.env" && echo "FEHLER: .env in Git!" || echo "OK"
```

#### Optional: Pre-Commit-Hook

```bash
# .git/hooks/pre-commit
#!/bin/bash
if git diff --cached --name-only | grep -q "^\.env$"; then
  echo "âŒ ERROR: .env darf nicht committed werden!"
  echo "Nur .env.template sollte versioniert sein."
  exit 1
fi
```

### Betroffene Secrets

| Secret | ENV-Variable | Verwendung |
|--------|--------------|------------|
| PostgreSQL User | `POSTGRES_USER` | Datenbank-Zugriff |
| PostgreSQL Password | `POSTGRES_PASSWORD` | Datenbank-Auth |
| Redis Password | `REDIS_PASSWORD` | Message-Bus-Auth |
| Grafana Admin Password | `GRAFANA_PASSWORD` | Monitoring-UI-Zugriff |
| MEXC API Key | `MEXC_API_KEY` | Exchange-API-Zugriff |
| MEXC API Secret | `MEXC_API_SECRET` | Exchange-API-Signierung |

### Referenzen

- **Pre-Migration Task**: SR-001 (Secrets bereinigen)
- **Security-Risk**: SR-001 in `infra_conflicts.md` (Exposed Secrets in ` - Kopie.env`)
- **Pipeline**: Pipeline 4 - Kanonische Systemrekonstruktion

---

## ADR-037: Legacy-Service cdb_signal_gen entfernt

**Datum**: 2025-11-16
**Status**: âœ… Akzeptiert
**Verantwortlicher**: jannekbuengener (via Pipeline 4 - Multi-Agenten-System)

### Kontext

Service `cdb_signal_gen` war in `docker-compose.yml` definiert:
```yaml
cdb_signal_gen:
  build:
    context: .
    dockerfile: Dockerfile.signal_gen  # â† Diese Datei fehlt!
  container_name: cdb_signal_gen
  restart: unless-stopped
  environment:
    REDIS_HOST: cdb_redis
    REDIS_PORT: 6379
    REDIS_PASSWORD: ${REDIS_PASSWORD}
  depends_on:
    - cdb_redis
  networks:
    - cdb_network
```

**Probleme**:
1. **Dockerfile.signal_gen fehlt** â†’ `docker compose up` schlÃ¤gt fehl
2. **Keine Service-Implementierung** gefunden (kein Code in `backoffice/services/`)
3. **Funktions-Ãœberschneidung**: Service `cdb_core` (Signal Engine) Ã¼bernimmt bereits Signal-Generierung

**Hypothese**: `cdb_signal_gen` ist Legacy aus frÃ¼herer Entwicklungsphase, wurde durch `cdb_core` abgelÃ¶st.

### Entscheidung

Service `cdb_signal_gen` aus `docker-compose.yml` entfernen (auskommentieren).

**BegrÃ¼ndung**:
- `cdb_core` (Signal Engine) ist vollstÃ¤ndig implementiert und Ã¼bernimmt Signal-Generierung
- Dockerfile fehlt â†’ Service nicht deploybar
- Keine Business-Logik identifiziert, die verloren ginge

**Alternative nicht gewÃ¤hlt**: Dockerfile.signal_gen neu erstellen
- **Grund**: WÃ¼rde doppelte Signal-Generierung bedeuten (cdb_core + cdb_signal_gen)
- **Aufwand**: Unklar, welche Logik der Service haben sollte

### Konsequenzen

**Positiv**:
- âœ… `docker compose config --quiet` â†’ kein Fehler mehr
- âœ… `docker compose up -d` â†’ erfolgreich (alle Services starten)
- âœ… Keine funktionale EinbuÃŸe (cdb_core Ã¼bernimmt Rolle)
- âœ… Klarere Service-Landschaft (weniger verwirrende Legacy-Reste)

**Negativ**:
- âš ï¸ Falls Service doch benÃ¶tigt: Dockerfile.signal_gen muss erstellt werden ODER Funktion in cdb_core migrieren
- âš ï¸ Unklarheit Ã¼ber ursprÃ¼ngliche Absicht (Doku fehlt)

**Risiko-Bewertung**: ğŸŸ¢ LOW
- Signal-Generierung funktioniert via cdb_core
- Kein Business-Impact identifiziert

### Rollback-Plan

Falls sich herausstellt, dass Service doch benÃ¶tigt wird:

**Option 1**: Dockerfile.signal_gen erstellen
```dockerfile
# Dockerfile.signal_gen (hypothetisch)
FROM python:3.11-slim
WORKDIR /app
COPY signal_generator.py .
COPY requirements.txt .
RUN pip install -r requirements.txt
CMD ["python", "signal_generator.py"]
```

**Option 2**: Funktion in cdb_core integrieren
- Legacy-Code reviewen
- Logik in cdb_core/service.py einbauen
- Tests ergÃ¤nzen

### Betroffene Dateien

| Datei | Ã„nderung |
|-------|----------|
| `docker-compose.yml` | Service-Block `cdb_signal_gen` entfernt/auskommentiert |
| `Dockerfile.signal_gen` | Fehlt (war nie vorhanden) |

### Signal-Generierung nach Entfernung

**Aktuelle Implementierung** (via cdb_core):
```
market_data (cdb_ws/cdb_rest)
    â†“
cdb_core (Signal Engine)
    â†’ Momentum-Strategie
    â†’ SIGNAL_THRESHOLD=3.0
    â†’ MIN_VOLUME=100000
    â†“
signals (Redis Topic)
    â†“
cdb_risk (Risk Manager)
```

### Referenzen

- **Pre-Migration Task**: Task 4 (cdb_signal_gen entfernen)
- **Security-Risk**: SR-006 in `infra_conflicts.md` (cdb_signal_gen ohne Health-Check & fehlende Dockerfile)
- **Canonical Schema**: `backoffice/docs/canonical_schema.yaml` â†’ Sektion `services` (cdb_signal_gen nicht enthalten)
- **Pipeline**: Pipeline 4 - Kanonische Systemrekonstruktion

---

## ADR-038: Test-Strategie - Phasenweise EinfÃ¼hrung (Smoke-Test statt pytest)

**Datum**: 2025-11-16
**Status**: âœ… Akzeptiert
**Verantwortlicher**: jannekbuengener (via Cleanroom-Migration Pipeline 4)

### Kontext

Nach Abschluss der Pre-Migration-Tasks (SR-001 bis SR-003 behoben, cdb_signal_gen entfernt per ADR-037) steht das Cleanroom-Repo vor dem ersten produktiven Start. Die Ã¼bliche Test-Strategie wÃ¤re:

1. Unit-Tests fÃ¼r alle Services (pytest)
2. Integration-Tests fÃ¼r Event-Flows
3. E2E-Tests fÃ¼r gesamte Pipeline

**Probleme in dieser Phase**:
- pytest ist weder im Host noch in den Service-Containern installiert
- requirements-dev.txt existiert nicht
- Test-Struktur (tests/unit/, tests/integration/) ist noch nicht definiert
- Alle Services sind jedoch healthy (8/8 Container laufen)
- Pre-Migration-Validierung war erfolgreich (Konflikte gelÃ¶st, Schema kanonisiert)

**Fragestellung**: KÃ¶nnen wir das System ohne vollstÃ¤ndige pytest-Suite als "funktionsfÃ¤hig" akzeptieren und den Initial-Commit durchfÃ¼hren?

### Entscheidung

**GewÃ¤hlte Strategie: Option C + A** (aus DECISION-004 in CLAUDE.md)

**Phase 1 - Cleanroom-Migration (JETZT)**:
1. **Smoke-Test als Acceptance-Kriterium**:
   - Manueller End-to-End-Test des Event-Flows: `market_data â†’ signals â†’ orders â†’ order_results`
   - Verifizierung Ã¼ber Docker-Logs (keine automatisierten Assertions)
   - Acceptance-Kriterien:
     - Alle Services bleiben healthy wÃ¤hrend des Tests
     - Event mit "smoke_test"-Marker ist in allen relevanten Logs sichtbar
     - Event-Flow ist vollstÃ¤ndig (kein Abbruch in der Kette)
     - Keine CRITICAL-Fehler in Logs

2. **Initial Commit nach Smoke-Test**:
   - Wenn Smoke-Test besteht â†’ Git-Commit + Tag `v1.0-cleanroom`
   - Wenn Smoke-Test fehlschlÃ¤gt â†’ Blocker identifizieren, fixen, wiederholen

**Phase 2 - Post-Migration (SPÃ„TER)**:
- pytest in virtualenv installieren
- requirements-dev.txt anlegen (pytest, pytest-cov, black, mypy)
- Test-Struktur definieren:
  - `tests/unit/` fÃ¼r Risk-Manager, Signal-Engine, Execution-Service
  - `tests/integration/` fÃ¼r Event-Flow-Validierung
  - `tests/e2e/` fÃ¼r Full-Stack-Szenarien
- Test-Coverage-Ziel: Risk-Manager 0% â†’ 80%, andere Services mind. 60%

**BegrÃ¼ndung**:
- Smoke-Test validiert die kritischste FunktionalitÃ¤t (Event-Flow) sofort
- pytest-Setup ist zeitintensiv und blockiert Initial-Commit unnÃ¶tig
- Alle Pre-Migration-Risiken (SR-001 bis SR-003) sind bereits behoben
- Services laufen stabil (Health-Checks grÃ¼n)

### Smoke-Test-DurchfÃ¼hrung (2025-11-16)

**Test-Event**:
```bash
docker exec cdb_redis redis-cli -a <REDIS_PASSWORD> PUBLISH market_data '{"symbol":"BTC_USDT","price":50000.0,"volume":1000000,"timestamp":1736600000,"pct_change":5.0,"source":"smoke_test"}'
```

**Ergebnis: âœ… BESTANDEN**

**Log-AuszÃ¼ge** (chronologisch):
```
cdb_core  | âœ¨ Signal generiert: BTC_USDT BUY @ $50000.00 (+5.00%, Confidence: 0.50)
cdb_risk  | ğŸ“¨ Signal empfangen: BTC_USDT BUY
cdb_risk  | âœ… Order freigegeben: BTC_USDT BUY qty=500.0000
cdb_execution | Processing order: BTC_USDT BUY qty=500.0000
cdb_execution | Order filled: MOCK_7f444f31 at 49968.68
cdb_execution | Published result to order_results
cdb_risk  | Order-Result empfangen: MOCK_7f444f31 status=FILLED qty=500.0000
```

**Acceptance-Kriterien** (alle erfÃ¼llt):
- âœ… Alle 8 Services blieben healthy (cdb_redis, cdb_postgres, cdb_prometheus, cdb_grafana, cdb_ws, cdb_core, cdb_risk, cdb_execution)
- âœ… Event "smoke_test" in Logs sichtbar (Symbol: BTC_USDT)
- âœ… Event-Flow vollstÃ¤ndig: market_data â†’ signal â†’ order â†’ order_result
- âœ… Keine CRITICAL-Fehler

**Beobachtungen**:
- cdb_execution: PostgreSQL-Warnung `relation "orders" does not exist` (erwartet bei frischer DB, Mock-Executor funktioniert trotzdem)
- Event-Latenz: <500ms fÃ¼r gesamte Pipeline (market_data bis order_result)

### Konsequenzen

**Positiv**:
- âœ… Initial-Commit kann durchgefÃ¼hrt werden (System funktionsfÃ¤hig validiert)
- âœ… Event-Flow nachweislich funktional (kritischster Use-Case erfolgreich)
- âœ… Klare Post-Migration-Roadmap fÃ¼r Test-Infrastruktur
- âœ… Kein Blocker durch pytest-Setup in kritischer Migrationsphase

**Negativ**:
- âš ï¸ Keine automatisierten Regressions-Tests (nur manueller Smoke-Test)
- âš ï¸ Kein Coverage-Report (unbekannt, welche Code-Pfade ungetestet sind)
- âš ï¸ Edge-Cases nicht validiert (nur Happy-Path getestet)
- âš ï¸ Risk-Manager-Logik nicht Unit-getestet (z. B. Drawdown-Limits, Position-Size-Checks)

**Risiko-Bewertung**: ğŸŸ¡ MEDIUM
- Event-Flow funktioniert (kritischste FunktionalitÃ¤t)
- Pre-Migration-Risiken behoben (SR-001 bis SR-003)
- Aber: Keine Tests fÃ¼r Risk-Limits, keine Fehlerfall-Validierung

**Mitigation**:
- Post-Migration: Test-Setup als **hÃ¶chste PrioritÃ¤t** (siehe TODO-Liste)
- Bis dahin: Nur Smoke-Tests nach grÃ¶ÃŸeren Ã„nderungen
- Deployment nur nach erfolgreichem Smoke-Test

### Post-Migration-Aufgaben (Test-Infrastruktur)

**Prio 1 - Test-Setup**:
1. Virtualenv erstellen: `python -m venv .venv`
2. requirements-dev.txt anlegen:
   ```
   pytest==7.4.3
   pytest-cov==4.1.0
   black==23.12.1
   mypy==1.8.0
   ```
3. Test-Verzeichnis-Struktur:
   ```
   tests/
   â”œâ”€â”€ conftest.py           # pytest-Fixtures
   â”œâ”€â”€ unit/
   â”‚   â”œâ”€â”€ test_risk_manager.py
   â”‚   â”œâ”€â”€ test_signal_engine.py
   â”‚   â””â”€â”€ test_execution_service.py
   â”œâ”€â”€ integration/
   â”‚   â””â”€â”€ test_event_flows.py
   â””â”€â”€ e2e/
       â””â”€â”€ test_smoke_automated.py
   ```

**Prio 2 - Test-Coverage-Ziele**:
- Risk-Manager: 80% (hÃ¶chste PrioritÃ¤t wegen kritischer Logik)
- Signal-Engine: 70%
- Execution-Service: 60%
- Screeners (cdb_ws): 50% (eher I/O-lastig)

**Prio 3 - CI-Integration**:
- GitHub Actions Workflow fÃ¼r pytest auf PRs
- Coverage-Report als Kommentar in PRs
- Smoke-Test als Health-Check in Deployment-Pipeline

### Referenzen

- **Pre-Migration Task**: Alle 4 Pipelines abgeschlossen (SR-001 bis SR-003 behoben)
- **DECISION-004**: Smoke-Test-Strategie (CLAUDE.md, Zeilen 1434-1574)
- **Smoke-Test-Log**: 2025-11-16, Event BTC_USDT, Flow komplett
- **Pipeline**: Pipeline 4 - Kanonische Systemrekonstruktion + Cleanroom-Migration
- **Canonical Schema**: `backoffice/docs/canonical_schema.yaml` (Referenz fÃ¼r Event-Validierung)

---

**Ende der Datei**
